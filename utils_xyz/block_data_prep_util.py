#xyz Sep 2017
'''
Data preparation for datsets: stanford_indoor, scannet, ETH_semantic3D
Core idea: store all the information in hdf5 file itself

# The workflow to use this tool:
Raw_H5f -> Sorted_H5f -> merge block to get new block size -> randomnly select n points
    -> Normed_H5f -> Net_Provider

## Raw_H5f store the raw data of dataset, which contains several datasets: xyz, label, color.... Each dataset
    stores the whole data for one dtype data.
    (.rh5)
## Sorted_H5f contains lots of lots of dataset. Each dataset stores all types of data within a spacial block.
    The point number of each block/dataset can be fix or not.
    (.sh5) Use class Sort_RawH5f to generate sorted file with unfixed point num in each block, and a small stride / step size.
    Then merge .sh5 file with small stride/step size to get larger size block.
    (.rsh5) Randomly sampling .sh5 file to get Sorted_H5f file with fixed point number in each block.
## Normed_H5f includes 4 datasets: data, label, raw_xyz, pred_logit
    (.nh5) This file is directly used to feed data for deep learning models.
    .nh5 file is generated by Sorted_H5f.file_normalize_to_NormedH5F()
## For all three files, show_h5f_summary_info() can use to show the info summary.
## scannet_block_sample.py is the basic usage for these classes.
'''

from __future__ import print_function
import os
import sys
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(BASE_DIR)
#from plyfile import (PlyData, PlyElement, make2d, PlyParseError, PlyProperty)
import math
import numpy as np
import h5py
import glob
import time
import multiprocessing as mp
import itertools
#from global_para import GLOBAL_PARA
sys.path.append(BASE_DIR+'/matterport_metadata')
from get_mpcat40 import MatterportMeta,get_cat40_from_rawcat
import csv,pickle
from gsbb_config import get_gsbb_config
import magic

DEBUGTMP=True

START_T = time.time()

g_h5_num_row_1M = 5*1000
ROOT_DIR = os.path.dirname(BASE_DIR)
UPER_DIR = os.path.dirname(ROOT_DIR)
DATA_DIR = os.path.join(ROOT_DIR,'data')

DATA_SOURCE_NAME_LIST = ['ETH','STANFORD_INDOOR3D','SCANNET','MATTERPORT']
FLOAT_BIAS = 1e-8

def isin_sorted( a,v ):
    i = np.searchsorted(a,v)
    if i>=a.size: return False
    r = a[i] == v
    return r

def get_stride_step_name(block_stride,block_step):
    assert block_step[0] == block_step[1]
    assert block_stride[0] == block_stride[1]
    assert (block_step[0] == block_step[2] and block_stride[0] == block_stride[2]) or (block_step[2]==-1 and block_stride[2]==-1)

    def get_str(v):
        assert (v*10) % 1 == 0, "v=%s"%(str(v))
        if v%1!=0: return '%dd%d'%(int(v),(v%1)*10)
        else: return str(int(v))
    if block_stride[2] == -1:
        return 'stride-%s-step-%s'%(get_str(block_stride[0]),get_str(block_step[0]))
    else:
        return 'stride_%s_step_%s'%(get_str(block_stride[0]),get_str(block_step[0]))
def rm_file_name_midpart(fn,rm_part):
    base_name = os.path.basename(fn)
    parts = base_name.split(rm_part)
    if len(parts)>1:
        new_bn = parts[0] + parts[1]
    else:
        new_bn = parts[0]
    new_fn = os.path.join(os.path.dirname(fn),new_bn)
    return new_fn


def copy_h5f_attrs(h5f_attrs):
    attrs = {}
    for e in h5f_attrs:
        attrs[e] = h5f_attrs[e]
    return attrs
def get_mean_sg_sample_rate(sum_sg_bidxmap_sample_num):
    global_block_num = sum_sg_bidxmap_sample_num[0,4]
    subblock_num = sum_sg_bidxmap_sample_num[:,-1]
    mean_sg_bidxmap_sample_num = np.copy(sum_sg_bidxmap_sample_num)
    for i in range(sum_sg_bidxmap_sample_num.shape[0]):
        mean_sg_bidxmap_sample_num[i,0:5] /= mean_sg_bidxmap_sample_num[i,4]
        mean_sg_bidxmap_sample_num[i,5:8] /= mean_sg_bidxmap_sample_num[i,7]
    return mean_sg_bidxmap_sample_num,global_block_num,subblock_num
def get_mean_flatten_sample_rate(sum_flatten_bmap_sample_num):
    global_block_num = sum_flatten_bmap_sample_num[0,2]
    mean_flatten_bmap_sample_num = np.copy(sum_flatten_bmap_sample_num)
    for i in range(sum_flatten_bmap_sample_num.shape[0]):
        mean_flatten_bmap_sample_num[i,0:3] /= mean_flatten_bmap_sample_num[i,2]
    return mean_flatten_bmap_sample_num,global_block_num

def get_attrs_str(attrs):
    attrs_str = ''
    for a in attrs:
        elenames = ''
        if type(attrs[a])==str:
            a_str = attrs[a]
        else:
            a_val = attrs[a]
            if a == "sum_sg_bidxmap_sample_num":
                a_val,global_block_num,subblock_num = get_mean_sg_sample_rate(a_val)
                elenames = str(GlobalSubBaseBLOCK.get_sg_bidxmap_sample_num_elename()) + '\n' + 'global_block_num: %d'%(global_block_num) + '\tsubblock_num: %s'%(subblock_num)  + '\n'
            if a == "sum_flatten_bmap_sample_num":
                a_val,global_block_num = get_mean_flatten_sample_rate(a_val)
                elenames = str(GlobalSubBaseBLOCK.get_flatten_bidxmaps_sample_num_elename()) +'\n' + 'global_block_num: %d'%(global_block_num) + '\n'

            a_str = np.array2string(a_val,precision=2,separator=',',suppress_small=True)
        attrs_str += ( a+':\n'+elenames+a_str+'\n' )
    return attrs_str

def show_h5f_summary_info(h5f):
    root_attrs = [attr for attr in h5f.attrs]
    print('--------------------------------------------------------------------------')
    print('The root_attr: ',root_attrs)
    print(get_attrs_str(h5f.attrs))

    print('\n--------------------------------------------------------------------------')
    print('The elements in h5f')
    def show_dset(dset_name,id):
        if id>10: return
        dset = h5f[dset_name]
        print('# dataset %d: '%(id),dset_name,'  shape=',dset.shape)
        if id>6: return
        print(get_attrs_str(dset.attrs))
        if len(dset.shape)==3:
            print(dset[0:min(2,dset.shape[0]),:])
        elif len(dset.shape)==4:
            var = dset[0:min(1,dset.shape[0]),0,0:min(2,dset.shape[2]),:]
            print(np.array2string(var,formatter={'float_kind':lambda var:"%0.2f"%var}))
        print('\n')
    def show_root_ele(ele_name,id):
        ele = h5f[ele_name]
        if type(ele) == h5py._hl.group.Group:
            print('The group: %s'%(ele_name))
            print(get_attrs_str(ele.attrs))
            for dset_name in ele:
                show_dset(ele_name+'/'+dset_name,id)
        else:
            show_dset(ele_name,id)
    k = -1
    for k, ele_name in enumerate(h5f):
        if ele_name == 'xyz':
            show_dset(ele_name,k)
            continue
        show_root_ele(ele_name,k)
    print('%d datasets totally'%(k+1))

def get_sample_choice(org_N,sample_N,random_sampl_pro=None):
    '''
    all replace with random_choice laer
    '''
    sample_method='random'
    if sample_method == 'random':
        if org_N == sample_N:
            sample_choice = np.arange(sample_N)
        elif org_N > sample_N:
            sample_choice = np.random.choice(org_N,sample_N,replace=False,p=random_sampl_pro)
        else:
            #sample_choice = np.arange(org_N)
            new_samp = np.random.choice(org_N,sample_N-org_N)
            sample_choice = np.concatenate( (np.arange(org_N),new_samp) )
        reduced_num = org_N - sample_N
        #str = '%d -> %d  %d%%'%(org_N,sample_N,100.0*sample_N/org_N)
        #print(str)
    return sample_choice,reduced_num
def random_choice(org_vector,sample_N,random_sampl_pro=None, keeporder=True, only_tile_last_one=False):
    assert org_vector.ndim == 1
    org_N = org_vector.size
    if org_N == sample_N:
        sampled_vector = org_vector
    elif org_N > sample_N:
        sampled_vector = np.random.choice(org_vector,sample_N,replace=False,p=random_sampl_pro)
        if keeporder:
            sampled_vector = np.sort(sampled_vector)
    else:
        if only_tile_last_one:
            new_vector = np.array( [ org_vector[-1] ]*(sample_N-org_N) ).astype(org_vector.dtype)
        else:
            new_vector = np.random.choice(org_vector,sample_N-org_N,replace=True)
        sampled_vector = np.concatenate( [org_vector,new_vector] )
    #str = '%d -> %d  %d%%'%(org_N,sample_N,100.0*sample_N/org_N)
    #print(str)
    return sampled_vector

def index_in_sorted(sorted_vector,values):
    values = np.array(values)
    assert values.ndim<=1 and sorted_vector.ndim==1
    values_valid = values[np.isin(values,sorted_vector)]
    indexs = np.searchsorted(sorted_vector,values_valid)
    assert indexs.size==0 or np.max(indexs) < sorted_vector.size, 'err in index_in_sorted'
    return indexs

def check_h5fs_intact(file_name):
    if not os.path.exists(file_name):
        return False,"file not exist: %s"%(file_name)
    f_format = os.path.splitext(file_name)[-1]
    if f_format == '.rh5':
        return Raw_H5f.check_rh5_intact(file_name)
    elif f_format == '.sh5' or f_format == '.rsh5':
        return Sorted_H5f.check_sh5_intact(file_name)
    elif f_format == '.nh5' or f_format == '.prh5':
        return Normed_H5f.check_nh5_intact(file_name)
    elif f_format == '.bmh5':
        return GlobalSubBaseBLOCK.check_bmh5_intact(file_name)
    else:
        return False, "file format not recognized %s"%(f_format)

def float_exact_division( A, B ):
    C = A / B
    r = np.isclose( C, np.rint(C) )
    R = r.all()
    return R

class GlobalSubBaseBLOCK():
    '''
        * Check + Problem:
            If nsubblock and sub_block_size are reasonable, to ensure all valid space is utilized, and no base block id is missed.
    '''
    flatbxmap_max_nearest_num = 3
    flatbxmap_max_dis = 3
    def load_default_parameters( self ):
        global_stride,global_step,global_num_point,sub_block_stride_candis,sub_block_step_candis,nsubblock_candis,npoint_subblock_candis, gsbb_config = \
            get_gsbb_config()
        for pn in self.para_names:
            setattr( self, pn, eval(pn) )

    def load_para_from_file( self, bmh5_fn ):
        f_format = os.path.splitext(bmh5_fn)[-1]
        if f_format == '.bmh5':
            IsIntact,s = GlobalSubBaseBLOCK.check_bmh5_intact( bmh5_fn )
            assert IsIntact, s
        else:
            IsIntact,s = Normed_H5f.check_nh5_intact( bmh5_fn )
            assert IsIntact, s
        with h5py.File( bmh5_fn,'r' ) as h5f:
            for ele_name in self.para_names + self.meta_names + self.root_para_names:
                setattr( self,ele_name, h5f.attrs[ele_name]  )


    def load_para_from_rootsh5( self ):
        IsIntact,s = Sorted_H5f.check_sh5_intact( self.root_s_h5f_fn )
        assert IsIntact, s
        for ele_name in self.root_para_names:
            setattr( self,ele_name, self.root_h5fattrs[ele_name[5:len(ele_name)]]  )
    #---------------------------------------------------------------------------
    def update_parameters( self ):
        self.cascade_num = cascade_num = len(self.sub_block_stride_candis)
        self.sum_sg_bidxmap_sample_num = np.zeros(shape=(cascade_num,2))
        self.sum_flatten_bmap_sample_num = np.zeros(shape=(cascade_num))

        sg_bidxmaps_extract_idx = np.zeros(shape=(cascade_num+1,2)).astype(np.int32)
        # [cascade_id+1,0] is end subblock indice of sg_bidxmaps[cascade_id]
        # [cascade_id+1,1] is npoint_subblock of sg_bidxmaps[cascade_id]
        for i in range(0,cascade_num):
            sg_bidxmaps_extract_idx[i+1,0] = sg_bidxmaps_extract_idx[i,0] + self.nsubblock_candis[i]
            sg_bidxmaps_extract_idx[i+1,1] = self.npoint_subblock_candis[i]
        self.sg_bidxmaps_extract_idx = sg_bidxmaps_extract_idx
        flatten_bidxmaps_extract_idx = np.zeros(shape=(cascade_num+1,2)).astype(np.int32)
        for i in range(1,cascade_num+1):
            if i==1:
                last_flatten_bmap_shape0 = self.global_num_point
            else:
                last_flatten_bmap_shape0 = self.nsubblock_candis[i-2]
            flatten_bidxmaps_extract_idx[i,0] = flatten_bidxmaps_extract_idx[i-1,0] + last_flatten_bmap_shape0
            flatten_bidxmaps_extract_idx[i,1] = 2
        self.flatten_bidxmaps_extract_idx = flatten_bidxmaps_extract_idx

        self.cascade_id_ls = cascade_id_ls = ['root']+range(cascade_num)+['global']
        base_cascade_ids = {}
        base_cascade_ids['global'] = 'root'
        for i in range(cascade_num):
            if i==0:
                base_cascade_ids[0] = 'root'
            else:
                base_cascade_ids[i] = i-1
        self.base_cascade_ids = base_cascade_ids

    def __init__(self,root_s_h5f = None, root_s_h5f_fn = None, bmh5_fn = None):
        self.new_attrs = {}
        self.bm_output = {}
        self.para_names = ['global_stride','global_step','global_num_point','sub_block_stride_candis','sub_block_step_candis','nsubblock_candis','npoint_subblock_candis', 'gsbb_config' ]
        self.root_para_names = ['root_block_stride','root_block_step']
        self.meta_names = ['sum_sg_bidxmap_sample_num','sum_flatten_bmap_sample_num']

        if bmh5_fn != None:
            assert root_s_h5f_fn == None and root_s_h5f == None
            self.mode = 'load'
            # load gsbb configuration from bmh5 file
            self.bmh5_fn = bmh5_fn
            self.load_para_from_file(bmh5_fn)
        else:
            self.load_default_parameters()
            if root_s_h5f != None and root_s_h5f_fn != None:
                assert bmh5_fn == None
                self.mode = 'write'
                self.root_s_h5f = root_s_h5f
                self.root_h5fattrs = root_s_h5f.attrs
                self.root_s_h5f_fn = root_s_h5f_fn
                # load gsbb configuration from gsbb_config.py
                self.load_para_from_rootsh5()
                self.bmh5_fn = self.get_bmapfn()
            elif root_s_h5f_fn == None and root_s_h5f == None and bmh5_fn == None:
                self.mode = 'empty_use'

        self.update_parameters()

    def get_bmapfn(self):
        assert self.mode == 'write'
        region_name = os.path.splitext( os.path.basename(self.root_s_h5f_fn) )[0]
        house_dir_name = os.path.dirname(self.root_s_h5f_fn)
        house_name = os.path.basename(house_dir_name)
        rootsort_dirname = os.path.dirname(house_dir_name)

        out_folder = rootsort_dirname + '_bmh5-' + self.get_pyramid_flag( OnlyGlobal = False)
        if not os.path.exists(out_folder):
            os.mkdir(out_folder)
        blockid_maps_fn = out_folder + '/' + house_name + '/' + region_name + '.bmh5'
        return blockid_maps_fn

    def write_paras_in_h5fattrs(self, aim_attrs ):
        assert self.mode == 'write'
        for ele_name in self.para_names + self.meta_names + self.root_para_names:
            aim_attrs[ele_name] = getattr( self,ele_name )

    def get_pyramid_flag(self, OnlyGlobal=False):
        def my_str(s):
            if s%1!=0:
                str_ = '%dd'%(int(s))+str(int((10*s)%10))
            else:
                str_ = str(int(s))
            return str_

        flag_str = str(self.global_num_point)
        flag_str += '_'+my_str(self.global_stride[0])+'_'+my_str(self.global_step[0])
        if OnlyGlobal:
            return flag_str
        flag_str += '-'
        for i,n in enumerate(self.nsubblock_candis):
            flag_str += str(n)
            if i<len(self.nsubblock_candis)-1:
                flag_str += '_'
            else:
                flag_str +='-'
        for i,n in enumerate(self.npoint_subblock_candis):
            flag_str += str(n)
            if i<len(self.npoint_subblock_candis)-1:
                flag_str += '_'
            else:
                flag_str +='-'
        for i,s in enumerate(self.sub_block_step_candis):
            flag_str += my_str(s)
            if i<len(self.sub_block_step_candis)-1:
                flag_str += '_'
            else:
                flag_str +='-'
        for i,s in enumerate(self.sub_block_stride_candis):
            flag_str += my_str(s)
            if i<len(self.sub_block_stride_candis)-1:
                flag_str += '_'
        return flag_str

    def get_block_sample_shape(self,cascade_id):
        base_casid = self.base_cascade_ids[cascade_id]
        return np.array((self.nsubblock_candis[base_casid],self.npoint_subblock_candis[base_casid],))

    def get_new_attrs(self,cascade_id) :
        assert self.mode == 'write'
        if cascade_id not in self.new_attrs:
            stride,step = self.get_stride_step_(cascade_id)
            new_attrs = Sorted_H5f.get_attrs_of_new_stride_step_(self.root_h5fattrs,stride,step)
            # add total_block_N, not add total_row_N yet
            total_block_N = self.get_block_n_of_new_stride_step_(cascade_id)
            new_attrs['total_block_N'] = total_block_N
            self.new_attrs[cascade_id] = new_attrs
        return self.new_attrs[cascade_id]

    def get_stride_step(self,cascade_id):
        return self.get_stride_step_(cascade_id)

    def get_stride_step_(self,cascade_id):
        #if cascade_id=='root': assert (root_h5fattrs!=None) or ('root_block_stride' in self)
        if cascade_id == 'global':
            stride = self.global_stride
            step = self.global_step
        elif cascade_id == 'root':
            if self.mode == 'w':
                stride = root_h5fattrs['block_stride']
                step = root_h5fattrs['block_step']
            else:
                stride = self.root_block_stride
                step = self.root_block_step
        else:
            assert cascade_id <= self.cascade_num-1 and cascade_id>=0, 'cascade_id=%s'%(str(cascade_id))
            stride  =  np.array([1.0,1.0,1.0])*self.sub_block_stride_candis[cascade_id]
            step =  np.array([1.0,1.0,1.0])*self.sub_block_step_candis[cascade_id]
        return stride,step

    def get_group_name(self,cascade_id):
        aim_stride,aim_step = self.get_stride_step_(cascade_id)
        aim_name = get_stride_step_name(aim_stride,aim_step)
        if cascade_id=='root':
            group_name = 'root-'+aim_name
        else:
            base_stride,base_step = self.get_stride_step_(self.base_cascade_ids[cascade_id])
            base_name = get_stride_step_name(base_stride,base_step)
            group_name = 'BASE_'+base_name+'-AIM_'+aim_name
        return group_name

    def get_block_n_of_new_stride_step_(self,cascade_id):
        return  self.load_one_bidxmap_(cascade_id,out=['block_num'])['block_num']

    def get_all_sorted_aimbids(self,cascade_id):
        ele_name = 'all_sorted_aimbids_'+str(cascade_id)
        if ele_name not in self.bm_output:
            self.bm_output[ele_name] = self.load_one_bidxmap(cascade_id,['all_sorted_aimbids'])['all_sorted_aimbids']
        return self.bm_output[ele_name]

    def get_baseids_inanew(self,cascade_id,new_bid):
        ele_name = 'allbaseids_in_new_dic-'+str(cascade_id)
        if ele_name in self.bm_output:
            return self.bm_output[ele_name][new_bid]
        else:
            return self.load_one_bidxmap(cascade_id,['baseids_inanew'],new_bid=new_bid)['baseids_inanew']
    def get_all_base_blockids_indic(self,cascade_id):
        ele_name = 'allbaseids_in_new_dic-'+str(cascade_id)
        if ele_name not in self.bm_output:
            self.bm_output[ele_name]  = self.load_one_bidxmap(cascade_id,['allbaseids_in_new_dic'])['allbaseids_in_new_dic']
        return self.bm_output[ele_name]
    @staticmethod
    def weighted_sample_bids(sorted_aimbids,base_bids_indic,aim_nsubblock):
        if 1.0 * sorted_aimbids.size / aim_nsubblock > 1.2:
            aim_num_bids = np.array([ base_bids_indic[aim_bid].shape[0] for aim_bid in sorted_aimbids ])
            random_sampl_pro = 1.0*aim_num_bids / np.sum(aim_num_bids)
        else:
            random_sampl_pro = None
        all_sorted_aimbids_sampled = np.sort( random_choice(sorted_aimbids,aim_nsubblock,random_sampl_pro) )
        valid_aimb_num = min(sorted_aimbids.size,aim_nsubblock)
        return all_sorted_aimbids_sampled, valid_aimb_num

    def get_bidxmap(self, cascade_id, valid_sorted_basebids ):
        '''
        valid_sorted_basebids: (valid_base_b_nun) base blocks are sampled at last process, some ids are lost
        bidxmap:(nsubblock,npoint_subblock)
            bidxmap[ aim_bid_index,: ] = [base_bid_index_0,1,2 ....31 ]
            base_bid_index is the index of base_bid stored in valid_sorted_basebids
            aim_bid_index is the index of aim_bid stored in sorted_aimbids_fixed
            flatten_bidxmap: (N,self.flatbxmap_max_nearest_num,3)
                         N: base_bid_index
                        [:,:,0]: aim_b_index
                        [:,:,1]: point_index_in_aimb
                        [:,:,2]: index_dis
        '''
        IsRecordTime = True and DEBUGTMP
        if IsRecordTime: t0 = time.time()
        if cascade_id==0:
            rootb_split_idxmap = valid_sorted_basebids
            # remove invalid values
            valid_rootb_n = np.searchsorted( rootb_split_idxmap[:,0]==-1,1 )
            rootb_split_idxmap = rootb_split_idxmap[0:valid_rootb_n,:]
            valid_sorted_basebids = np.arange( rootb_split_idxmap[-1,1] ) # root bids are the point indexs
            assert rootb_split_idxmap.ndim == 2
        else: assert valid_sorted_basebids.ndim == 1
        IsCheck = True
        # valid_sorted_basebids.size is the valid number of base blocks. Maximum value is nsubblock of last cascade.
        # Maybe less than this because of insufficient number in last one. Use valid number intead of sample number here.
        #valid_sorted_basebids = np.sort(valid_sorted_basebids)

        aim_nsubblock =  self.nsubblock_candis[cascade_id]
        aim_npoint_subblock = self.npoint_subblock_candis[cascade_id]

        # (1) Remove all the aim blocks contain no valid base blocks
        if IsRecordTime: t1 = time.time()
        all_sorted_aimbids = self.get_all_sorted_aimbids(cascade_id)
        all_base_blockids_indic = self.get_all_base_blockids_indic(cascade_id)
        bidxmap_dic={}
        raw_valid_base_bnum = []
        for aim_bid in all_sorted_aimbids:
            base_bids = all_base_blockids_indic[aim_bid]
            # use valid_sorted_basebids, instead of
            # valid_sorted_basebids_sample. Thus, when some base bids are
            # replicated, base_bid_valid_indexs are always assigned with smaller
            # locations. (In random_choice(), replicated eles are placed at end)
            if cascade_id == 0:
                rootbid_valid_indexs = index_in_sorted( rootb_split_idxmap[:,0], base_bids )
                point_indexs = np.array([]).astype(np.int32)
                for rootb_index in rootbid_valid_indexs:
                    if rootb_index == 0: start = 0
                    else: start = rootb_split_idxmap[rootb_index-1,1]
                    point_idx = np.arange( start, rootb_split_idxmap[rootb_index,1] )
                    point_indexs = np.concatenate( [point_indexs,point_idx] )
                base_bid_valid_indexs = point_indexs
            else:
                base_bid_valid_indexs = index_in_sorted(valid_sorted_basebids,base_bids)
            if len(base_bid_valid_indexs)==0:
                continue
            raw_valid_base_bnum.append(len(base_bid_valid_indexs))
            bidxmap_dic[aim_bid] = base_bid_valid_indexs
        raw_valid_base_bnum = np.array(raw_valid_base_bnum)
        valid_sorted_aimbids = np.sort( bidxmap_dic.keys() )


        if IsRecordTime: t2a = time.time()
        aim_attrs = self.get_new_attrs(cascade_id)
        if aim_nsubblock < valid_sorted_aimbids.size:
            sorted_aimbids_fixed, bidxmap_dic_sampled  = GlobalSubBaseBLOCK.fix_bmap( valid_sorted_aimbids, bidxmap_dic, aim_nsubblock, aim_npoint_subblock, aim_attrs )
            valid_aimb_num = aim_nsubblock
            valid_sorted_aimbids_fixed = sorted_aimbids_fixed
        else:
            valid_aimb_num = valid_sorted_aimbids.size
            # only add the end bid, and flatten_bidxmap follows the same rule.
            # So that, at the next cascade_id, the flatted aim_b_index is right.
            sorted_aimbids_fixed = random_choice(valid_sorted_aimbids, aim_nsubblock, only_tile_last_one=True)
            valid_sorted_aimbids_fixed = valid_sorted_aimbids

        if IsRecordTime: t2b = time.time()

        # (2) Get all the maps of valid aim blocks
        sg_bidxmap = np.zeros(shape=(aim_nsubblock,aim_npoint_subblock)).astype(np.int32)
        aimb_valid_point_num = np.zeros( shape=(valid_aimb_num) ).astype(np.int32)

        flatten_bidxmap = np.ones(shape=(valid_sorted_basebids.size,self.flatbxmap_max_nearest_num,3)).astype(np.int32)*(-1)
        #flatten_bidxmap = np.ones(shape=(valid_sorted_basebids.size,2)).astype(np.int32)*(-1)
        flatten_bidxmap_num = np.zeros( shape=(valid_sorted_basebids.size) ).astype(np.int8)
        for aim_b_index in range(aim_nsubblock):
            aim_bid = sorted_aimbids_fixed[aim_b_index]
            base_bid_valid_indexs = bidxmap_dic[aim_bid]

            sg_bidxmap[aim_b_index,:] = random_choice( base_bid_valid_indexs,aim_npoint_subblock )

            if aim_b_index < valid_aimb_num:
                aimb_valid_point_num[aim_b_index] = base_bid_valid_indexs.size
                for pointindex_within_subblock, baseb_index in enumerate(base_bid_valid_indexs):
                    if flatten_bidxmap_num[baseb_index] < self.flatbxmap_max_nearest_num:
                        flatten_bidxmap[baseb_index,flatten_bidxmap_num[baseb_index],:] = [aim_b_index,pointindex_within_subblock,0]
                        #flatten_bidxmap[baseb_index,:] = [aim_b_index,pointindex_within_subblock]
                        flatten_bidxmap_num[baseb_index] += 1

        if IsRecordTime: t3 = time.time()


        # (3) Tile flatten_bidxmap for all the missed blocks
        #     Because of insufficient aim_nsubblock or small aim_sub_block_size,some valid base blocks are missed. When a base point is missed, it would be hard to back-propogate global features to this base point.
        #     The correct way is to propogate from nearest 1/3 others aim_blocks.
        if cascade_id==0:
            base_attrs = self.root_h5fattrs
        else:
            base_attrs = self.get_new_attrs( cascade_id-1 )

        def get_around_bid( ixyzs, attrs, valid_bids, max_need_num ):
            ar_bidxs = []
            ar_ixyzs = []
            block_dims_N = attrs['block_dims_N']
            offset_ls = []
            offset_ls.append( np.array( [ [1,0,0],[0,1,0],[0,0,1] ] ) )
            offset_ls.append( -offset_ls[0] )
            offset_ls.append( np.array( [ [1,1,0],[1,-1,0],[0,1,1],[0,1,-1],[1,0,1],[1,0,-1] ] ) )
            offset_ls.append( -offset_ls[2] )
            offset_ls.append( np.array( [ [1,1,1],[-1,-1,-1] ] ) )
            sr_offsets0 = np.concatenate( offset_ls, 0 )
            for step in [1,2]:
                sr_offsets = sr_offsets0 * step
                for ixyz in ixyzs:
                    if len(ar_bidxs) >= max_need_num: break
                    for i in range(sr_offsets.shape[0]):
                        ixyz_sr = np.copy(ixyz)
                        ixyz_sr += sr_offsets[i,:]
                        if (ixyz_sr >= 0).all() and (ixyz_sr < block_dims_N).all():
                            bid = Sorted_H5f.ixyz_to_block_index_( ixyz_sr,attrs )
                            if (bid not in ar_bidxs) and isin_sorted( valid_bids, bid):
                                b_index = index_in_sorted( valid_bids,bid )
                                dis = np.linalg.norm( ixyz - ixyz_sr )
                                ar_bidxs.append( [b_index, dis] )
                                ar_ixyzs.append( ixyz_sr )
                                if len(ar_bidxs) >= max_need_num: break
                               # if step>2:
                               #         import pdb; pdb.set_trace()  # XXX BREAKPOINT
                               #         print('step>2')
            ar_bidxs = np.array( ar_bidxs )
            return ar_bidxs

        for baseb_index in range(flatten_bidxmap.shape[0]):
            if flatten_bidxmap_num[baseb_index] < self.flatbxmap_max_nearest_num:
                base_bid = valid_sorted_basebids[baseb_index]
                aim_ixyzs = Sorted_H5f.get_blockids_of_dif_stride_step( base_bid, base_attrs, aim_attrs )[1]
                # search around aim_ixyz to find the nearest self.flatbxmap_max_nearest_num valid aim_bids
                around_aimbidxs = get_around_bid( aim_ixyzs, aim_attrs, valid_sorted_aimbids, self.flatbxmap_max_nearest_num-flatten_bidxmap_num[baseb_index] )
                for k in range( min( around_aimbidxs.shape[0] ,self.flatbxmap_max_nearest_num) ):
                    flatten_bidxmap[baseb_index,k,:] = [ around_aimbidxs[k,0],-1,around_aimbidxs[k,1] ]
                    flatten_bidxmap_num[baseb_index] += 1

        # If there are still any missed blocks, till random aim bids and set the dis to be very large =>>> the weight will be 0
        for baseb_index in range(flatten_bidxmap.shape[0]):
            if flatten_bidxmap_num[baseb_index] == 0:
                for i in range(flatten_bidxmap.shape[1]):
                    flatten_bidxmap[baseb_index,i,:] = [0,-2,1000]
                    flatten_bidxmap_num[baseb_index] += 1
        # If there are still some bidxmaps not fill, till any valid one but weight be 0
        for baseb_index in range(flatten_bidxmap.shape[0]):
            if flatten_bidxmap_num[baseb_index]!=self.flatbxmap_max_nearest_num:
                for i in range(flatten_bidxmap_num[baseb_index],self.flatbxmap_max_nearest_num):
                    flatten_bidxmap[baseb_index,i,:] = flatten_bidxmap[baseb_index,0,:]
                    flatten_bidxmap[baseb_index,i,2] = 1000
                    flatten_bidxmap_num[baseb_index] += 1

        nonfull_baseb_num = np.sum( flatten_bidxmap_num != self.flatbxmap_max_nearest_num )
        assert nonfull_baseb_num==0
        #if DEBUGTMP:
        #    missed_baseb_num = np.sum( flatten_bidxmap_num == 0 )
        #    print('missed_baseb_num=%d'%(missed_baseb_num))
        #    print( 'nonfull_baseb_num=%d'%(nonfull_baseb_num) )

        if IsRecordTime: t4 = time.time()
        if IsRecordTime:
            print('(1) get aim_nsubblock t=%f'%(t1-t0))
            print('(2a) fix_bmap t=%f'%(t2a-t1))
            print('(2b) get valid_sorted_aimbids t=%f'%(t2b-t2a))
            print('(3) get flatten_bidxmap_num t=%f'%(t3-t2b))
            print('(4) fix flatten_bidxmap_num t=%f'%(t4-t3))

        # tile the last one to fix flatten_bidxmap shape_0
        if cascade_id>0: base_nsubblock = self.nsubblock_candis[cascade_id-1]
        else: base_nsubblock = self.global_num_point
        if flatten_bidxmap.shape[0] > base_nsubblock:
            print('e')
            import pdb; pdb.set_trace()  # XXX BREAKPOINT
        flatten_bidxmap_tile = np.tile( np.expand_dims(flatten_bidxmap[-1,:,:],0), (base_nsubblock-flatten_bidxmap.shape[0],1,1) )
        flatten_bidxmap_fixed = np.concatenate( [flatten_bidxmap, flatten_bidxmap_tile],0 )

        import pdb; pdb.set_trace()  # XXX BREAKPOINT

        #sg_sample_num = np.array( [missed_baseb_num, valid_sorted_basebids.size, aim_nsubblock, all_sorted_aimbids.size, 1,
        #                           aim_npoint_subblock*raw_valid_base_bnum.size, raw_valid_base_bnum.sum(),raw_valid_base_bnum.size ] ).astype(np.uint64)
        return sg_bidxmap, valid_sorted_aimbids_fixed, flatten_bidxmap_fixed

    def get_bidxmap0( self, rootb_split_idxmap ):

        return sg_bidxmap0, flatten_bidxmap0, valid_sorted_bids_cas0

    @staticmethod
    def get_sg_bidxmap_sample_num_elename():
        return [ 'missed_baseb_num','all_baseb_num',  'nsubblock','valid_subblock_num', 'unit_block_num',  'npoint_subblock', 'valid_npoint_subblock','subblock_num' ]
    @staticmethod
    def get_flatten_bidxmaps_sample_num_elename():
        return [ 'flatten_fixed_num', 'flatten_valid_num', 'block_num' ]

    def get_all_bidxmaps(self, rootb_split_idxmap ):
        '''
        fuse bidxmap from cascade_id 0 to end
        (1)sg_bidxmaps_ls: list, len= self.cascade_num-1, start from cascade_id=1
            sg_bidxmaps_ls[cascade_id-1]: (nsubblock,npoint_subblock)
            eg. cascade_id=1; bidxmap1 = sg_bidxmaps_ls[0] are all the cas0_b_indexs in cas1_b.
            bidxmap1[cas1_b_index,cas1_subb_index] = cas0_b_index
            This is used to get grouped points by:  grouped_points0 = tf.gather(points0,bidxmap1)

        (2)flatten_bidxmaps_ls, len = self.cascade_num, start from cascade_id=0
            flatten_bidxmaps_ls[cascade_id]:(last_nsubblock,2)
            eg. cascade_id=0, flatten_bidxmap0 = flatten_bidxmaps_ls[0] are all the cas1_b_index and cas1_subb_index.
            flatten_bidxmap0[cas0_b_index,:] = [cas1_b_index, cas1_subb_index]
            This is used to get raw unsampled points from grouped points by: point0 = tf.gather(grouped_points0,flatten_bidxmap0)
        (3) sg_bidxmap_sample_num:
            [ missed_baseb_num, all_baseb_num,  nsubblock,valid_subblock_num, unit_block_num,  npoint_subblock, valid_npoint_subblock, subblock_num ]

            In an unit block, there a <subblock_num> subbloks. In a subblock, there are <npoint_subblock> points.
            missed_baseb_num  is affected by both sub_block_size & nsubblock.
            Eg. based on sg_bidxmap_sample_num of cascade 0:
                missed_baseb_num_ca0 > 0 => (1) sub_block_size too small or (2) nsubblock_cas0 too small
                nsubblock_cas0 < valid_subblock_num_cas0 => nsubblock_cas0 too small
                npoint_subblock_cas0 < valid_npoint_subblock => npoint_subblock_cas0 too small

            Note: missed_baseb_num != valid_npoint_subblock - npoint_subblock
                  Because some base blocks may not in aim_block.
        (4) flatten_bmap_sample_num:
            [ flatten fixed num, flatten valid num, block_num ]
        '''
        IsCheck_bidxmaps = True

        sg_bidxmaps_ls = []
        sg_bidxmaps_fixed_ls =  []
        sg_bidxmaps_fixed_shape1 = self.get_sg_bidxmaps_fixed_shape()[1]
        #sg_bidxmap_sample_num = np.zeros(shape=(self.cascade_num,8)).astype(np.uint64)
        flatten_bidxmaps_ls = []
        #flatten_bmap_sample_num =  np.zeros(shape=(self.cascade_num,3)).astype(np.uint64)

        #sg_bidxmap_sample_num[0,:] = sg_sample_num_cas0
        def fix_var_shape(org_var,aim_shape,axis):
            tile_num = aim_shape - org_var.shape[axis]
            if tile_num == 0:
                new_var = org_var
            elif tile_num < 0:
                assert "tile_num shoue >= 0, but =%d"%(tile_num)
                fix_indices = random_choice( np.arange(0,org_var.shape[axis]), aim_shape )
                new_var = np.take( org_var,fix_indices,axis=axis )
            elif tile_num >0:
                tile_method = 'invalid'
                if tile_method == 'valid':
                    if axis==0:
                        tiled = np.tile(org_var[0:1,:],[tile_num,1])
                    elif axis==1:
                        tiled = np.tile(org_var[:,0:1],[1,tile_num])
                    new_var = np.concatenate([org_var, tiled],axis=axis)
                if tile_method == 'invalid':
                    if axis==0:
                        new_var = np.ones(shape=(aim_shape,org_var.shape[1])).astype(org_var.dtype) * (-1)
                        new_var[0:org_var.shape[0],:] = org_var
                    elif axis==1:
                        new_var = np.ones(shape=(org_var.shape[0],aim_shape)).astype(org_var.dtype) * (-1)
                        new_var[:,0:org_var.shape[1]] = org_var
            sample_num = np.array([aim_shape,org_var.shape[axis],1]).reshape(1,3)
            return new_var, sample_num


        global_num = self.global_num_point
        #flatten_bidxmap0,inv_sample_num0 = fix_var_shape(flatten_bidxmap0,global_num,0)
        #flatten_bidxmaps_ls.append(flatten_bidxmap0)
        #flatten_bmap_sample_num[0,:] = inv_sample_num0

        valid_sorted_basebids_fixed = rootb_split_idxmap
        for cascade_id in range(0,self.cascade_num):
            sg_bidxmap, valid_sorted_basebids_fixed,flatten_bidxmap = self.get_bidxmap(cascade_id, valid_sorted_basebids_fixed )
            if IsCheck_bidxmaps:  sg_bidxmaps_ls.append( sg_bidxmap )
            sg_bidxmap_fixed = np.ones( shape=(sg_bidxmap.shape[0],sg_bidxmaps_fixed_shape1) ).astype(np.int32) * (-1)
            sg_bidxmap_fixed[:,0:sg_bidxmap.shape[1]] = sg_bidxmap
            sg_bidxmaps_fixed_ls.append( sg_bidxmap_fixed )

            #sg_bidxmap_sample_num[cascade_id,:] = sg_sample_num
            #flatten_bidxmap,sample_num_flatten = fix_var_shape(flatten_bidxmap,self.nsubblock_candis[cascade_id-1],0)
            flatten_bidxmaps_ls.append(flatten_bidxmap)
            #flatten_bmap_sample_num[cascade_id,:] = sample_num_flatten
        sg_bidxmaps = np.concatenate(sg_bidxmaps_fixed_ls,axis=0)
        flatten_bidxmaps = np.concatenate(flatten_bidxmaps_ls,axis=0)

        if IsCheck_bidxmaps:
            assert sg_bidxmaps.shape == self.get_sg_bidxmaps_fixed_shape()
            assert flatten_bidxmaps.shape ==  self.get_flatten_bidxmaps_shape()
            for cascade_id in range(0,self.cascade_num):
                sg_bidxmap0_extracted = self.extract_sg_bidxmaps(sg_bidxmaps,cascade_id)
                assert np.sum(sg_bidxmaps_ls[cascade_id] != self.extract_sg_bidxmaps(sg_bidxmaps,cascade_id))==0
                assert np.sum(flatten_bidxmaps_ls[cascade_id] != self.extract_flatten_bidxmaps(flatten_bidxmaps,cascade_id))==0

        return sg_bidxmaps, flatten_bidxmaps

    def extract_sg_bidxmaps(self,sg_bidxmaps,cascade_id):
        start = self.sg_bidxmaps_extract_idx[cascade_id,:]
        end = self.sg_bidxmaps_extract_idx[cascade_id+1,:]
        #start_ = np.sum(self.nsubblock_candis[1:cascade_id])+0
        #end_ = start + self.nsubblock_candis[cascade_id]
        return sg_bidxmaps[ start[0]:end[0],0:end[1] ]

        #return sg_bidxmaps[start:end,0:self.npoint_subblock_candis[cascade_id]]

    def extract_flatten_bidxmaps(self,flatten_bidxmaps,cascade_id):
        start = self.flatten_bidxmaps_extract_idx[cascade_id,:]
        end = self.flatten_bidxmaps_extract_idx[cascade_id+1,:]

        #tmp = [self.flatten_bmap_shape0(cid) for cid in range(cascade_id)]
        #start_ = int(np.sum(tmp)+0)
        #end_ = int(start_ + self.flatten_bmap_shape0(cascade_id))
        return flatten_bidxmaps[start[0]:end[0],:]

    def get_sg_bidxmaps_fixed_shape(self):
        # tile all the sg_bidxmaps to same(max) shape[0], so that they can be
        # concatenated in one array
        shape0 = np.sum(self.nsubblock_candis[0:self.cascade_num])
        shape1 = max(self.npoint_subblock_candis[0:self.cascade_num])
        return (shape0,shape1)
    def load_one_bidxmap(self,cascade_id,out=['block_num','all_sorted_aimbids','baseids_inanew','allbaseids_in_new_dic'],new_bid=None):
        # load one block id map
        # return block id map from cascade_id-1 to cascade_id
        return self.load_one_bidxmap_(cascade_id,out,new_bid)
    def flatten_bmap_shape0(self,cascade_id):
        if cascade_id==0:
            return self.global_num_point
        else:
            return self.nsubblock_candis[cascade_id-1]
    def get_flatten_bidxmaps_shape(self):
        shape0 = np.sum([self.flatten_bmap_shape0(cid) for cid in range(self.cascade_num)])
        return (shape0,self.flatbxmap_max_nearest_num,3)

    def load_one_bidxmap_(self,cascade_id,out=['block_num','all_sorted_aimbids','baseids_inanew','allbaseids_in_new_dic'],new_bid=None):
        assert os.path.exists(self.bmh5_fn),"file not exist: %s"%(self.bmh5_fn)
        with h5py.File(self.bmh5_fn, 'r') as h5f:
            larger_stride, larger_step = self.get_stride_step_(cascade_id)
            group_name = self.get_group_name(cascade_id)
            grp = h5f[group_name]
            bm_output = {}
            if 'block_num' in out or 'all_sorted_aimbids' in out:
                all_sorted_aimbids = grp['all_sorted_aimbids'][...]
                if 'block_num' in out:
                    bm_output['block_num'] = all_sorted_aimbids.shape[0]
                if 'all_sorted_aimbids' in out:
                    bm_output['all_sorted_aimbids'] = all_sorted_aimbids
            if 'baseids_inanew' in out:
                bm_output['baseids_inanew'] = grp[str(new_bid)][...]
            if 'allbaseids_in_new_dic' in out:
                allbaseids_in_new_dic = {}
                for new_bid_str in grp:
                    if new_bid_str != 'all_sorted_aimbids':
                        allbaseids_in_new_dic[int(new_bid_str)] = grp[new_bid_str][...]
                bm_output['allbaseids_in_new_dic'] = allbaseids_in_new_dic
            return bm_output

    def show_all_groupnames(self):
        with h5py.File(self.bmh5_fn,'r') as h5f:
            print('\nall group names')
            for group_name in h5f:
                print(group_name)

    def show_all(self):
        self.show_all_groupnames( )
        t0 = time.time()
        new_bid = 0
        for out in ['block_num','all_sorted_aimbids']:
            cascade_id_ls = self.cascade_id_ls
            for cascade_id in cascade_id_ls:
                if cascade_id=='root' and out=='baseids_inanew':
                    continue
                output = self.load_one_bidxmap_(cascade_id,[out],new_bid)

                if out=='all_sorted_aimbids':
                    n = output[out].shape[0]
                    print('all_sorted_aimbids: shape=',output[out].shape)
                    print('\t',output[out][0:min(20,n)])
                if out=='block_num':
                    print('block_num:',output[out])
                #print('\nt = %f ms, cascade_id = %s   %s'%(1000*(time.time()-t0),str(cascade_id),out))
        IsIntact,ck_str = GlobalSubBaseBLOCK.check_bmh5_intact( self.bmh5_fn )
        print('IsIntact:',IsIntact)


    def save_bmap_between_dif_stride_step(self):
        '''
        bmh5f structure:
            for each cascde_id, create a group. eg cascade_id='root' grp_name='root-stride_0d1_step_0d1'
            In each grp,
                1) create a dataset: "all_sorted_aimbids", shape=all_sorted_larger_blockids.shape
                2) for each new_bid, create a dataset: str(new_bid), shape=(len(base_ids),) blockid_map_dset[...] = base_ids

        '''
        assert self.mode == 'write'
        folder = os.path.dirname(self.bmh5_fn)
        if not os.path.exists( folder ):
            os.makedirs( folder )
        print('start writing %s'%(self.bmh5_fn))
        with h5py.File(self.bmh5_fn,'w') as h5f:
            h5f.attrs['is_intact_bmh5'] = 0
            self.write_paras_in_h5fattrs( h5f.attrs )

            all_sorted_blockids_dic={}
            all_sorted_blockids_dic['root'] = np.sort([int(k) for k in self.root_s_h5f])

            cascade_id_ls = self.cascade_id_ls
            cascade_attrs = {}
            cascade_attrs['root'] = self.root_s_h5f.attrs
            for cascade_id in cascade_id_ls:
                if cascade_id == 'root':
                    all_sorted_larger_blockids = all_sorted_blockids_dic[cascade_id]
                else:
                    base_cascadeid = self.base_cascade_ids[cascade_id]
                    all_sorted_base_blockids = all_sorted_blockids_dic[base_cascadeid]
                    base_attrs = cascade_attrs[base_cascadeid]
                    larger_stride, larger_step = self.get_stride_step_(cascade_id)
                    new_attrs, basebids_in_each_largerbid_dic, all_sorted_larger_blockids = GlobalSubBaseBLOCK.get_basebids_in_all_largerbid(
                                                            base_attrs,all_sorted_base_blockids,larger_stride,larger_step)
                    all_sorted_blockids_dic[cascade_id] = all_sorted_larger_blockids
                    cascade_attrs[cascade_id] = new_attrs
                group_name = self.get_group_name(cascade_id)

                grp = h5f.create_group(group_name)
                all_sorted_aimbids_dset = grp.create_dataset( 'all_sorted_aimbids',shape=all_sorted_larger_blockids.shape,dtype=np.int32  )
                all_sorted_aimbids_dset[...] = all_sorted_larger_blockids
                if not cascade_id == 'root':
                    for new_bid,base_ids in basebids_in_each_largerbid_dic.items():
                        blockid_map_dset = grp.create_dataset( str(new_bid),shape=(len(base_ids),),dtype=np.int32  )
                        blockid_map_dset[...] = base_ids
            h5f.attrs['is_intact_bmh5'] = 1
            h5f.flush()
            print('write finish: %s'%(self.bmh5_fn))


    @staticmethod
    def check_bmh5_intact(file_name):
        #file_name = self.get_bmapfn(root_s_h5f_fn)
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.bmh5'
        if not os.path.exists(file_name):
            return False,"%s not exist"%(file_name)
        #if os.path.getsize( file_name ) / 1000.0 < 10:
        #    return False,"file too small < 20 K"
        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        #print('checking bmh5 file:',file_name)
        with h5py.File(file_name,'r') as h5f:
            if 'is_intact_bmh5' not in h5f.attrs:
                return False,""
            IsIntact = h5f.attrs['is_intact_bmh5'] == 1
            print('bmh5 file intact:',file_name)
            return IsIntact,""

      #      attrs_to_check = ['num_group','cascade_idstr_ls']
      #      for attrs in attrs_to_check:
      #          if attrs not in h5f.attrs:
      #              return False, "%s not in %s"%(attrs,f_format)
      #  return True,""


    @staticmethod
    def get_scope_of_bids(bids,attrs):
        xyz_min = np.array([1.0,1.0,1.0])*(10000)
        xyz_max = np.array([1.0,1.0,1.0])*(-10000)
        for i in range(bids.size):
            block_min, block_max, i_xyz = Sorted_H5f.get_block_scope_from_k_(bids[i],attrs)
            for j in range(3):
                if block_min[j] < xyz_min[j]:
                    xyz_min[j] = block_min[j]
                if block_max[j] > xyz_max[j]:
                    xyz_max[j] = block_max[j]
        assert (xyz_max > xyz_min).all()
        xyz_scope = xyz_max - xyz_min
        return xyz_scope
    @staticmethod
    def get_basebids_in_all_largerbid(base_attrs,all_base_blockids,larger_stride,larger_step):
        '''
        find all the valid block ids with larger_stride and larger_step,
        and all the base block ids in each larger_stride and larger_step.
        root_s_h5f: sorted h5f object
        '''
        new_sorted_h5f_attrs = Sorted_H5f.get_attrs_of_new_stride_step_(base_attrs,larger_stride,larger_step)
        new_block_dims_N = new_sorted_h5f_attrs['block_dims_N']
        if larger_stride[-1]==-1 and larger_step[-1]==-1:
            assert new_block_dims_N[-1]==1
        max_new_block_id = Sorted_H5f.ixyz_to_block_index_(new_block_dims_N-1,new_sorted_h5f_attrs)
        new_total_block_N = 0
        basebids_in_each_largerbid_dic = {}
        print('max_new_block_id = ',max_new_block_id)
        for new_block_id in range(max_new_block_id+1):

            base_bid_ls,_ = Sorted_H5f.get_blockids_of_dif_stride_step(
                                    new_block_id,new_sorted_h5f_attrs,base_attrs)
            base_bids = np.array(base_bid_ls).astype(np.uint32)

            mask = np.in1d( base_bids,all_base_blockids )
            valid_base_bids = base_bids[mask]
            # check the scope of valid_cur_bids

            if valid_base_bids.shape[0] > 0:
                valid_scope = GlobalSubBaseBLOCK.get_scope_of_bids( valid_base_bids, base_attrs )
                scope_rate = valid_scope / new_sorted_h5f_attrs['block_step']
                valid_scope_rate = np.min(scope_rate)
                if valid_scope_rate > 0.2:
                    new_total_block_N += 1
                    basebids_in_each_largerbid_dic[new_block_id] = valid_base_bids

            if new_block_id >0 and new_block_id % 10000==0:
                rate = 1.0*(new_block_id+1)/(max_new_block_id+1)*100
                print('%f%%  new id: %d  new stride step: %s      base stride step: %s'%(rate,new_block_id,
                      get_stride_step_name(larger_stride,larger_step),get_stride_step_name(base_attrs['block_stride'],base_attrs['block_step'])))
        larger_blockids = np.array(list(basebids_in_each_largerbid_dic.keys())).astype(np.uint32)
        all_sorted_larger_blockids = np.sort(larger_blockids)

        # check: basebids_in_each_largerbid_dic shoule contain all the all_base_blockids. If larger_stride==larger_step, each base bid should occur one time.
        CHECK = True
        if CHECK:
            all_base_blockids_indic = np.concatenate( basebids_in_each_largerbid_dic.values()).astype(np.int32)
            if not (larger_stride == larger_step).all():
                all_base_blockids_indic = np.setxor1d(all_base_blockids_indic,np.array([])).astype(np.int32)
            all_base_blockids_indic = np.sort(all_base_blockids_indic)
            if not all_base_blockids_indic.shape[0] == all_base_blockids.shape[0]:
                assert False, "Not  all the base blocks are exactly included"
            if not (all_base_blockids_indic == all_base_blockids).all():
                assert False, "Not  all the base blocks are exactly included"

            print('\nbasebids in each largerbid dic check ok\n  new stride step: %s      base stride step: %s'%(
                      get_stride_step_name(larger_stride,larger_step),get_stride_step_name(base_attrs['block_stride'],base_attrs['block_step'])))

        return new_sorted_h5f_attrs, basebids_in_each_largerbid_dic, all_sorted_larger_blockids

    @staticmethod
    def fix_bmap( all_sorted_aimbids,all_base_bids_in_aim_dic, nsubblock, npoint_subblock, aim_attrs ):
        '''
        When aim block num is larger than nsubblock, select the aim blocks with more points
        '''
        org_aim_b_num = len(all_sorted_aimbids)
        assert org_aim_b_num > nsubblock
        base_b_num = np.zeros(shape=(org_aim_b_num)).astype(np.uint32)
        for i in range(org_aim_b_num):
            base_b_num[i] = len(all_base_bids_in_aim_dic[all_sorted_aimbids[i]])
        sort_aim_indices = np.argsort(base_b_num)
        cut_aim_indices = sort_aim_indices[0:org_aim_b_num-nsubblock]
        cut_aim_bids = all_sorted_aimbids[cut_aim_indices]
        keep_aim_indices = sort_aim_indices[org_aim_b_num-nsubblock:sort_aim_indices.size]
        keep_aim_bids = np.sort(all_sorted_aimbids[keep_aim_indices])
        # append all base_bids of each cutted aim_bids to nearest aim_bid
        import copy
        all_base_bids_in_aim_dic_valid = copy.deepcopy(all_base_bids_in_aim_dic)
        for cut_aim_bid in cut_aim_bids:
            dis = np.zeros(shape=(nsubblock))+1000.0
            for j,aim_bid_search in enumerate(keep_aim_bids):
                dis[j] = Sorted_H5f.get_block_dis_( aim_bid_search,cut_aim_bid,aim_attrs )
            mindis_indice = np.argmin(dis)
            mindis_aimbid = keep_aim_bids[mindis_indice]

            all_base_bids_in_aim_dic_valid[mindis_aimbid] = np.concatenate([ all_base_bids_in_aim_dic[mindis_aimbid], all_base_bids_in_aim_dic[cut_aim_bid] ] )
            del all_base_bids_in_aim_dic_valid[cut_aim_bid]
        return keep_aim_bids, all_base_bids_in_aim_dic_valid


gsbb_empty =  GlobalSubBaseBLOCK()

class Raw_H5f():
    '''
    * raw data:unsorted points,all the time in one dataset
    * Each data type as a hdf5 dataset: xyz, intensity, label, color
    * class "Sorted_H5f" will sort data to blocks based on this class
    '''
    file_flag = 'RAW_H5F'
    h5_num_row_1M = 50*1000
    dtypes = { 'xyz':np.float32, 'nxnynz':np.float32, 'intensity':np.int32, 'color':np.uint8,'label_category':np.uint32,'label_instance':np.int32,'label_material':np.int32,'label':np.int32 }
    num_channels = {'xyz':3,'nxnynz':3,'intensity':1,'color':3,'label_category':1,'label_instance':1,'label_material':1,'label':1}
    def __init__(self,raw_h5_f,file_name,datasource_name=None):
        self.h5f = raw_h5_f
        if datasource_name == None:
            assert 'datasource_name' in self.h5f.attrs
        else:
            self.h5f.attrs['datasource_name'] = datasource_name
        assert self.h5f.attrs['datasource_name'] in DATA_SOURCE_NAME_LIST
        self.get_summary_info()
        self.file_name = file_name
        self.num_default_row = 0

    def show_h5f_summary_info(self):
        print('\n\nsummary of file: ',self.file_name)
        show_h5f_summary_info(self.h5f)

    def set_num_default_row(self,N):
        self.num_default_row = N

    def get_dataset(self,data_name):
        if data_name in self.h5f:
            return self.h5f[data_name]
        assert(data_name in self.dtypes)
        nc = self.num_channels[data_name]
        dset = self.h5f.create_dataset(data_name,shape=(self.num_default_row,nc),\
                                    maxshape=(None,nc),dtype=self.dtypes[data_name],\
                                    chunks = (self.h5_num_row_1M,nc),\
                                    compression = "gzip")
        dset.attrs['valid_num'] = 0
        setattr(self,data_name+'_dset',dset)
        if 'element_names' not in self.h5f.attrs:
            self.h5f.attrs['element_names'] = [data_name]
        else:
            self.h5f.attrs['element_names'] = [data_name]+[e for e in self.h5f.attrs['element_names']]
        return dset
    def get_total_num_channels_name_list(self):
        total_num_channels = 0
        data_name_list = [str(dn) for dn in self.h5f]
        for dn in data_name_list:
            total_num_channels += self.num_channels[dn]

        return total_num_channels,data_name_list

    def append_to_dset(self,dset_name,new_data):
       self.add_to_dset(dset_name,new_data,None,None)

    def get_all_dsets(self,start_idx,end_idx):
        out_dset_order = ['xyz','color','label','intensity']
        data_list = []
        for dset_name in out_dset_order:
            if dset_name in self.h5f:
                data_k = self.h5f[dset_name][start_idx:end_idx,:]
                data_list.append(data_k)
        data = np.concatenate(data_list,1)
        return data

    def add_to_dset(self,dset_name,new_data,start,end):
        dset = self.get_dataset(dset_name)
        valid_n  = dset.attrs['valid_num']
        if start == None:
            start = valid_n
            end = start + new_data.shape[0]
        if dset.shape[0] < end:
            dset.resize((end,)+dset.shape[1:])
        if valid_n < end:
            dset.attrs['valid_num'] = end
        if new_data.ndim==1 and dset.ndim==2 and dset.shape[1]==1:
            new_data = np.expand_dims(new_data,1)
        dset[start:end,:] = new_data

    def rm_invalid(self):
        for dset_name in self.h5f:
            dset = self.h5f[dset_name]
            if 'valid_num' in dset.attrs:
                valid_num = dset.attrs['valid_num']
                if valid_num < dset.shape[0]:
                    dset.resize( (valid_num,dset.shape[1:]) )

    def get_summary_info(self):
        for dset_name in self.h5f:
            setattr(self,dset_name+'_dset',self.h5f[dset_name])
        if 'xyz' in self.h5f:
            self.total_row_N = self.xyz_dset.shape[0]
            self.xyz_max = self.xyz_dset.attrs['max']
            self.xyz_min = self.xyz_dset.attrs['min']
            self.xyz_scope = self.xyz_max - self.xyz_min


    def generate_objfile(self,obj_file_name=None,IsLabelColor=False,xyz_cut_rate=None):
        if obj_file_name==None:
            base_fn = os.path.basename(self.file_name)
            base_fn = os.path.splitext(base_fn)[0]
            folder_path = os.path.dirname(self.file_name)
            obj_folder = os.path.join(folder_path,base_fn)
            print('obj_folder:',obj_folder)
            obj_file_name = os.path.join(obj_folder,base_fn+'.obj')
            if not os.path.exists(obj_folder):
                os.makedirs(obj_folder)
            print('automatic obj file name: %s'%(obj_file_name))


        with open(obj_file_name,'w') as out_obj_file:
            xyz_dset = self.xyz_dset
            color_dset = self.color_dset
            label_category_dset = self.label_category_dset

            if xyz_cut_rate != None:
                # when rate < 0.5: cut small
                # when rate >0.5: cut big
                xyz_max = np.array([ np.max(xyz_dset[:,i]) for i in range(3) ])
                xyz_min = np.array([ np.min(xyz_dset[:,i]) for i in range(3) ])
                xyz_scope = xyz_max - xyz_min
                xyz_thres = xyz_scope * xyz_cut_rate + xyz_min
                print('xyz_thres = ',str(xyz_thres))
            cut_num = 0

            row_step = self.h5_num_row_1M * 10
            row_N = xyz_dset.shape[0]
            for k in range(0,row_N,row_step):
                end = min(k+row_step,row_N)
                xyz_buf_k = xyz_dset[k:end,:]


                color_buf_k = color_dset[k:end,:]
                buf_k = np.hstack((xyz_buf_k,color_buf_k))
                label_k = label_category_dset[k:end,0]
                for j in range(0,buf_k.shape[0]):
                    is_cut_this_point = False
                    if xyz_cut_rate!=None:
                        # cut by position
                        for xyz_j in range(3):
                            if (xyz_cut_rate[xyz_j] >0.5 and buf_k[j,xyz_j] > xyz_thres[xyz_j]) or \
                                (xyz_cut_rate[xyz_j]<=0.5 and buf_k[j,xyz_j] < xyz_thres[xyz_j]):
                                is_cut_this_point =  True
                    if is_cut_this_point:
                        cut_num += 1
                        continue

                    if not IsLabelColor:
                        str_j = 'v   ' + '\t'.join( ['%0.5f'%(d) for d in  buf_k[j,0:3]]) + '  \t'\
                        + '\t'.join( ['%d'%(d) for d in  buf_k[j,3:6]]) + '\n'
                    else:
                        label = label_k[j]
                        label_color = Normed_H5f.g_label2color[label]
                        str_j = 'v   ' + '\t'.join( ['%0.5f'%(d) for d in  buf_k[j,0:3]]) + '  \t'\
                        + '\t'.join( ['%d'%(d) for d in  label_color ]) + '\n'
                    out_obj_file.write(str_j)

                rate = int(100.0 * end / row_N)
                e = row_step / row_N
                if rate > 3 and rate % 3 <= e:
                    print('gen raw obj: %d%%'%(rate))
                if rate > 3:
                    break

    def create_done(self):
        self.rm_invalid()
        self.add_geometric_scope()

    def add_geometric_scope(self,line_num_limit=None):
        ''' calculate the geometric scope of raw h5 data, and add the result to attrs of dset'''
        #begin = time.time()
        max_xyz = -np.ones((3))*1e10
        min_xyz = np.ones((3))*1e10

        xyz_dset = self.xyz_dset
        row_step = self.h5_num_row_1M
        print('File: %s   %d lines'\
              %(os.path.basename(self.file_name),xyz_dset.shape[0]) )
        #print('read row step = %d'%(row_step))

        for k in range(0,xyz_dset.shape[0],row_step):
            end = min(k+row_step,xyz_dset.shape[0])
            xyz_buf = xyz_dset[k:end,:]
            xyz_buf_max = xyz_buf.max(axis=0)
            xyz_buf_min = xyz_buf.min(axis=0)
            max_xyz = np.maximum(max_xyz,xyz_buf_max)
            min_xyz = np.minimum(min_xyz,xyz_buf_min)

            if line_num_limit!=None and k > line_num_limit:
                print('break at k = ',line_num_limit)
                break
        xyz_dset.attrs['max'] = max_xyz
        xyz_dset.attrs['min'] = min_xyz
        self.h5f.attrs['xyz_max'] = max_xyz
        self.h5f.attrs['xyz_min'] = min_xyz
        max_str = '  '.join([ str(e) for e in max_xyz ])
        min_str = '  '.join([ str(e) for e in min_xyz ])
        print('max_str=%s\tmin_str=%s'%(max_str,min_str) )
        #print('T=',time.time()-begin)

    @staticmethod
    def check_rh5_intact( file_name ):
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.rh5'
        if not os.path.exists(file_name):
            return False, "%s not exist"%(file_name)
        #if os.path.getsize( file_name ) / 1000.0 < 100:
        #    return False,"file too small < 20 K"
        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        with h5py.File(file_name,'r') as h5f:
            attrs_to_check = ['xyz_max','xyz_min']
            for attrs in attrs_to_check:
                if attrs not in h5f.attrs:
                    return False, "%s not in %s"%(attrs,file_name)
        return True,""


class Sorted_H5f():
    '''
    (1) sorted: sort Raw_H5f by position to blocks, each block in one dataset.
        The dataset name is the voxel index.
    (2) store all types of data (xyz,color,intensity,label..) together (float32) in one dataset
    (3) All the information are stored in self.h5f.attrs,like:
The root_attr:  [u'datasource_name', u'xyz_max', u'xyz_min', u'element_names', u'stride_to_align', u'block_step', u'block_stride', u'block_dims_N', u'xyz_min_aligned', u'xyz_max_aligned', u'xyz_scope_aligned', u'total_row_N', u'total_block_N']
datasource_name: MATTERPORT
element_names: ['label_material' 'label_instance' 'label_category' 'color' 'nxnynz' 'xyz']
total_row_N: 65536
total_block_N: 8
block_step: [4 4 2]
block_stride: [2 2 2]
block_dims_N: [2 2 2]
xyz_min: [-0.95031667 -2.15375018 -0.01794876]
xyz_max: [ 2.48232388  0.5618372   2.2046504 ]
xyz_min_aligned: [-1.  -2.2 -0.2]
xyz_max_aligned: [ 2.5  0.6  2.3]
xyz_scope_aligned: [ 3.5  2.8  2.5]
    (4) The label_category in Sorted_H5f is raw_category_idx, the label_category in Normed_H5f is mpcat40 index
    (5) The xyz_1norm_file in h5f is normed with file space scale. Can be used directly in training.
        The xyz_midnorm_block may be not useful if larger scale used in training.
        While using larger block scale in training, xyz_1norm_block and xyz_midnorm_block has to be generated online.
    '''
    file_flag = 'SORTED_H5F'
    labels_order = ['label_category','label_instance','label_material']
    #label_candi_eles_len = {'label_category':1,'label_instance':1,'label_material':1}
    data_label_ele_candidates_order = ['xyz','nxnynz','color','label','intensity'] + labels_order
    data_label_ele_candidates_order += ['org_row_index']
    data_label_channels = {'xyz':3,'nxnynz':3,'color':3,'label':1,'label_category':1,'label_instance':1,
                           'label_material':1,'intensity':1,'org_row_index':1,'xyz_1norm_file':3,'xyz_midnorm_block':3,
                           'color_1norm':3}
    IS_CHECK = False # when true, store org_row_index
    data_idxs = {}
    total_num_channels = 0

    actions = ''
    h5_num_row_1M = g_h5_num_row_1M

    def __init__(self,h5f,file_name=None):
        self.h5f = h5f
        if file_name != None:
            self.file_name = file_name
        else:
            self.file_name = None
        self.reduced_num = 0
        self.update_data_index_by_elementnames()
        #self.show_summary_info()


    def show_summary_info(self):
        print('\n\nsummary of file: ',self.file_name)
        show_h5f_summary_info(self.h5f)

    def update_data_index_by_elementnames(self):
        # update by self.h5f.attrs['element_names']
        data_index = {}
        last_index = 0
        if 'element_names' in self.h5f.attrs:
            element_names = self.h5f.attrs['element_names']
        else:
            element_names = []
        if self.IS_CHECK and 'org_row_index' not in element_names:
            element_names += ['org_row_index']

        element_names = set(element_names)
        for dn in self.data_label_ele_candidates_order:
            if dn in element_names:
                data_index[dn] = range(last_index,last_index+self.data_label_channels[dn])
                last_index += self.data_label_channels[dn]
        self.data_idxs = data_index
        self.total_num_channels = last_index

        if 'element_names' in self.h5f.attrs:
            label_set_elements = []
            for e in self.labels_order:
                if e in self.h5f.attrs['element_names']:
                    label_set_elements += [e]
            self.label_set_elements = label_set_elements
            self.label_ele_idxs = self.get_label_ele_ids(label_set_elements)

        if 'datasource_name' in self.h5f.attrs:
            self.DatasetMeta = DatasetMeta(self.h5f.attrs['datasource_name'])
            self.num_classes = self.DatasetMeta.num_classes

    def get_label_ele_ids(self,label_eles):
        label_ele_idxs = {}
        k = 0
        for e in label_eles:
            assert e in self.labels_order
            label_ele_idxs[e] = range(k,k+self.data_label_channels[e])
            k += self.data_label_channels[e]
        return label_ele_idxs
    def get_data_ele_ids(self,data_eles):
        data_ele_idxs = {}
        k = 0
        for e in data_eles:
            assert e in self.data_label_channels, "%s not in self.data_label_channels"%(e)
            data_ele_idxs[e] = range(k,k+self.data_label_channels[e])
            k += self.data_label_channels[e]
        return data_ele_idxs

    def set_step_stride(self,block_step,block_stride,stride_to_align=0.1):
        self.h5f.attrs['block_step'] = block_step
        self.h5f.attrs['block_stride'] = block_stride
        self.h5f.attrs['stride_to_align'] = stride_to_align
        Sorted_H5f.update_align_scope_by_stridetoalign_(self.h5f.attrs)

    @staticmethod
    def set_whole_scene_stride_step(h5fattrs):
        for i in range(0,len(h5fattrs['block_step'])):
            if h5fattrs['block_step'][i]  == -1:
                h5fattrs['block_step'][i] = h5fattrs['xyz_scope_aligned'][i]
            if h5fattrs['block_stride'][i]  == -1:
                h5fattrs['block_stride'][i] = h5fattrs['xyz_scope_aligned'][i]
       # get_attrs_str(h5fattrs)
       # print( h5fattrs['block_stride'] - h5fattrs['xyz_scope_aligned'] )


    @staticmethod
    def update_align_scope_by_stridetoalign_(h5fattrs):
        if 'xyz_min' not in h5fattrs:
            return
        xyz_min = h5fattrs['xyz_min']
        xyz_max = h5fattrs['xyz_max']
        xyz_min_aligned = xyz_min - xyz_min % h5fattrs['stride_to_align'] - [0,0,0.1]
        xyz_max_aligned = xyz_max - xyz_max % 0.1 + 0.1
        xyz_scope_aligned =  xyz_max_aligned - xyz_min_aligned

        # step or stride ==-1 means one step/stride the whole scene
        if 'block_step' in h5fattrs:
            block_step = h5fattrs['block_step']
            block_stride = h5fattrs['block_stride']
            h5fattrs['block_step']  = block_step
            h5fattrs['block_stride'] = block_stride
            Sorted_H5f.set_whole_scene_stride_step(h5fattrs)
            h5fattrs['block_dims_N'] = np.ceil( (xyz_scope_aligned - h5fattrs['block_step']) / h5fattrs['block_stride'] + 1 ).astype(np.int64)
        h5fattrs['xyz_min_aligned'] = xyz_min_aligned
        h5fattrs['xyz_max_aligned'] = xyz_max_aligned
        h5fattrs['xyz_scope_aligned'] = xyz_scope_aligned

    def update_align_scope_by_stridetoalign(self):
        if 'xyz_min' not in self.h5f.attrs:
            return
        xyz_min = self.h5f.attrs['xyz_min']
        xyz_max = self.h5f.attrs['xyz_max']
        xyz_min_aligned = xyz_min - xyz_min % self.h5f.attrs['stride_to_align'] - [0,0,0.1]
        xyz_max_aligned = xyz_max - xyz_max % 0.1 + 0.1
        xyz_scope_aligned =  xyz_max_aligned - xyz_min_aligned

        # step or stride ==-1 means one step/stride the whole scene
        if 'block_step' in self.h5f.attrs:
            block_step = self.h5f.attrs['block_step']
            block_stride = self.h5f.attrs['block_stride']
            self.h5f.attrs['block_step']  = block_step
            self.h5f.attrs['block_stride'] = block_stride
            self.h5f.attrs['block_dims_N'] = np.ceil(xyz_scope_aligned / self.h5f.attrs['block_stride']).astype(np.int64)
            Sorted_H5f.set_whole_scene_stride_step(self.h5f.attrs)
        self.h5f.attrs['xyz_min_aligned'] = xyz_min_aligned
        self.h5f.attrs['xyz_max_aligned'] = xyz_max_aligned
        self.h5f.attrs['xyz_scope_aligned'] = xyz_scope_aligned


    def add_total_row_block_N(self,raw_h5f_total_row_N=None):
        total_row_N = 0
        n = -1
        for n,dn in enumerate( self.h5f ):
            total_row_N += self.h5f[dn].shape[0]

        if raw_h5f_total_row_N != None:
            assert total_row_N == raw_h5f_total_row_N, 'ERROR: blocked total_row_N= %d, raw = %d'%(total_row_N,raw_h5f_total_row_N)
        self.h5f.attrs['total_row_N']=total_row_N
        self.h5f.attrs['total_block_N']=n+1
        print('add_total_row_block_N:  file: %s \n   total_row_N = %d,  total_block_N = %d'%(
            os.path.basename(self.file_name),total_row_N,n+1))
        return total_row_N, n+1

    @staticmethod
    def check_sh5_intact( file_name ):
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.sh5' or f_format == '.rsh5'
        if not os.path.exists(file_name):
            return False,"%s not exist"%(file_name)
        #if os.path.getsize( file_name ) / 1000.0 < 100:
        #    return False,"file too small < 20 K"
        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        with h5py.File(file_name,'r') as h5f:
            if 'is_intact' in h5f.attrs:
                IsIntact = h5f.attrs['is_intact'] == 1
                return IsIntact,""
            else:
                attrs_to_check = ['total_row_N','total_block_N','label_category_hist','label_category_hist1norm']
                for attrs in attrs_to_check:
                    if attrs not in h5f.attrs:
                        return False, "%s not in %s"%(attrs,f_format)
                return True,""

    def add_label_histagram(self):
        label_name = 'label_category'
        if label_name in self.label_ele_idxs:
            label_category_id = self.data_idxs['label_category'][0]
            label_hist  = np.zeros(shape=[self.num_classes]).astype(np.int64)
            for k in self.h5f:
                label_hist_k,_ = np.histogram(self.h5f[k][:,label_category_id],range(self.num_classes+1))
                label_hist += label_hist_k.astype(np.int64)
            label_hist_1norm = label_hist / np.sum(label_hist).astype(np.float)
            self.h5f.attrs[label_name+'_hist'] = label_hist
            self.h5f.attrs[label_name+'_hist1norm'] = label_hist_1norm
            print('adding label hist ok:',label_hist)

    def copy_root_summaryinfo_from_another(self,h5f0,copy_flag):
        attrs = ['datasource_name','xyz_max','xyz_min','element_names','stride_to_align']   # 'new_stride'
        if copy_flag == 'sub' or copy_flag == 'sample':
            attrs += ['block_step','block_stride','block_dims_N','total_block_N']
        if copy_flag == 'all':
            attrs = [a for a in h5f0.attrs]

        for attr in attrs:
            if attr in h5f0.attrs:
                self.h5f.attrs[attr] = h5f0.attrs[attr]
        self.h5f.attrs['is_intact'] = 0
        Sorted_H5f.update_align_scope_by_stridetoalign_(self.h5f.attrs)
        self.update_data_index_by_elementnames()

    def copy_root_attrs_from_raw(self,h5f_raw):
        attrs=['datasource_name','element_names','xyz_max','xyz_min']
        for attr in attrs:
            if attr in h5f_raw.attrs:
                self.h5f.attrs[attr] = h5f_raw.attrs[attr]
        self.h5f.attrs['is_intact'] = 0
        self.update_data_index_by_elementnames()


    @staticmethod
    def block_index_to_xyz_(block_k,attrs):
        ixyz = Sorted_H5f.block_index_to_ixyz_( block_k,attrs )
        xyz = Sorted_H5f.ixyz_to_xyz( ixyz,attrs )
        return xyz
    def block_index_to_ixyz(self,block_k):
        return Sorted_H5f.block_index_to_ixyz_(block_k,self.h5f.attrs)
    @staticmethod
    def block_index_to_ixyz_(block_k,attrs):
        i_xyz = np.zeros(3,np.int64)
        assert 'block_dims_N' in attrs
        block_dims_N = attrs['block_dims_N']
        i_xyz[2] = block_k % block_dims_N[2]
        k = int( block_k / block_dims_N[2] )
        i_xyz[1] = k % block_dims_N[1]
        k = int( k / block_dims_N[1] )
        i_xyz[0] = k % block_dims_N[0]
        return i_xyz
    @staticmethod
    def get_block_dis_(bid0,bid1,attrs):
        ixyz0 = Sorted_H5f.block_index_to_ixyz_(bid0,attrs)
        ixyz1 = Sorted_H5f.block_index_to_ixyz_(bid1,attrs)
        xyz0 = Sorted_H5f.ixyz_to_xyz( ixyz0,attrs )
        xyz1 = Sorted_H5f.ixyz_to_xyz( ixyz1,attrs )
        dis = np.linalg.norm(xyz1-xyz0)
        return dis

    @staticmethod
    def ixyz_to_xyz( ixyz, attrs ):
        xyz =  ixyz * attrs['block_stride'] + attrs['xyz_min_aligned'] + attrs['block_step']*0.5
        return xyz
    @staticmethod
    def xyz_to_ixyz( xyz, attrs ):
        ixyz =  (xyz - attrs['xyz_min_aligned'] - attrs['block_step']*0.5) / attrs['block_stride']
        return ixyz.astype(np.int64)

    def ixyz_to_block_index(self,i_xyz):
        i_xyz = i_xyz.astype(np.uint64)
        block_dims_N = self.h5f.attrs['block_dims_N']
        block_k = int( i_xyz[0]*block_dims_N[1]*block_dims_N[2] + i_xyz[1]*block_dims_N[2] + i_xyz[2] )
        return block_k

    def xyz_to_block_index(self,xyz_k):
        return Sorted_H5f.xyz_to_block_index_( xyz_K, self.h5f.attrs )

    @staticmethod
    def ixyz_to_block_index_(i_xyz,attrs):
        block_dims_N = attrs['block_dims_N']
        assert (i_xyz >= 0).all(),"i_xyz < 0 : %s"%(i_xyz)
        assert (i_xyz < block_dims_N).all(),"i_xyz > block_dims_N %s > %s"%(i_xyz,block_dims_N)
        i_xyz = i_xyz.astype(np.uint64)
        block_k = int( i_xyz[0]*block_dims_N[1]*block_dims_N[2] + i_xyz[1]*block_dims_N[2] + i_xyz[2] )
        return block_k

    @staticmethod
    def xyz_to_block_index_(xyz_k,attrs):
        assert((attrs['block_step'] == attrs['block_stride']).all()),"step != stride,the out k is not unique"
        i_xyz = Sorted_H5f.xyz_to_ixyz( xyz_k, attrs )
        block_k = Sorted_H5f.ixyz_to_block_index_(i_xyz,attrs)
        return block_k, i_xyz

    def get_block_scope_from_k(self,block_k):
        return Sorted_H5f.get_block_scope_from_k_(block_k,self.h5f.attrs)
    @staticmethod
    def get_block_scope_from_k_(block_k,h5fattrs):
        i_xyz = Sorted_H5f.block_index_to_ixyz_(block_k,h5fattrs)
        return Sorted_H5f.get_block_scope_from_ixyz_(i_xyz,h5fattrs)
    @staticmethod
    def get_block_scope_from_ixyz_(i_xyz,h5fattrs):
        block_dims_N = h5fattrs['block_dims_N']
        block_k = int( i_xyz[0]*block_dims_N[1]*block_dims_N[2] + i_xyz[1]*block_dims_N[2] + i_xyz[2] )
        block_min = i_xyz * h5fattrs['block_stride'] + h5fattrs['xyz_min_aligned']
        block_max = block_min + h5fattrs['block_step']
        return block_min,block_max,i_xyz

    def get_attrs_of_new_stride_step(self,new_stride,new_step):
        return Sorted_H5f.get_attrs_of_new_stride_step_(self.h5f.attrs,new_stride,new_step)
    @staticmethod
    def get_attrs_of_new_stride_step_(base_h5fattrs,new_stride,new_step):
        new_sorted_h5f_attrs = copy_h5f_attrs( base_h5fattrs )
        new_sorted_h5f_attrs['block_step'] = np.array(new_step).astype(np.float64)
        new_sorted_h5f_attrs['block_stride'] = np.array(new_stride).astype(np.float64)
        if 'total_block_N' in new_sorted_h5f_attrs:
            del new_sorted_h5f_attrs['total_block_N']
        if 'total_row_N' in new_sorted_h5f_attrs:
            del new_sorted_h5f_attrs['total_row_N']
        Sorted_H5f.update_align_scope_by_stridetoalign_(new_sorted_h5f_attrs)
       # print('new_attrs')
        #get_attrs_str(new_sorted_h5f_attrs)
       # print('\n\norg_attrs')
       # get_attrs_str(self.h5f.attrs)
        return new_sorted_h5f_attrs

    @staticmethod
    def get_blockids_of_dif_stride_step(base_block_id,base_attrs,aim_attrs):
        '''
        1) base_block_id: int, the input block id
           base_attrs: the input stride step size
           aim_attrs:  the outpit (new) stride step size
           return the all the aim block ids (a list)   "within"/"contain" the input "base_block_id"
        2) If aim stride or step are larger, returned blocks contains base block.
            The containing relationship should be totally containing.
           If aim stride or step are smaller, returned blocks are contained within the base block.
        '''
        IsRecordTime = False
        if IsRecordTime:
            t0 = time.time()

        assert (base_attrs['xyz_min_aligned'] == aim_attrs['xyz_min_aligned']).all()
        assert (base_attrs['xyz_max_aligned'] == aim_attrs['xyz_max_aligned']).all()

        def my_fix(orgvar):
            # why do not use np.fix() directly: np.fix(2.999999) = 2.0
            assert orgvar.ndim == 1
            rint_var = np.rint(orgvar)
            zero_gap = rint_var - orgvar
            fix_var = np.copy(orgvar).astype(np.uint64)
            for i in range(orgvar.size):
                if np.isclose(zero_gap[i],0):
                    fix_var[i] = rint_var[i].astype(np.uint64)
                else:
                    fix_var[i] = np.fix(orgvar[i]).astype(np.uint64)
            return fix_var

        def my_ceil(orgvar):
            # why do not use np.ceil: np.ceil(12.0000000000000001)=13
            assert orgvar.ndim == 1
            rint_var = np.rint(orgvar)
            zero_gap = rint_var - orgvar
            ceil_var = np.copy(orgvar).astype(np.uint64)
            for i in range(orgvar.size):
                if np.isclose(zero_gap[i],0):
                    ceil_var[i] = rint_var[i].astype(np.uint64)
                else:
                    ceil_var[i] = np.ceil(orgvar[i]).astype(np.uint64)
            return ceil_var

        i_xyz = Sorted_H5f.block_index_to_ixyz_(base_block_id,base_attrs)
        i_xyz_new_start = i_xyz * base_attrs['block_stride'] / aim_attrs['block_stride']
        ## use rint, if strides are not aligned, padding abs less than half of new step
        #i_xyz_new_start_fixed = np.rint(i_xyz_new_start).astype(np.int32)
        ## use fix, always get more space is not aligned. start padding >= 0
        i_xyz_new_start_fixed = my_fix(i_xyz_new_start).astype(np.int32)
        start_padding = (i_xyz_new_start - i_xyz_new_start_fixed) * aim_attrs['block_stride']
        i_xyz_new_start = i_xyz_new_start_fixed

        max_padding = aim_attrs['block_step'] # max abs padding at both start and end
        assert (start_padding < max_padding).all()
        #print( self.xyz_min_aligned )
        #print( new_sorted_h5f.xyz_min_aligned )
        i_xyz_new_list = []
        block_k_new_list = []

        IsCheck_Scope =  True
        if IsCheck_Scope:
            min_k,max_k,_ = Sorted_H5f.get_block_scope_from_k_(base_block_id, base_attrs)

        #if (base_attrs['block_stride'] > aim_attrs['block_stride']).any() or (base_attrs['block_step'] > aim_attrs['block_step']).any():
        if (base_attrs['block_step'] >= aim_attrs['block_step']).any():
            '''
            find all the small out (aim) blocks within the large (base) input block
            The out dataset is a base dataset in which: block_step_out == block_stride_out
            '''
            assert((base_attrs['block_step'] >= aim_attrs['block_step'] ).all())
            # check stride step aligned:
            #IsExactDivision0 = float_exact_division( base_attrs['block_step'], aim_attrs['block_step'] )
            #assert IsExactDivision0
            #IsExactDivision1 = float_exact_division( base_attrs['block_stride'], aim_attrs['block_stride'] )
            #assert IsExactDivision1

            search = my_ceil( ( base_attrs['block_step'] - aim_attrs['block_step'] ) / aim_attrs['block_stride'] + 1).astype(np.int64)
            for i_x in range(0,search[0]):
                for i_y in range(0,search[1]):
                    for i_z in range(0,search[2]):
                        i_xyz_new = ( i_xyz_new_start + np.array([i_x,i_y,i_z]) ).astype(np.uint64)
                        if (i_xyz_new >= aim_attrs['block_dims_N']).any():
                            # out of global scope: invalid search
                            continue
                        block_k_new = Sorted_H5f.ixyz_to_block_index_(i_xyz_new,aim_attrs)

                        #check scope
                        if IsCheck_Scope:
                            i_xyz_new_tocheck = Sorted_H5f.block_index_to_ixyz_(block_k_new,aim_attrs)
                            ixyz_check = (i_xyz_new == i_xyz_new_tocheck).all()
                            min_k_new,max_k_new,_ = Sorted_H5f.get_block_scope_from_k_(block_k_new,aim_attrs)
                            min_check = ( (min_k - min_k_new) < max_padding ).all()
                            max_check = ( (max_k_new - max_k) < max_padding ).all()
                            if not (min_check and max_check and ixyz_check):
                                print('base step:', base_attrs['block_step'])
                                print('base stride:', base_attrs['block_stride'])
                                print('base xyz_min_aligned:', base_attrs['xyz_min_aligned'])
                                print('aim step:', aim_attrs['block_step'])
                                print('aim stride:', aim_attrs['block_stride'])
                                print('aim xyz_min_aligned:', aim_attrs['xyz_min_aligned'])

                                print('new=small failed i_xyz=',i_xyz,'\t i_xyz_new=',i_xyz_new)
                                if not min_check:
                                    print('\nmin check failed in get_sub_blcok_ks')
                                    print('new min = ',min_k_new,'\norg min = ',min_k)
                                if not max_check:
                                    print('\nmax check failed in get_blockids_of_dif_stride_step 1')
                                    print('new max = ',max_k_new,'\norg max = ',max_k)
                                if not ixyz_check:
                                    print('ixyz check failed, i_xyz_new:',i_xyz_new,'\t i_xyz_new_tocheck:',i_xyz_new_tocheck)
                            assert ixyz_check and min_check and max_check

                        i_xyz_new_list.append(i_xyz_new)
                        block_k_new_list.append(block_k_new)
            #if base_attrs['block_step'][0] == 0.6:
            #    print('0.6')
            #    import pdb; pdb.set_trace()  # XXX BREAKPOINT
            #if not (base_attrs['block_step'] == base_attrs['block_stride']).all():
            #    print('base step != stride')
            #    import pdb; pdb.set_trace()  # XXX BREAKPOINT
        else:
            '''
            find all the large(out) blocks contains the small input block
            check: all xyz_scope_k_new contain xyz_scope_k
            '''
            if not (base_attrs['block_step'] <= aim_attrs['block_step']).all():
                print(  "base_attrs['block_step']:%s \n aim_attrs['block_step']:%s"%(base_attrs['block_step'],aim_attrs['block_step'] ) )
                import pdb; pdb.set_trace()  # XXX BREAKPOINT
                assert False
            assert( (aim_attrs['block_stride'] >= base_attrs['block_step']).all() )

            #assert( ((aim_attrs['block_step'] / base_attrs['block_step'])%1 == 0).all() )
            #assert( ((aim_attrs['block_stride'] / base_attrs['block_step'])%1 == 0).all() )

            #search = ( aim_attrs['block_step'] / aim_attrs['block_stride'] ).astype(np.float64)
            search = ( (aim_attrs['block_step']-base_attrs['block_step']) / aim_attrs['block_stride'] + 1).astype(np.float64)
            if ( search%1*aim_attrs['block_stride'] >= base_attrs['block_step']).all() :
                search = np.ceil(search).astype(np.int64)
            else:
                search = np.trunc(search).astype(np.int64)
            for i_x in range( -search[0]+1,1 ):
                for i_y in range(  -search[1]+1,1  ):
                    for i_z in range(  -search[2]+1,1 ):
                        i_xyz_new = ( i_xyz_new_start + np.array([i_x,i_y,i_z]) ).astype(np.int64)
                        if ( (i_xyz_new < 0).any() or (i_xyz_new >= aim_attrs['block_dims_N']).any() ):
                            continue

                        block_k_new = Sorted_H5f.ixyz_to_block_index_(i_xyz_new,aim_attrs)
                        # check
                        if IsCheck_Scope:
                            min_k_new,max_k_new,_ = Sorted_H5f.get_block_scope_from_k_(block_k_new,aim_attrs)
                            min_check = (min_k_new - min_k < max_padding).all()
                            max_check = (max_k_new - max_k > -max_padding).all()
                        else:
                            min_check = True
                            max_check = True

                        if not min_check & max_check:
                            print('new=large failed i_xyz=',[i_x,i_y,i_z])
                            if not min_check:
                                print('\nmin check failed in get_sub_blcok_ks')
                                print('new min = ',min_k_new,'\norg min = ',min_k)
                            if not max_check:
                                print('\nmax check failed in get_blockids_of_dif_stride_step 2')
                                print('new max = ',max_k_new,'\norg max = ',max_k)
                                import pdb; pdb.set_trace()  # XXX BREAKPOINT
                        else:
                            #print('both min and max check passed, i_xyz= ',[i_x,i_y,i_z])
                            i_xyz_new_list.append(i_xyz_new)
                            block_k_new_list.append(block_k_new)

        if IsRecordTime:
            print('base: ',base_attrs['block_stride'],base_attrs['block_step'])
            print('aim: ',aim_attrs['block_stride'],aim_attrs['block_step'])
            print('t = %f  ms'%(1000.0*(time.time()-t0)))
            print('search = ',search)
            print('\n')
            '''
            base:  [ 1.  1.  1.] [ 2.  2.  2.]
            aim:  [ 0.1  0.1  0.1] [ 0.1  0.1  0.1]
            t = 332.636118  ms
            search =  [20 20 20]
            '''
        return block_k_new_list,i_xyz_new_list


#    def get_sub_block_ks(self,block_k,new_sorted_h5f):
#        '''
#        For the space k in current file,
#        return the corresponding block_ks in a new file with new step and stride
#        block_ks is a list
#        '''
#        i_xyz = self.block_index_to_ixyz(block_k)
#        i_xyz_new_start = i_xyz * self.h5f.attrs['block_stride'] / new_sorted_h5f.h5f.attrs['block_stride']
#        i_xyz_new_start = (i_xyz_new_start).astype(np.int)
#        #print( self.xyz_min_aligned )
#        #print( new_sorted_h5f.xyz_min_aligned )
#        i_xyz_new_list = []
#        block_k_new_list = []
#
#        # for check
#        IsCheck_Scope =  False
#        if IsCheck_Scope:
#            min_k,max_k,_ = self.get_block_scope_from_k(block_k)
#
#        if (self.h5f.attrs['block_step'] > new_sorted_h5f.h5f.attrs['block_step']).any():
#            '''
#            find all the small(out) blocks within the large input block
#            The out dataset is a base dataset in which: block_step_out == block_stride_out
#            '''
#            assert((self.h5f.attrs['block_step'] > new_sorted_h5f.h5f.attrs['block_step'] ).all())
#            assert((new_sorted_h5f.h5f.attrs['block_step'] == new_sorted_h5f.h5f.attrs['block_stride']).all())
#
#            search = np.ceil(self.block_step / new_sorted_h5f.block_step).astype(np.int64)
#            for i_x in range(0,search[0]):
#                for i_y in range(0,search[1]):
#                    for i_z in range(0,search[2]):
#                        i_xyz_new = ( i_xyz_new_start + np.array([i_x,i_y,i_z]) ).astype(np.uint64)
#                        block_k_new = new_sorted_h5f.ixyz_to_block_index(i_xyz_new)
#
#                        #check
#                        if IsCheck_Scope:
#                            min_k_new,max_k_new,_ = new_sorted_h5f.get_block_scope_from_k(block_k_new)
#                            min_check = (min_k_new >= min_k).all()
#                            max_check = (max_k_new <= max_k).all()
#                        else:
#                            min_check = True
#                            max_check = True
#                        if not min_check & max_check:
#                            print('new=small failed i_xyz=',[i_x,i_y,i_z])
#                            if not min_check:
#                                print('\nmin check failed in get_sub_blcok_ks')
#                                print('new min = ',min_k_new,'\norg min = ',min_k)
#                            if not max_check:
#                                print('\nmax check failed in get_sub_blcok_ks 1')
#                                print('new max = ',max_k_new,'\norg max = ',max_k)
#
#                        else:
#                            i_xyz_new_list.append(i_xyz_new)
#                            block_k_new_list.append(block_k_new)
#                            #print('both min and max check passed')
#
#        else:
#            '''
#            find all the large(out) blocks contains the small input block
#            check: all xyz_scope_k_new contain xyz_scope_k
#            '''
#            assert( (self.h5f.attrs['block_step'] <= new_sorted_h5f.h5f.attrs['block_step']).all() )
#            assert( ((new_sorted_h5f.h5f.attrs['block_step'] / self.h5f.attrs['block_step'])%1 == 0).all() )
#            assert( (new_sorted_h5f.h5f.attrs['block_stride'] >= self.h5f.attrs['block_step']).all() )
#            assert( ((new_sorted_h5f.h5f.attrs['block_stride'] / self.h5f.attrs['block_step'])%1 == 0).all() )
#
#            search = ( new_sorted_h5f.h5f.attrs['block_step'] / new_sorted_h5f.h5f.attrs['block_stride'] ).astype(np.float64)
#            if ( search%1*new_sorted_h5f.h5f.attrs['block_stride'] >= self.h5f.attrs['block_step']).all() :
#                search = np.ceil(search).astype(np.int64)
#            else:
#                search = np.trunc(search).astype(np.int64)
#            for i_x in range( -search[0]+1,1 ):
#                for i_y in range(  -search[1]+1,1  ):
#                    for i_z in range(  -search[2]+1,1 ):
#                        i_xyz_new = ( i_xyz_new_start + np.array([i_x,i_y,i_z]) ).astype(np.int64)
#                        if ( (i_xyz_new < 0).any() or (i_xyz_new > new_sorted_h5f.h5f.attrs['block_dims_N']).any() ):
#                            continue
#
#                        block_k_new = new_sorted_h5f.ixyz_to_block_index(i_xyz_new)
#                        # check
#                        if IsCheck_Scope:
#                            min_k_new,max_k_new,_ = new_sorted_h5f.get_block_scope_from_k(block_k_new)
#                            min_check = (min_k_new <= min_k).all()
#                            max_check = (max_k_new >= max_k).all()
#                        else:
#                            min_check = True
#                            max_check = True
#
#                        if not min_check & max_check:
#                            print('new=large failed i_xyz=',[i_x,i_y,i_z])
#                            if not min_check:
#                                print('\nmin check failed in get_sub_blcok_ks')
#                                print('new min = ',min_k_new,'\norg min = ',min_k)
#                            if not max_check:
#                                print('\nmax check failed in get_sub_blcok_ks 2')
#                                print('new max = ',max_k_new,'\norg max = ',max_k)
#
#                        else:
#                            #print('both min and max check passed, i_xyz= ',[i_x,i_y,i_z])
#                            i_xyz_new_list.append(i_xyz_new)
#                            block_k_new_list.append(block_k_new)
#        return block_k_new_list,i_xyz_new_list

    def get_blocked_dset(self,block_k,new_set_default_rows=None,column_N = 9):
        if not type(block_k) is int:
            block_k = int(block_k)

        dset_name = str(block_k)
        if dset_name in self.h5f:
            return self.h5f[dset_name]
        if new_set_default_rows==None:
            new_set_default_rows = self.h5_num_row_1M
        #dset = self.h5f_blocked.create_dataset( dset_name,shape=(new_set_default_rows,n),\
                #maxshape=(None,n),dtype=np.float32,chunks=(self.h5_num_row_1M/5,n) )
        dset = self.h5f.create_dataset( dset_name,shape=(new_set_default_rows,column_N),\
                maxshape=(None,column_N),dtype=np.float32,compression="gzip"  )
        dset.attrs['valid_num']=0
        block_min, block_max,i_xyz = self.get_block_scope_from_k(block_k)
        dset.attrs['i_xyz'] = i_xyz
        dset.attrs['xyz_min'] = block_min
        dset.attrs['xyz_max'] = block_max
        #print('block %s min = %s  max = %s '%(dset_name,block_min,block_max))
        return dset
    def rm_invalid_data(self):
        for dset_name_i in self.h5f:
            dset_i = self.h5f[dset_name_i]
            valid_n = dset_i.attrs['valid_num']
            if dset_i.shape[0] > valid_n:
                #print('resizing block %s from %d to %d'%(dset_name_i,dset_i.shape[0],valid_n))
                dset_i.resize( (valid_n,dset_i.shape[1]) )


    def check_xyz_scope_k(self,block_k):
        '''
        (1) anno-scope == scope_from_k
        (2) xyz data is in scope
        '''
        dset = self.h5f[str(block_k)]
        min_anno = dset.attrs['xyz_min']
        max_anno = dset.attrs['xyz_max']
        min_k,max_k,i_xyz = self.get_block_scope_from_k(block_k)

        e_min = min_anno-min_k
        e_max = max_anno-max_k
        e = np.linalg.norm(e_min) + np.linalg.norm(e_max)
        if e > 1e-5:
            print('block %d scope anno error! '%(block_k),'\nscope_k=\n',[min_k,max_k],'scope_anno=\n',[min_anno,max_anno])
            return False

        xyz = dset[:,0:3]
        xyz_max = xyz.max(axis=0)
        xyz_min = xyz.min(axis=0)
        if (max_k >= xyz_max).all() and (min_k <= xyz_min).all():
            #print('scope checked OK')
            return True
        else:
            if not (min_k <= xyz_min).all():
                print('\nmin check failed')
            if not (max_k >= xyz_max).all():
                print('\nmax check failed')
            print('scope_min=\n',min_k,'\nreal_min=\n',xyz_min)
            print('scope_max=\n',max_k,'\nreal_max=\n',xyz_max)
            print('stride=\n',self.block_stride,'\nstep=\n',self.block_step)
            return False
    def check_xyz_scope(self):
        step = int(self.total_block_N/20)+1
        Flag = True
        n=0
        for i,dset_n in enumerate(self.h5f):
            block_k = int(dset_n)
            if i%step == 0:
                flag = self.check_xyz_scope_k(block_k)
                if not flag:
                    Flag = False
                    print('dset: %s xyz scope check                   failed'%(dset_n))
                else:
                    n += 1
                    pass
                    #print('dset: %s xyz scope check passed'%(dset_n))
        if Flag:
            print('\nall %d dsets  xyz scope check passed\n'%(n))
        return Flag

    def check_equal_to_raw(self,raw_h5f):
        check_flag = True
        for k,block_k in enumerate(self.h5f):
            #print('checing block %s'%(block_k))
            dset_k = self.h5f[block_k]
            step = max(int(dset_k.shape[0]/30),1)
            for i in range(0,dset_k.shape[0],step):
                sorted_d_i = dset_k[i,0:-1]
                raw_k = int(dset_k[i,-1])
                if raw_k < 0 or raw_k > 16777215: # for float32, it is not accurate again
                    continue
                #raw_d_i = np.concatenate(  [raw_xyz_set[raw_k,:],raw_color_set[raw_k,:],raw_label_set[raw_k,:],raw_intensity_set[raw_k,:]] )
                raw_d_i = raw_h5f.get_all_dsets(raw_k,raw_k+1)
                error = raw_d_i - sorted_d_i
                err = np.linalg.norm( error )
                if err != 0:
                    check_flag = False
                    print('\nsorted error:raw_k=%d  block_k=%s,i=%d'%(raw_k,block_k,i))
                    print('raw_data = \n',raw_d_i,'\nsorted_data = \n',sorted_d_i)
                    break
                else:
                    pass
                    #print('equal check passed: block_k=%s,i=%d'%(block_k,i))
#            if flag_k:
#                    print('equal check passed: block_k=%s '%(block_k))
#            else:
#                    print('equal check failed: block_k=%s '%(block_k))
        return check_flag

    def append_to_dset(self,aim_block_k,source_dset,vacant_size=0,IsSample=False,sample_num=None):
        '''
        if append frequently to one dataset, vacant_size > 0 to avoid frequent resize
        '''

        source_N = source_dset.shape[0]
        if IsSample:
            sample_choice,reduced_num = get_sample_choice(source_N,sample_num)
            self.reduced_num += reduced_num
            #sample_choice = np.sort(sample_choice)
            new_row_N = sample_choice.size
        else:
            new_row_N = source_N

        aim_dset = self.get_blocked_dset(aim_block_k,vacant_size,self.total_num_channels)
        assert aim_dset.shape[-1] == source_dset.shape[-1], "The num_channels may be wrong (in append_to_dset)"
        row_step = self.h5_num_row_1M * 10
        org_row_N = aim_dset.attrs['valid_num']
        aim_dset.resize((org_row_N+new_row_N+vacant_size,aim_dset.shape[1]))
        for k in range(0,new_row_N,row_step):
            end = min(k+row_step,new_row_N)
            if IsSample == False:
                dset_buf = source_dset[k:end,:]
                self.raw_category_idx_2_mpcat40(dset_buf)
                aim_dset[org_row_N+k:org_row_N+end,:] =  dset_buf
            else:
                choice_k = sample_choice[k:end]
                dset_buf = source_dset[choice_k.min():choice_k.max()+1,:]
                self.raw_category_idx_2_mpcat40(dset_buf)
                aim_dset[org_row_N+k:org_row_N+end,:] = dset_buf[choice_k-choice_k.min(),:]
            aim_dset.attrs['valid_num'] = end + org_row_N

    def raw_category_idx_2_mpcat40(self,data_labels_with_rawcategory):
        if self.h5f.attrs['datasource_name']=='MATTERPORT' and 'label_category' in self.data_idxs:
            assert data_labels_with_rawcategory.ndim == 2
            raw_category_idx = self.data_idxs['label_category'][0]
            data_labels_with_rawcategory[:,raw_category_idx] = get_cat40_from_rawcat(data_labels_with_rawcategory[:,raw_category_idx])

    def generate_one_block_to_object(self,block_k,out_obj_file,IsLabelColor=False):
        row_step = self.h5_num_row_1M * 10
        dset_k = self.get_blocked_dset(block_k)
        row_N = dset_k.shape[0]
        for k in range(0,row_N,row_step):
            end = min(k+row_step,row_N)
            buf_k = dset_k[k:end,:]
            #buf_k[:,0:3] -= middle
            for j in range(0,buf_k.shape[0]):
                if not IsLabelColor:
                    str_j = 'v ' + ' '.join( ['%0.3f'%(d) for d in  buf_k[j,0:3]]) + ' \t'\
                    + ' '.join( ['%d'%(d) for d in  buf_k[j,3:6]]) + '\n'
                else:
                    label = buf_k[j,self.data_idxs['label'][0]]
                  #  if label == 0:
                  #      continue
                    label_color = Normed_H5f.g_label2color_dic[self.h5f.attrs['datasource_name']][label]
                    str_j = 'v ' + ' '.join( ['%0.3f'%(d) for d in  buf_k[j,0:3]]) + ' \t'\
                    + ' '.join( ['%d'%(d) for d in  label_color ]) + '\n'

                out_obj_file.write(str_j)

    def gen_file_obj(self,IsLabelColor=False):
        if self.file_name == None:
            print('set file_name (gen_file_obj)')
            return
        base_fn = os.path.basename(self.file_name)
        base_fn = os.path.splitext(base_fn)[0]
        folder_path = os.path.dirname(self.file_name)
        obj_folder = os.path.join(folder_path,base_fn)
        print('obj path:',obj_folder)
        if not os.path.exists(obj_folder):
            os.makedirs(obj_folder)

        aim_scope = np.array([[-30,-30,-20],[20,20,50]])
        aim_scope = None
        n = 0
        last_rate = -20
        out_info_fn = os.path.join(obj_folder,'info.txt')
        with open(out_info_fn,'w') as info_f:
            for dset_name in self.h5f:
                row_N = self.h5f[dset_name].shape[0]

                min_i = self.h5f[dset_name].attrs['xyz_min']
                max_i = self.h5f[dset_name].attrs['xyz_max']
                if aim_scope == None:
                    IsInScope = True
                else:
                    IsInScope = (min_i > aim_scope[0,:]).all() and ( max_i < aim_scope[1,:]).all()
                if not IsInScope:
                    continue
                if IsLabelColor:
                    name_meta = 'labeled_'
                else:
                    name_meta = ''
                out_fn = os.path.join(obj_folder,name_meta+dset_name+'_'+str(row_N)+'.obj')
                with open(out_fn,'w') as out_f:
                    self.generate_one_block_to_object(dset_name,out_f,IsLabelColor)
                n += row_N
                rate = 100.0 * n / self.h5f.attrs['total_row_N']
                if int(rate) % 2 == 0 and rate - last_rate > 3:
                    last_rate = rate
                    print('%0.2f%% generating file: %s'%(rate,os.path.basename(out_fn)) )

                info_str = 'dset: %s \tN= %d   \tmin=%s   \tmax=%s \n'%(dset_name,self.h5f[dset_name].shape[0], np.array_str(min_i), np.array_str(max_i)  )
                info_f.write(info_str)
                #print(info_str)
                #if rate > 30:
                    #break
    def extract_sub_area(self,sub_xyz_scope,sub_file_name):
        with h5py.File(sub_file_name,'w') as sub_h5f:
            sub_f = Sorted_H5f(sub_h5f,sub_file_name)

            sub_f.copy_root_summaryinfo_from_another(self.h5f,'sub')
            sub_f.set_step_stride(self.block_step,self.block_stride)
            for dset_name_i in self.h5f:
                xyz_min_i = self.h5f[dset_name_i].attrs['xyz_min']
                xyz_max_i = self.h5f[dset_name_i].attrs['xyz_max']
                if (xyz_min_i > sub_xyz_scope[0,:]).all() and (xyz_max_i < sub_xyz_scope[1,:]).all():
                    sub_f.get_blocked_dset(dset_name_i,0)
                    sub_f.append_to_dset(dset_name_i,self.h5f[dset_name_i])
            sub_f.add_total_row_block_N()


    def file_random_sampling(self,sample_num,gen_norm=False,gen_obj=False,min_keep_rate=0.01):
        '''
        automatically create a folder in uper directory to store sampled files
        '''
        # randomly select n points
        out_folder = os.path.dirname(self.file_name)+'_'+str(sample_num)
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)
        file_name_base = os.path.splitext(os.path.basename(self.file_name))[0]
        sampled_filename = os.path.join(out_folder,file_name_base+'.rsh5')

        print('start genrating sampled file: ',sampled_filename)
        ave_dset_num = self.h5f.attrs['total_row_N'] /  self.h5f.attrs['total_block_N']
        print('ave_org_num = ',ave_dset_num)
        print('sample_num = %d   %d%%'%(sample_num,100.0*sample_num/ave_dset_num) )
        with h5py.File(sampled_filename,'w') as sampled_h5f:
            sampled_sh5f = Sorted_H5f(sampled_h5f,sampled_filename)
            sampled_sh5f.copy_root_summaryinfo_from_another(self.h5f,'sample')
            #sampled_sh5f.set_root_attr('sample_num',sample_num)
            for i, k_str in enumerate( self.h5f ):
                dset_k = self.h5f[k_str]
                if dset_k.shape[0] < sample_num*min_keep_rate:
                    continue
                sampled_sh5f.append_to_dset(int(k_str),dset_k,vacant_size=0,\
                                            IsSample=True,sample_num=sample_num)
            sampled_sh5f.add_total_row_block_N()
            print('reduced_num = %d  %d%%'%(sampled_sh5f.reduced_num,100.0*sampled_sh5f.reduced_num/self.h5f.attrs['total_row_N'] ))
            reduced_block_N = self.h5f.attrs['total_block_N'] - sampled_sh5f.h5f.attrs['total_block_N']
            print('reduced block num = %d  %d%%'%(reduced_block_N,100*reduced_block_N/self.h5f.attrs['total_block_N']))

            if gen_obj:
               sampled_sh5f.gen_file_obj()
            if gen_norm:
                sampled_sh5f.file_normalize_to_NormedH5F()


    def get_sample_shape(self):
            for i,k_str in  enumerate(self.h5f):
                dset = self.h5f[k_str]
                return dset.shape

    @staticmethod
    def norm_xyz(raw_xyz,h5fattrs,block_id,norm_list,out_norm_data_dic):
        block_min,block_max,_ = Sorted_H5f.get_block_scope_from_k_(block_id,h5fattrs)
        if norm_list==None or 'xyz_1norm_file' in norm_list:
            # used by QI
            # 1norm within the whole scene
            # use by QI in indoor. Since room scale is not large, this is fine.
            # For outdoor,a scene could be too large, maybe not a good choice
            IsUseAligned = True
            if IsUseAligned:
                file_scene_zero = h5fattrs['xyz_min_aligned']
                file_scene_scope = h5fattrs['xyz_max_aligned'] - h5fattrs['xyz_min_aligned']
            else:
                file_scene_zero = h5fattrs['xyz_min']
                file_scene_scope = h5fattrs['xyz_max'] - h5fattrs['xyz_min']
            xyz_1norm_file = (raw_xyz - file_scene_zero) / file_scene_scope
            out_norm_data_dic['xyz_1norm_file'] = xyz_1norm_file
        if norm_list==None or 'xyz_1norm_block' in norm_list:
            # 1norm within the block
            block_scope = block_max - block_min
            xyz_1norm_block = (raw_xyz-block_min) / block_scope
            out_norm_data_dic['xyz_1norm_block'] = xyz_1norm_block

        # xyz_midnorm
        if norm_list==None or 'xyz_midnorm_block' in norm_list:
            xyz_midnorm_block = np.copy( raw_xyz ) # as a new variable, not a reference
            # only norm x,y. Keep z be the raw value
            #xyz_min_real = np.min(raw_xyz,axis=0)
            #xyz_midnorm_block[:,0:2] -= (xyz_min_real[0:2] + self.block_step[0:2]/2)  # used by QI
            block_mid = (block_min + block_max ) / 2
            xyz_midnorm_block[:,0:2] -= block_mid[0:2]  # I think is better
            # for z, just be positive
            xyz_midnorm_block[:,2] -= h5fattrs['xyz_min'][2]
            out_norm_data_dic['xyz_midnorm_block'] = xyz_midnorm_block

    def normalize_dset(self,block_k_str,xyz_1norm_scale='file'):
        '''
        (1) xyz/max
        (2) xy-min-block_size/2  (only xy)
        (3) color / 255
        '''
        raw_dset_k = self.h5f[block_k_str]

        norm_data_dic = {}
        raw_xyz = raw_dset_k[:,self.data_idxs['xyz']]
        norm_data_dic['xyz'] = raw_xyz
        norm_data_dic['nxnynz'] = raw_dset_k[:,self.data_idxs['nxnynz']]
      #  #  xyz_1norm
      #  if xyz_1norm_scale == 'file': # used by QI
      #      # 1norm within the whole scene
      #      # use by QI in indoor. Since room scale is not large, this is fine.
      #      # For outdoor,a scene could be too large, maybe not a good choice
      #      IsUseAligned = True
      #      if IsUseAligned:
      #          file_scene_zero = self.h5f.attrs['xyz_min_aligned']
      #          file_scene_scope = self.h5f.attrs['xyz_max_aligned'] - self.h5f.attrs['xyz_min_aligned']
      #      else:
      #          file_scene_zero = self.h5f.attrs['xyz_min']
      #          file_scene_scope = self.h5f.attrs['xyz_max'] - self.h5f.attrs['xyz_min']
      #      xyz_1norm = (raw_xyz - file_scene_zero) / file_scene_scope
      #  elif 'block' in xyz_1norm_scale:
      #      # 1norm within the block
      #      block_scope = raw_dset_k.attrs['xyz_max'] - raw_dset_k.attrs['xyz_min']
      #      xyz_1norm = (raw_xyz-raw_dset_k.attrs['xyz_min']) / block_scope

      #  # xyz_midnorm
      #  xyz_midnorm = raw_xyz+0 # as a new variable, not a reference
      #  # only norm x,y. Keep z be the raw value
      #  #xyz_min_real = np.min(raw_xyz,axis=0)
      #  #xyz_midnorm[:,0:2] -= (xyz_min_real[0:2] + self.block_step[0:2]/2)  # used by QI
      #  block_mid = (raw_dset_k.attrs['xyz_min'] + raw_dset_k.attrs['xyz_max'] ) / 2
      #  xyz_midnorm[:,0:2] -= block_mid[0:2]  # I think is better
      #  # for z, just be positive
      #  xyz_midnorm[:,2] -= self.h5f.attrs['xyz_min'][2]

        xyz_norm_list = ['xyz_1norm_file','xyz_midnorm_block']
        Sorted_H5f.norm_xyz(raw_xyz,self.h5f.attrs,block_k_str,xyz_norm_list,norm_data_dic)
      #  norm_data_dic['xyz_midnorm_block'] = xyz_midnorm
      #  norm_data_dic['xyz_1norm_'+xyz_1norm_scale] = xyz_1norm

        # color_1norm
        if 'color' in self.data_idxs:
            color_1norm = raw_dset_k[:,self.data_idxs['color']] / 255.0
            norm_data_dic['color_1norm']=color_1norm


        # intensity_1norm
        if 'intensity' in self.data_idxs:
            # ETH senmantic3D intensity range from -2047 to 2048
            intensity = raw_dset_k[:,self.data_idxs['intensity']]
            intensity_1norm = (intensity+2047)/(2048+2047)
            norm_data_dic['intensity_1norm']=intensity_1norm

        return norm_data_dic

    def add_normalize_to_self(self,IsShowSummaryFinished=False):
        xyz_1norm_scale = 'file'
        IsAddColor1Norm = ('color' in self.h5f) and ('color_1norm' not in self.h5f)
        IsAddXyzNorm = ('xyz' in self.h5f) and ('xyz_1norm_file' not in self.h5f)
        if IsAddColor1Norm:
            self.h5f.attrs['element_names'] += ['color1norm']
        for i,k_str in  enumerate(self.h5f):
            dset_i = self.h5f[k_str]
            if IsAddColor1Norm:
                color = dset_i[...,self.data_idxs['color']]
                color1norm = color / 255.0



            norm_data_dic = self.normalize_dset(k_str,xyz_1norm_scale)

            norm_data_list = []
            for data_name in Normed_H5f.normed_ele_idx_order:
                if data_name in norm_data_dic:
                    norm_data_list.append(norm_data_dic[data_name])
            data_norm = np.concatenate( norm_data_list,1 )

            label_eles = [lb for lb in Normed_H5f.labels_order if lb in self.h5f.attrs['element_names']]
            labels = []
            for label_e_name in label_eles:
                label_e_d = raw_dset_k[:,self.data_idxs[label_e_name][0]]
                labels.append(np.expand_dims(label_e_d,axis=-1))
            labels = np.concatenate(labels,axis=-1)


            normed_h5f.append_to_dset('data',normed_data_i)
            normed_h5f.append_to_dset('labels',normed_labels_i)
            #normed_h5f.append_to_dset('blockid',int(k_str))
            normed_h5f.create_done()
            if IsShowSummaryFinished:
                normed_h5f.show_summary_info()
            print('normalization finished: data shape: %s'%(str(normed_h5f.data_set.shape)) )

    def file_normalize_to_NormedH5F(self,IsShowSummaryFinished=False):
        '''
        automatically create a folder in uper directory to store sampled files
        '''
        xyz_1norm_scale = 'file'

        out_folder = os.path.dirname(self.file_name)+'_normed'
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)
        file_name_base = os.path.splitext(os.path.basename(self.file_name))[0]
        normalized_filename = os.path.join(out_folder,file_name_base+'.nh5')

        print('start gen normalized file: ',normalized_filename)
        with h5py.File(normalized_filename,'w') as h5f:
            normed_h5f = Normed_H5f(h5f,normalized_filename,self.h5f.attrs['datasource_name'])
            for i,k_str in  enumerate(self.h5f):
                dset = self.h5f[k_str]
                h5f_sorted_shape = dset.shape
                break
            sample_num =  (h5f_sorted_shape[0],)
            normed_h5f.copy_root_attrs_from_sorted(self.h5f.attrs,sample_num,self.IS_CHECK)

            for i,k_str in  enumerate(self.h5f):
                norm_data_dic = self.normalize_dset(k_str,xyz_1norm_scale)

                norm_data_list = []
                for data_name in Normed_H5f.normed_ele_idx_order:
                    if data_name in norm_data_dic:
                        norm_data_list.append(norm_data_dic[data_name])
                normed_data_i = np.concatenate( norm_data_list,1 )

                label_eles = [lb for lb in Normed_H5f.labels_order if lb in self.h5f.attrs['element_names']]
                labels = []
                for label_e_name in label_eles:
                    label_e_d = raw_dset_k[:,self.data_idxs[label_e_name][0]]
                    labels.append(np.expand_dims(label_e_d,axis=-1))
                labels_i = np.concatenate(labels,axis=-1)


                normed_h5f.append_to_dset('data',normed_data_i)
                normed_h5f.append_to_dset('labels',labels_i,IsLabelWithRawCategory=False)
                #normed_h5f.append_to_dset('blockid',int(k_str))
            normed_h5f.create_done()
            if IsShowSummaryFinished:
                normed_h5f.show_summary_info()
            print('normalization finished: data shape: %s'%(str(normed_h5f.data_set.shape)) )

    def merge_to_new_step(self,larger_stride,larger_step,out_folder,more_actions_config=None):
        '''
        merge blocks of sorted raw h5f to get new larger step / stride
        '''
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)
        new_name = os.path.join(out_folder,os.path.basename(self.file_name))
        print('new file: ',new_name)
        #if os.path.exists(new_name):
        #    print('already exists, skip')
        #    return
        with  h5py.File(self.file_name,'r') as base_h5f:
            with h5py.File(new_name,'w') as new_h5f:
                new_sh5f = Sorted_H5f(new_h5f,new_name)
                new_sh5f.copy_root_summaryinfo_from_another(base_h5f,'new_stride')
                new_sh5f.set_step_stride(larger_step,larger_stride)

                read_row_N = 0
                rate_last = -10
                print('%d rows and %d blocks to merge'%(self.h5f.attrs['total_row_N'],self.h5f.attrs['total_block_N']))
                for dset_name in  base_h5f:
                    block_i_base = int(dset_name)
                    base_dset_i = base_h5f[dset_name]
                    #block_k_new_ls,i_xyz_new_ls = self.get_sub_block_ks(block_i_base,new_sh5f)
                    block_k_new_ls,i_xyz_new_ls = Sorted_H5f.get_blockids_of_dif_stride_step(block_i_base,self.h5f.attrs,new_sh5f.h5f.attrs)

                    read_row_N += base_dset_i.shape[0]
                    rate = 100.0 * read_row_N / self.h5f.attrs['total_row_N']
                    if int(rate)%10 < 1 and rate-rate_last>5:
                        rate_last = rate
                        print(str(rate),'%   ','  dset_name = ',dset_name, '  new_k= ',block_k_new_ls,'   id= ',os.getpid())
                        new_sh5f.h5f.flush()

                    for block_k_new in block_k_new_ls:
                        new_sh5f.append_to_dset(block_k_new,base_dset_i)
                    #if rate > 5:
                        #break
                if read_row_N != self.h5f.attrs['total_row_N']:
                    print('ERROR!!!  total_row_N = %d, but only read %d'%( self.h5f.attrs['total_row_N'],read_row_N))

                total_block_N = 0
                total_row_N = 0
                for total_block_N,dn in enumerate(new_sh5f.h5f):
                    total_row_N += new_sh5f.h5f[dn].shape[0]
                total_block_N += 1
                new_sh5f.h5f.attrs['total_row_N']=total_row_N
                new_sh5f.h5f.attrs['total_block_N']=total_block_N
                new_sh5f.h5f.attrs['is_intact'] = 1
                print('total_row_N = ',total_row_N)
                print('total_block_N = ',total_block_N)
                new_sh5f.h5f.flush()

                #new_sh5f.check_xyz_scope()

                if more_actions_config != None:
                    actions = more_actions_config['actions']
                    if 'obj_merged' in actions:
                        new_sh5f.gen_file_obj(True)
                        new_sh5f.gen_file_obj(False)
                    if 'sample_merged' in actions:
                        Is_gen_obj = 'obj_sampled_merged' in actions
                        Is_gen_norm = 'norm_sampled_merged' in actions
                        new_sh5f.file_random_sampling(more_actions_config['sample_num'],\
                                            gen_norm=Is_gen_norm,gen_obj = Is_gen_obj)


    #***************************************************************************
    #Net feed utils: extract data from unsampled sorted dataset
    #***************************************************************************
    def get_blockids_of_dif_stride_step_byxyz( self,xyz1norm_k, new_stride, new_step ):
        '''
        1) new stride and step is larger than current
        2) get the new_blockid with new stride and step include xyz_k
        3) get all the current block ids included within the new block
        '''
        new_sorted_h5f_attrs = self.get_attrs_of_new_stride_step(new_stride,new_step)
        xyz_k = np.array(xyz1norm_k) * self.h5f.attrs['xyz_scope_aligned'] + self.h5f.attrs['xyz_min_aligned']
        new_block_id,new_ixyz = Sorted_H5f.xyz_to_block_index_(xyz_k,new_sorted_h5f_attrs)
       # print(xyz_k)
       # print(cur_block_id)
       # print(cur_ixyz)

        cur_block_id_ls,cur_i_xyz_ls = Sorted_H5f.get_blockids_of_dif_stride_step(new_block_id,new_sorted_h5f_attrs,self.h5f.attrs)
        print(cur_block_id_ls)
        return cur_block_id_ls,cur_i_xyz_ls,new_block_id

    def get_block_data_of_new_stride_step_byxyz1norm( self,xyz1norm_k, new_stride,new_step,
                                          feed_data_elements=['xyz_midnorm'],feed_label_elements=['label_category'], sample_num=None ):
        xyz_k = np.array(xyz1norm_k) * self.h5f.attrs['xyz_scope_aligned'] + self.h5f.attrs['xyz_min_aligned']
        return self.get_block_data_of_new_stride_step_byxyz(xyz_k,new_stride,new_step,feed_data_elements,feed_label_elements,sample_num)
    def get_block_data_of_new_stride_step_byxyz( self,xyz_k, new_stride,new_step,
                                          feed_data_elements=['xyz_midnorm'],feed_label_elements=['label_category'] ):
        new_sorted_h5f_attrs = self.get_attrs_of_new_stride_step(new_stride,new_step)
        new_block_id,new_ixyz = Sorted_H5f.xyz_to_block_index_(xyz_k,new_sorted_h5f_attrs)
        return self.get_block_data_of_new_stride_step_byid(new_block_id,new_sorted_h5f_attrs,feed_data_elements,feed_label_elements)

    def get_numpoint_of_new_stride_step_byid( self,new_block_id, new_sorted_h5f_attrs,gsbb):
        root_bids_in_cas0 = gsbb.get_baseids_inanew(0,new_block_id)
        num_point_all = 0
        for cur_block_id in root_bids_in_cas0:
            num_point_all += self.h5f[str(cur_block_id)].shape[0]
        return num_point_all


    def remove_void(self,feed_data,feed_label):
        if self.h5f.attrs['datasource_name'] == 'MATTERPORT':
            NoneVoidIndices, = np.nonzero( feed_label[ :,self.label_ele_idxs['label_category'][0] ] )
            void_num = feed_data.shape[0] - NoneVoidIndices.size
            feed_data = feed_data[ NoneVoidIndices,: ]
            feed_label = feed_label[ NoneVoidIndices,: ]
            #if void_num>0:
            #    print('void num = %d'%void_num)
        return feed_data, feed_label

    def get_block_data_of_new_stride_step_byid( self,root_bids_in_cas0, feed_data_elements, feed_label_elements, new_block_id=None, new_sorted_h5f_attrss=None ):
        # feed data and label ele orders are stored according to feed_data_elements and feed_label_elements
        datas = []
        labels = []
        #root_bids_in_cas0 = gsbb.get_baseids_inanew(0,new_block_id)
        for cur_block_id in root_bids_in_cas0:
           # if str(cur_block_id) not in self.h5f:
           #     continue
            dset = self.h5f[str(cur_block_id)]
            feed_label_ids_inall = []
            for le in feed_label_elements:
                feed_label_ids_inall += self.data_idxs[le]
            dset_data = dset[...]
            raw_xyz = dset_data[...,self.data_idxs['xyz']]
            feed_label = dset_data[:,feed_label_ids_inall].astype(np.int32)

            norm_data_dic = {}
            if 'xyz' in feed_data_elements:
                norm_data_dic['xyz'] = raw_xyz
            if 'nxnynz' in feed_data_elements:
                norm_data_dic['nxnynz'] = dset_data[...,self.data_idxs['nxnynz']]
            # cal normalizaed data
            if 'xyz_1norm_file' in feed_data_elements or 'xyz_midnorm_block' in feed_data_elements:
                assert new_block_id!= None
                Sorted_H5f.norm_xyz(raw_xyz, new_sorted_h5f_attrs, new_block_id, feed_data_elements, norm_data_dic)
            if 'color_1norm' in feed_data_elements:
                color = dset[...,self.data_idxs['color']]
                color_1norm = color/255.0
                norm_data_dic['color_1norm'] = color_1norm

            feed_data = np.concatenate( [norm_data_dic[de] for de in feed_data_elements],axis=-1 )
            feed_data, feed_label = self.remove_void( feed_data, feed_label )

            datas.append(feed_data)
            labels.append(feed_label)
        datas = np.concatenate(datas,axis=0)
        labels = np.concatenate(labels,axis=0)

        return datas, labels

    def get_data_larger_block( self,global_block_id,gsbb,feed_data_elements,feed_label_elements, global_num_point, max_rootb_num ):
        '''
        1) global block is the learning block unit. Use current stride and step as base block units.
        2) ( corresponding to farest distance sampling ) Within each global block, select npoint sub-points. Each sub-point is the center of a sub-block. The sub-block stride and step is manually  set to ensure all valid space is used.
        Use 0.1 stride and 0.1 step block as base blocks. All the base block centers are candidate sub-points. Randomly select nsubblock points from all candidate sub-points.
        3) Get sub-group data for each sub-point.

        * Return:
            sampled_xyzs: [nsubblock,xyz] the sampled points in global block
            global_block_datas: [ nsubblock,npoint_subblock,data_nchannel ]
            global_block_labels: [ nsubblock,npoint_subblock,label_nchannel ]

        * Check + Problem:
            If nsubblock and sub_block_size are reasonable, to ensure all valid space is utilized, and no base block id is missed.
        '''
        h5f = self.h5f
        # (1) SAMPLE: Use all the center of base blocks as candidate sub-points
        base_cascadeid = gsbb.base_cascade_ids['global']
        assert base_cascadeid == 'root'
        #nsubblock = int(gsbb.nsubblock_candis[base_cascadeid])
        #npoint_subblock = gsbb.npoint_subblock_candis[base_cascadeid]
        #rootb_attrs = gsbb.root_h5fattrs
        root_bids_in_global = gsbb.get_baseids_inanew('global',global_block_id)
        #root_all_base_bids_indic = gsbb.get_all_base_blockids_indic('')

        # (2) GROUP: Collect all the base blocks for each sub-point.
        global_block_datas = []
        global_block_labels = []
        rootb_split_idxmap = []
        sum_global_point_num = 0
        for root_bid_index,root_bid in enumerate( root_bids_in_global ):
            datas_k, labels_k = self.get_block_data_of_new_stride_step_byid( [root_bid], feed_data_elements, feed_label_elements )
            num_point_k = datas_k.shape[0]
            if num_point_k !=0:
                global_block_datas.append( datas_k )
                global_block_labels.append( labels_k )
                sum_global_point_num += num_point_k
                rootb_split_idxmap.append( np.expand_dims( np.array([root_bid, sum_global_point_num]),0 ) )

        if len( global_block_datas )==0:
            # all void points
            return np.array([]),None,None

        global_block_datas = np.concatenate(global_block_datas,axis=0).astype( np.float32 )
        global_block_labels = np.concatenate(global_block_labels,axis=0).astype( np.int32 )
        rootb_split_idxmap = np.concatenate(rootb_split_idxmap,axis=0)

        global_block_datas, global_block_labels, rootb_split_idxmap, global_sampling_meta = Sorted_H5f.down_sample_bsplit_idxmap( global_block_datas, global_block_labels, rootb_split_idxmap, global_num_point )

        # fix root b num
        assert max_rootb_num >= rootb_split_idxmap.shape[0]
        rootb_split_idxmap_fixed = (np.ones( shape=(max_rootb_num,2) ) * (-1)).astype(np.int32)
        rootb_split_idxmap_fixed[0:rootb_split_idxmap.shape[0],:] = rootb_split_idxmap

       # raw_point_num = global_block_datas.shape[0]
       # choices = random_choice( range(raw_point_num), global_num_point )
       # global_block_datas = global_block_datas[ choices,... ]
       # global_block_labels = global_block_labels[ choices,... ]


        #choices = random_choice( np.arange(len(global_sg_bidxmaps)), nsubblock, keeporder=True )
        #global_sg_bidxmaps0 = global_sg_bidxmaps[ choices,: ]

        return global_block_datas, global_block_labels, rootb_split_idxmap_fixed, global_sampling_meta

    @staticmethod
    def down_sample_bsplit_idxmap( data, label,  bsplit_idxmap, sample_num ):
        org_num = bsplit_idxmap[-1,1]
        assert org_num == data.shape[0]
        sampling_meta = {}
        if org_num <= sample_num:
            data_tile = np.tile( data[org_num-1:org_num,:],[sample_num-org_num,1] )
            data = np.concatenate( [data,data_tile], 0 )
            label_tile = np.tile( label[org_num-1:org_num,:],[sample_num-org_num,1] )
            label = np.concatenate( [label,label_tile], 0 )
            bsplit_idxmap[-1,1] = sample_num
            sampling_meta['miss_rootb_num'] = 0
        else:
            del_choice = np.sort( random_choice(  np.arange(org_num), org_num - sample_num, keeporder=True ) )
            data = np.delete( data, del_choice, axis=0 )
            label = np.delete( label, del_choice, axis=0 )

            # update bsplit_idxmap after downsampling
            del_num_eachb = np.zeros( shape=(bsplit_idxmap.shape[0]) )
            del_bidx = 0
            for del_point_idx in del_choice:
                while del_point_idx >= bsplit_idxmap[del_bidx,1]:
                    del_bidx += 1
                del_num_eachb[del_bidx] += 1
            sum_del_point_num = 0
            for bidx in range( bsplit_idxmap.shape[0] ):
                sum_del_point_num += del_num_eachb[bidx]
                bsplit_idxmap[bidx,1] -= sum_del_point_num
            # check if points in some blocks are all delted
            empty_bidxs = []
            last_point_idx = 0
            for bidx in range( bsplit_idxmap.shape[0] ):
                if bsplit_idxmap[bidx][1] == last_point_idx:
                    empty_bidxs.append( bidx )
                last_point_idx = bsplit_idxmap[bidx][1]
            empty_bidxs = np.array( empty_bidxs )
            bsplit_idxmap = np.delete( bsplit_idxmap, empty_bidxs, 0 )
            assert bsplit_idxmap[-1,1] == sample_num

            sampling_meta['miss_rootb_num'] = empty_bidxs.size
        sampling_meta['miss_point_n'] = org_num - sample_num

        return data, label, bsplit_idxmap, sampling_meta

    def get_batch_of_larger_block( self,global_blockid_start,global_blockid_end,feed_data_elements,feed_label_elements ):
        gsbb = GlobalSubBaseBLOCK(self.h5f,self.file_name)
        all_sorted_global_bids = gsbb.get_all_sorted_aimbids('global')
        assert 0<= global_blockid_start  <=all_sorted_global_bids.shape[0]
        assert 0<= global_blockid_end <= all_sorted_global_bids.shape[0]
        all_sorted_global_bids = all_sorted_global_bids[global_blockid_start:global_blockid_end]
        batch_datas = []
        batch_labels = []
        for global_block_id in all_sorted_global_bids:
            block_datas,block_labels,_,_ = self.get_data_larger_block( global_block_id,gsbb,feed_data_elements,feed_label_elements )
            batch_datas.append(np.expand_dims(block_datas,axis=0))
            batch_labels.append(np.expand_dims(block_labels,axis=0))
        batch_datas = np.concatenate(batch_datas,axis=0)
        batch_labels = np.concatenate(batch_labels,axis=0)

       # print(batch_datas.shape)
       # print(batch_labels.shape)

        return batch_datas, batch_labels

    @staticmethod
    def check_sgfh5_intact( file_name ):
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.sgfh5'
        if not os.path.exists(file_name):
            return False, "%s not exist"%(file_name)
        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        with h5py.File(file_name,'r') as h5f:
            if 'is_intact_sgfh5' not in h5f.attrs:
                return False,"no is_intact_sgfh5 attr"
            IsIntact = h5f.attrs['is_intact_sgfh5'] == 1
            return IsIntact,"is_intact_sgfh5=1"

    def file_saveas_pyramid_feed(self,IsShowSummaryFinished=False,Always_CreateNew_pyh5=False,Always_CreateNew_bmh5=False,Always_CreateNew_bxmh5=False):
        '''
        save by global block
        '''
        gsbb_write = GlobalSubBaseBLOCK( root_s_h5f = self.h5f, root_s_h5f_fn = self.file_name )

        region_name = os.path.splitext( os.path.basename(self.file_name) )[0]
        house_dir_name = os.path.dirname(self.file_name)
        house_name = os.path.basename(house_dir_name)
        rootsort_dirname = os.path.dirname(house_dir_name)

        out_folder_pl = rootsort_dirname + '_pl_nh5_' + gsbb_write.get_pyramid_flag( OnlyGlobal = True ) + '/' + house_name
        out_folder_bmap = rootsort_dirname + '_bmap_nh5_' + gsbb_write.get_pyramid_flag( OnlyGlobal = False) + '/' + house_name

        if not os.path.exists(out_folder_pl):
            os.makedirs(out_folder_pl)
        if not os.path.exists(out_folder_bmap):
            os.makedirs(out_folder_bmap)

        IsIntact_sh5,ck_str = Sorted_H5f.check_sh5_intact( self.file_name )
        if not IsIntact_sh5:
            print( "\n\nsh5 not intact:  %s \nAbandon generating nh5"%(self.file_name) )
            return

        # check bmh5 intact primarily
        IsIntact_bmh5,ck_str = GlobalSubBaseBLOCK.check_bmh5_intact(gsbb_write.bmh5_fn)
        if Always_CreateNew_bmh5 or ( not IsIntact_bmh5 ):
            gsbb_write.save_bmap_between_dif_stride_step()

        #-----------------------------------------------------------------------
        def save_pl_prh5( pl_nh5_filename, gsbb_write, S_H5f):
            print('start gen pyramid file: ',pl_nh5_filename)
            with h5py.File(pl_nh5_filename,'w') as h5f:
                #global_block_sample_shape = gsbb_write.get_block_sample_shape('global')
                global_num_point = gsbb_write.global_num_point
                global_attrs = gsbb_write.get_new_attrs('global')

                pl_nh5f = Normed_H5f(h5f,pl_nh5_filename,S_H5f.h5f.attrs['datasource_name'])
                pl_nh5f.copy_root_attrs_from_sorted(global_attrs,global_num_point,S_H5f.IS_CHECK)

                file_datas = []
                file_labels = []
                file_rootb_split_idxmaps = []

                feed_norm_ele_info = Normed_H5f.get_norm_eles_by_attrs(S_H5f.h5f.attrs['element_names'])
                feed_data_elements = feed_norm_ele_info['norm_data_eles']
                feed_label_elements = feed_norm_ele_info['label_eles']

                #sg_all_bidxmaps = []
                #all_flatten_bidxmaps = []
                #sum_sg_bidxmap_sample_num = np.zeros(shape=(gsbb_write.cascade_num,8))
                #sum_flatten_bmap_sample_num = np.zeros(shape=(gsbb_write.cascade_num,3))

                all_sorted_global_bids_valid = []
                cas0_bidxmap_meta = {}
                cas0_bidxmap_meta[ 'sg_sample_num_cas0_dic' ] = {}
                cas0_bidxmap_meta[ 'cas0_bids_in_global_valid_dic' ] = {}
                cas0_bidxmap_meta[ 'flatten_bidxmap0_dic' ] = {}

                all_sorted_global_bids = gsbb_write.get_all_sorted_aimbids('global')
                for global_block_id in all_sorted_global_bids:
                    #block_datas, block_labels,cas0_bids_in_global_valid,sg_sample_num_cas0,flatten_bidxmap0 = \
                    block_datas, block_labels, rootb_split_idxmap, global_sampling_meta = \
                        self.get_data_larger_block( global_block_id,gsbb_write,feed_data_elements,feed_label_elements, gsbb_write.global_num_point, Normed_H5f.max_rootb_num )
                    if block_datas.size == 0:
                        continue

#                    all_sorted_global_bids_valid.append( global_block_id )
#                    cas0_bidxmap_meta['sg_sample_num_cas0_dic' ][global_block_id] = sg_sample_num_cas0
#                    cas0_bidxmap_meta['cas0_bids_in_global_valid_dic'][global_block_id] = cas0_bids_in_global_valid
#                    cas0_bidxmap_meta['flatten_bidxmap0_dic'][global_block_id] = flatten_bidxmap0

                    #sg_bidxmaps,sg_bidxmap_sample_num,flatten_bidxmaps,flatten_bmap_sample_num = \
                    #      gsbb_write.get_all_bidxmaps(cas0_bids_in_global_valid,sg_sample_num_cas0,flatten_bidxmap0)
                    #sg_all_bidxmaps.append(np.expand_dims(sg_bidxmaps,0))
                    #all_flatten_bidxmaps.append(np.expand_dims(flatten_bidxmaps,0))
                    #sum_sg_bidxmap_sample_num += sg_bidxmap_sample_num
                    #sum_flatten_bmap_sample_num += flatten_bmap_sample_num

                    file_datas.append(np.expand_dims(block_datas,axis=0))
                    file_labels.append(np.expand_dims(block_labels,axis=0))
                    file_rootb_split_idxmaps.append(np.expand_dims(rootb_split_idxmap,axis=0))

                if len(file_datas) == 0:
                    h5f.attrs['intact_void_file'] = 1
                    print('all point in this file are void : %s\n'%(pl_nh5_filename))
                else:
                    file_datas = np.concatenate(file_datas,axis=0)
                    file_labels = np.concatenate(file_labels,axis=0)
                    file_rootb_split_idxmaps = np.concatenate(file_rootb_split_idxmaps,axis=0)

                    pl_nh5f.append_to_dset('data',file_datas)
                    pl_nh5f.append_to_dset('labels',file_labels,IsLabelWithRawCategory=False)
                    pl_nh5f.append_to_dset('rootb_split_idxmap', file_rootb_split_idxmaps)

                    #sg_all_bidxmaps = np.concatenate(sg_all_bidxmaps,0)
                    #all_flatten_bidxmaps = np.concatenate(all_flatten_bidxmaps,0)
                    #gsbb_write.sum_sg_bidxmap_sample_num = sum_sg_bidxmap_sample_num
                    #gsbb_write.sum_flatten_bmap_sample_num = sum_flatten_bmap_sample_num
                    #gsbb_write.write_paras_in_h5fattrs( pl_nh5f.h5f['bidxmaps_sample_group'].attrs )
                    #pl_nh5f.append_to_dset('bidxmaps_sample_group',sg_all_bidxmaps)
                    #pl_nh5f.append_to_dset('bidxmaps_flatten',all_flatten_bidxmaps)

                    pl_nh5f.create_done()
                    if IsShowSummaryFinished:
                        pl_nh5f.show_summary_info()
                    print('pyramid file save finished: data shape: %s'%(str(pl_nh5f.data_set.shape)) )
        #-----------------------------------------------------------------------
        #def save_sgf_h5f( sgflat_map_filename, all_sorted_global_bids_valid,  cas0_bidxmap_meta ):
        #    with h5py.File( sgflat_map_filename,'w' ) as sgf_h5f:
        #        sgf_h5f.attrs['is_intact_sgfh5'] = 0
        #        all_sorted_global_bids_valid_dset = sgf_h5f.create_dataset('all_sorted_global_bids_valid', shape=all_sorted_global_bids_valid.shape,dtype=np.int64)
        #        all_sorted_global_bids_valid_dset[...] = all_sorted_global_bids_valid
        #        for dname,data in cas0_bidxmap_meta.items():
        #            for global_block_id in all_sorted_global_bids_valid:
        #                dset = sgf_h5f.create_dataset(dname+'/'+str(global_block_id),shape=data[global_block_id].shape, dtype=np.int32)
        #                dset[...] = data[global_block_id]
        #        sgf_h5f.attrs['is_intact_sgfh5'] = 1
        #        sgf_h5f.flush()
        #        print('write finish: %s'%(sgflat_map_filename))
        #-----------------------------------------------------------------------
        pl_nh5_filename = os.path.join(out_folder_pl,region_name+'.nh5')
        gsbb_write = GlobalSubBaseBLOCK(self.h5f,self.file_name)
        IsIntact_pl_nh5,ck_str = Normed_H5f.check_nh5_intact( pl_nh5_filename )
        if (not Always_CreateNew_pyh5) and IsIntact_pl_nh5:
            print('pyh5 intact: %s'%(pl_nh5_filename))
        else:
            save_pl_prh5( pl_nh5_filename, gsbb_write, self)
            # save cas0 sg_flatten_block_indexmap for this prh5 file
            #all_sorted_global_bids_valid = np.array(all_sorted_global_bids_valid).astype(np.int64)
            #save_sgf_h5f( sgflat_map_filename, all_sorted_global_bids_valid, cas0_bidxmap_meta )

        #-----------------------------------------------------------------------
        # save bmap file
        def save_bxmap_h5f(bmap_nh5_filename, S_H5f, pl_nh5_filename):
            print( 'start writing bxmap h5f: %s'%(bmap_nh5_filename))
            with h5py.File(pl_nh5_filename,'r') as pl_nh5f, h5py.File(bmap_nh5_filename,'w') as bmap_h5f:
                rootb_split_idxmap = pl_nh5f['rootb_split_idxmap']
                global_block_num = rootb_split_idxmap.shape[0]

                bmap_nh5f = Normed_H5f(bmap_h5f,bmap_nh5_filename,S_H5f.h5f.attrs['datasource_name'])
                bmap_nh5f.create_bidxmap_dsets( gsbb_write )

                sg_all_bidxmaps = []
                all_flatten_bidxmaps = []
                sum_sg_bidxmap_sample_num = np.zeros(shape=(gsbb_write.cascade_num,8))
                sum_flatten_bmap_sample_num = np.zeros(shape=(gsbb_write.cascade_num,3))


                for global_bidx in range( global_block_num ):
                    sg_bidxmaps, flatten_bidxmaps =\
                           gsbb_write.get_all_bidxmaps( rootb_split_idxmap[global_bidx] )
                    sg_all_bidxmaps.append(np.expand_dims(sg_bidxmaps,0))
                    all_flatten_bidxmaps.append(np.expand_dims(flatten_bidxmaps,0))
                    #sum_sg_bidxmap_sample_num += sg_bidxmap_sample_num
                    #sum_flatten_bmap_sample_num += flatten_bmap_sample_num

                sg_all_bidxmaps = np.concatenate(sg_all_bidxmaps,0)
                all_flatten_bidxmaps = np.concatenate(all_flatten_bidxmaps,0)
                #gsbb_write.sum_sg_bidxmap_sample_num = sum_sg_bidxmap_sample_num
                #gsbb_write.sum_flatten_bmap_sample_num = sum_flatten_bmap_sample_num

                #gsbb_write.write_paras_in_h5fattrs( bmap_nh5f.h5f.attrs )
                bmap_nh5f.append_to_dset('bidxmaps_sample_group',sg_all_bidxmaps)
                bmap_nh5f.append_to_dset('bidxmaps_flatten',all_flatten_bidxmaps)
                bmap_nh5f.create_done()
                print('write finish: %s'%(bmap_nh5_filename))

        bmap_nh5_filename = os.path.join(out_folder_bmap,region_name+'.bxmh5')
        IsIntact_nh5_bmap,ck_str = Normed_H5f.check_nh5_intact( bmap_nh5_filename )
        if (not Always_CreateNew_bxmh5) and  IsIntact_nh5_bmap:
            print('bxmh5 intact: %s'%(bmap_nh5_filename))
        else:
            save_bxmap_h5f( bmap_nh5_filename, self, pl_nh5_filename )


    def get_feed_ele_ids(self,feed_data_elements,feed_label_elements):
        feed_data_ele_ids = self.get_data_ele_ids(feed_data_elements)
        feed_label_ele_ids = self.get_label_ele_ids(feed_label_elements)
        return feed_data_ele_ids,feed_label_ele_ids


def sort_to_blocks_onef(Sort_RawH5f_Instance,file_name,block_step_xyz=[1,1,1]):
    '''
    split th ewhole scene to space sorted small blocks
    The whole scene is a group. Each block is one dataset in the group.
    The block attrs represents the field.
    '''
    print('start sorting file to blocks: %s'%file_name)
    block_step = np.array( block_step_xyz )
    print('block step = ',block_step)
    Sort_RawH5f_Instance.row_num_limit = None

    if not os.path.exists(Sort_RawH5f_Instance.out_folder):
        os.makedirs(Sort_RawH5f_Instance.out_folder)
    basefn = os.path.splitext(os.path.basename(file_name))[0]
    blocked_file_name = os.path.join(Sort_RawH5f_Instance.out_folder,basefn)+'.sh5'
    with h5py.File(blocked_file_name,'w') as h5f_blocked:
        with h5py.File(file_name,'r') as h5_f:
            Sort_RawH5f_Instance.raw_h5f = Raw_H5f(h5_f,file_name)
            Sort_RawH5f_Instance.s_h5f = Sorted_H5f(h5f_blocked,blocked_file_name)

            Sort_RawH5f_Instance.s_h5f.copy_root_attrs_from_raw( Sort_RawH5f_Instance.raw_h5f.raw_h5f )
            Sort_RawH5f_Instance.s_h5f.set_step_stride(block_step,block_step)

            #Sort_RawH5f_Instance.row_num_limit = int(self.raw_h5f.total_row_N/1000)

            row_step = g_h5_num_row_1M*2
            sorted_buf_dic = {}
            raw_row_N = Sort_RawH5f_Instance.raw_h5f.xyz_dset.shape[0]

            for k in range(0,raw_row_N,row_step):
                end = min(k+row_step,raw_row_N)
                _,data_name_list = Sort_RawH5f_Instance.raw_h5f.get_total_num_channels_name_list()
                raw_buf = np.zeros((end-k,Sort_RawH5f_Instance.s_h5f.total_num_channels))
                for dn in data_name_list:
                    raw_buf[:,Sort_RawH5f_Instance.s_h5f.data_idxs[dn] ] = Sort_RawH5f_Instance.raw_h5f.raw_h5f[dn][k:end,:]
                if Sort_RawH5f_Instance.s_h5f.IS_CHECK:
                    if end < 16777215: # this is the largest int float32 can acurately present
                        org_row_index = np.arange(k,end)
                    else:
                        org_row_index = -1
                    raw_buf[:,Sort_RawH5f_Instance.s_h5f.data_idxs['org_row_index'][0]] = org_row_index

                sorted_buf_dic={}
                Sort_RawH5f_Instance.sort_buf(raw_buf,k,sorted_buf_dic)

                Sort_RawH5f_Instance.h5_write_buf(sorted_buf_dic)

                if int(k/row_step) % 1 == 0:
                    print('%%%.1f  line[ %d:%d ] block_N = %d'%(100.0*end/Sort_RawH5f_Instance.raw_h5f.total_row_N, k,end,len(sorted_buf_dic)))
                     #print('line: [%d,%d] blocked   block_T=%f s, read_T=%f ms, cal_t = %f ms, write_t= %f ms'%\
                           #(k,end,time.time()-t0_k,(t1_k-t0_k)*1000,(t2_1_k-t2_0_k)*1000, (t2_2_k-t2_1_k)*1000 ))
                if hasattr(Sort_RawH5f_Instance,'row_num_limit') and Sort_RawH5f_Instance.row_num_limit!=None and  end>=Sort_RawH5f_Instance.row_num_limit:
                #if k /row_step >3:
                    print('break read at k= ',end)
                    break

            total_row_N,total_block_N = Sort_RawH5f_Instance.s_h5f.add_total_row_block_N()

            if total_row_N != Sort_RawH5f_Instance.raw_h5f.total_row_N:
                print('ERROR: blocked total_row_N= %d, raw = %d'%(total_row_N,Sort_RawH5f_Instance.raw_h5f.total_row_N))
            print('total_block_N = ',total_block_N)

            if Sort_RawH5f_Instance.s_h5f.IS_CHECK:
                check = Sort_RawH5f_Instance.s_h5f.check_equal_to_raw(Sort_RawH5f_Instance.raw_h5f) & Sort_RawH5f_Instance.s_h5f.check_xyz_scope()
                print('overall check of equal and scope:')
                if check:
                    print('both passed')
                else:
                    print('somewhere check failed')
            #Sort_RawH5f_Instance.s_h5f.show_summary_info()
class Sort_RawH5f():
    '''
    (1) Do sort: from "Raw_H5f" to "Sorted_H5f"
    unsampled: .sh5
    sampled: .rsh5  (fix number in each block)
    block_step_xyz=[0.5,0.5,0.5]
    '''
    def __init__(self,raw_file_list,block_step_xyz,out_folder,IsShowInfoFinished=False):
        self.IsShowInfoFinished = IsShowInfoFinished
        self.out_folder = out_folder
        self.Do_sort_to_blocks(raw_file_list,block_step_xyz)

    def Do_sort_to_blocks(self,raw_file_list,block_step_xyz):
        IsMulti = False
        if not IsMulti:
            for fn in raw_file_list:
                self.sort_to_blocks(fn,block_step_xyz)
                #sort_to_blocks_onef(self,fn,block_step_xyz)
        else:
            #pool = mp.Pool( max(mp.cpu_count()/2,1) )
            print('cpu_count= ',mp.cpu_count())
            pool = mp.Pool(processes=4)
            for i,fn in enumerate(raw_file_list):
                pool.apply_async(sort_to_blocks_onef,(self,fn,block_step_xyz,))
                print('apply_async %d  fn=%s'%(i,fn))
            pool.close()
            pool.join()


    def sort_to_blocks(self,file_name,block_step_xyz):
        '''
        split th ewhole scene to space sorted small blocks
        The whole scene is a group. Each block is one dataset in the group.
        The block attrs represents the field.
        '''
        IsIntact,_ = Raw_H5f.check_rh5_intact(file_name)
        if not IsIntact:
            print('Abandon sorting, rh5 not intact:'%(file_name))

        block_step = np.array( block_step_xyz )
        self.row_num_limit = None

        if not os.path.exists(self.out_folder):
            os.makedirs(self.out_folder)
        basefn = os.path.splitext(os.path.basename(file_name))[0]
        blocked_file_name = os.path.join(self.out_folder,basefn)+'.sh5'

        IsIntact,_ = Sorted_H5f.check_sh5_intact(blocked_file_name)
        if IsIntact:
            print('sh5 file intact: %s'%(blocked_file_name))
            return

        print('start sorting file to blocks: %s'%file_name)
        print('block step = ',block_step)

        with h5py.File(blocked_file_name,'w') as h5f_blocked:
            with h5py.File(file_name,'r') as h5_f:
                self.raw_h5f = Raw_H5f(h5_f,file_name)
                self.s_h5f = Sorted_H5f(h5f_blocked,blocked_file_name)

                self.s_h5f.copy_root_attrs_from_raw( self.raw_h5f.h5f )
                self.s_h5f.set_step_stride(block_step,block_step)

                #self.row_num_limit = int(self.raw_h5f.total_row_N/1000)

                row_step = g_h5_num_row_1M*3
                sorted_buf_dic = {}
                raw_row_N = self.raw_h5f.xyz_dset.shape[0]

                for k in range(0,raw_row_N,row_step):
                    end = min(k+row_step,raw_row_N)
                    _,data_name_list = self.raw_h5f.get_total_num_channels_name_list()
                    raw_buf = np.zeros((end-k,self.s_h5f.total_num_channels))
                    for dn in data_name_list:
                        raw_buf[:,self.s_h5f.data_idxs[dn] ] = self.raw_h5f.h5f[dn][k:end,:]
                    if self.s_h5f.IS_CHECK:
                        if end < 16777215: # this is the largest int float32 can acurately present
                            org_row_index = np.arange(k,end)
                        else:
                            org_row_index = -1
                        raw_buf[:,self.s_h5f.data_idxs['org_row_index'][0]] = org_row_index

                    sorted_buf_dic={}
                    self.sort_buf(raw_buf,k,sorted_buf_dic)

                    self.h5_write_buf(sorted_buf_dic)

                    if int(k/row_step) % 1 == 0:
                        print('%%%.1f  line[ %d:%d ] block_N = %d'%(100.0*end/self.raw_h5f.total_row_N, k,end,len(sorted_buf_dic)))
                         #print('line: [%d,%d] blocked   block_T=%f s, read_T=%f ms, cal_t = %f ms, write_t= %f ms'%\
                               #(k,end,time.time()-t0_k,(t1_k-t0_k)*1000,(t2_1_k-t2_0_k)*1000, (t2_2_k-t2_1_k)*1000 ))
                    if hasattr(self,'row_num_limit') and self.row_num_limit!=None and  end>=self.row_num_limit:
                    #if k /row_step >3:
                        print('break read at k= ',end)
                        break
                assert end == self.raw_h5f.total_row_N
                total_row_N,total_block_N = self.s_h5f.add_total_row_block_N(self.raw_h5f.total_row_N)
                print('total_block_N = ',total_block_N)
                self.s_h5f.add_label_histagram()

                if self.s_h5f.IS_CHECK:
                    check = self.s_h5f.check_equal_to_raw(self.raw_h5f) & self.s_h5f.check_xyz_scope()
                    print('overall check of equal and scope:')
                    if check:
                        print('both passed')
                    else:
                        print('somewhere check failed')
                self.s_h5f.h5f.attrs['is_intact'] = 1
                if self.IsShowInfoFinished:
                    self.s_h5f.show_summary_info()
        print('sorted OK: %s'%(blocked_file_name))

    def sort_buf(self,raw_buf,buf_start_k,sorted_buf_dic):
        #t0 = time.time()
        IsMulti = False
        if IsMulti:
            block_ks = self.get_block_index_multi(raw_buf)
        else:
            block_ks = np.zeros(raw_buf.shape[0],np.int64)
            for j in range(raw_buf.shape[0]):
                block_ks[j] = self.s_h5f.xyz_to_block_index(raw_buf[j,0:3])

        #t1 = time.time()
        for i in range(raw_buf.shape[0]):
            block_k = block_ks[i]
            row = raw_buf[i,:].reshape(1,-1)
            if not block_k in sorted_buf_dic:
                sorted_buf_dic[block_k]=[]
            sorted_buf_dic[block_k].append(row)
        #t2 = time.time()
        #print('t1 = %d ms, t2 = %d ms'%( (t1-t0)*1000,(t2-t1)*1000 ))

    def h5_write_buf(self,sorted_buf_dic):
        for key in sorted_buf_dic:
            sorted_buf_dic[key] = np.concatenate(sorted_buf_dic[key],axis=0)
        for block_k in sorted_buf_dic:
            self.s_h5f.append_to_dset(block_k,sorted_buf_dic[block_k],vacant_size=g_h5_num_row_1M)
        self.s_h5f.rm_invalid_data()
        self.s_h5f.h5f.flush()

    def get_block_index_multi(self,raw_buf):
        block_ks = mp.Array('i',raw_buf.shape[0])
        num_workers = 2
        step = int(raw_buf.shape[0]/num_workers)
        pool = []
        for i in range(0,raw_buf.shape[0],step):
            end = min( (i+1)*step, raw_buf.shape[0])
            p = mp.Process(target=self.get_block_index_subbuf,args=(raw_buf[i:end,0:3],block_ks,i) )
            p.start()
            pool.append(p)
        for p in pool:
            p.join()
        return block_ks

    def get_block_index_subbuf(self,sub_buf_xyz,block_ks,i_start):
        for i in range(sub_buf_xyz.shape[0]):
            block_ks[i+i_start] = self.s_h5f.xyz_to_block_index(sub_buf_xyz[i,0:3])



class DatasetMeta():
    g_label2class_dic = {}
    g_label2class_dic['MATTERPORT'] = MatterportMeta['label2class']
    g_label2class_dic['ETH'] = {0: 'unlabeled points', 1: 'man-made terrain', 2: 'natural terrain',\
                     3: 'high vegetation', 4: 'low vegetation', 5: 'buildings', \
                     6: 'hard scape', 7: 'scanning artefacts', 8: 'cars'}

    g_label2class_dic['STANFORD_INDOOR3D'] = \
                    {0:'ceiling', 1:'floor', 2:'wall', 3:'beam', 4:'column', 5:'window', 6:'door', 7:'table',
                     8:'chair', 9:'sofa', 10:'bookcase', 11:'board', 12:'clutter'}

    g_label2class_dic['SCANNET'] = {0:'unannotated', 1:'wall', 2:'floor', 3:'chair', 4:'table', 5:'desk',\
                                6:'bed', 7:'bookshelf', 8:'sofa', 9:'sink', 10:'bathtub', 11:'toilet',\
                                12:'curtain', 13:'counter', 14:'door', 15:'window', 16:'shower curtain',\
                                17:'refridgerator', 18:'picture', 19:'cabinet', 20:'otherfurniture'}
    g_label2color_dic = {}
    g_label2color_dic['MATTERPORT'] = MatterportMeta['label2color']
    g_label2color_dic['ETH'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],5:[10,200,100]}
    g_label2color_dic['STANFORD_INDOOR3D'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100]}
    g_label2color_dic['SCANNET'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100],
                    13: [100,200,200],14: [200,100,200],15: [100,200,100],16: [100,100,200],
                     17:[100,100,100],18:[200,200,200],19:[200,200,100],20:[200,200,100]}
    def __init__(self,datasource_name):
        self.datasource_name = datasource_name
        self.g_label2class = self.g_label2class_dic[self.datasource_name]
        self.g_label2color = self.g_label2color_dic[self.datasource_name]
        self.g_class2label = {cls:label for label,cls in self.g_label2class.iteritems()}
        self.g_class2color = {}
        for i in self.g_label2class:
            cls = self.g_label2class[i]
            self.g_class2color[cls] = self.g_label2color[i]
        self.num_classes = len(self.g_label2class)


class Normed_H5f():
    '''
    format: .nhf5
    (1) There are 3 datasets:
        'data' data_set store all normalized data, shape: N*(H*W)*C, like [num_block,4096,9]
        'labels' [num_block,4096]
        'pred_logits'
    (2) The root attrs are inherited from Sorted_H5f, just record the raw data information. Not all attrs are meaningful here.
        Especially, 'element_names' means the raw elements in Sorted_H5f.
        The normed data elements are stored in attrs of dataset "data".
    (3) The elements to be stored are flexible, the idx order responds to self.normed_ele_idx_order
    (4) The label_category in Sorted_H5f is raw_category_idx, the label_category in Normed_H5f is mpcat40 index

    *** example of root attrs:
    The root_attr:  [u'datasource_name', u'element_names', u'total_block_N', u'xyz_max', u'xyz_min', u'xyz_max_aligned', u'xyz_min_aligned', u'xyz_scope_aligned', u'block_step', u'block_stride', u'block_dims_N', u'total_row_N', u'sample_num']
    datasource_name: MATTERPORT
    element_names: ['label_material' 'label_instance' 'label_category' 'color' 'nxnynz' 'xyz']
    total_row_N: 98304
    total_block_N: 12
    block_step: [4 4 2]
    block_stride: [2 2 2]
    block_dims_N: [2 3 2]
    xyz_min: [-2.2845428   0.40383905 -0.02779843]
    xyz_max: [ 0.87131613  5.36070538  2.57167315]
    xyz_min_aligned: [-2.3  0.4 -0.2]
    xyz_max_aligned: [ 0.9  5.4  2.6]
    xyz_scope_aligned: [ 3.2  5.   2.8]

    *** example of attrs of data_dset: ( shape= (12, 8192, 15))
    color_1norm  =  [12 13 14]
    nxnynz  =  [ 9 10 11]
    xyz  =  [0 1 2]
    xyz_midnorm  =  [3 4 5]
    xyz_1norm  =  [6 7 8]
    valid_num  =  12

    *** example of attrs of labels_dset:
    labels   shape= (12, 8192, 3)
    label_material  =  [2]
    label_instance  =  [1]
    label_category  =  [0]
    valid_num  =  12
    label_category_hist  =  [    5 25276     0 14310  9288     0     0     0     0     0     0     0
    5122     0 26148     0     0     0     0     0     0     0     0     0
    0     0     0     0     0 13049     0     0     0     0     0     0
    0     0   409     0  1477     0]
    label_category_1norm  =  [  5.25850827e-05   2.65828110e-01   0.00000000e+00   1.50498507e-01
    '''
    # -----------------------------------------------------------------------------
    # CONSTANTS
    # -----------------------------------------------------------------------------
    g_label2class_dic = {}
    g_label2class_dic['MATTERPORT'] = MatterportMeta['label2class']
    g_label2class_dic['ETH'] = {0: 'unlabeled points', 1: 'man-made terrain', 2: 'natural terrain',\
                     3: 'high vegetation', 4: 'low vegetation', 5: 'buildings', \
                     6: 'hard scape', 7: 'scanning artefacts', 8: 'cars'}

    g_label2class_dic['STANFORD_INDOOR3D'] = \
                    {0:'ceiling', 1:'floor', 2:'wall', 3:'beam', 4:'column', 5:'window', 6:'door', 7:'table',
                     8:'chair', 9:'sofa', 10:'bookcase', 11:'board', 12:'clutter'}

    g_label2class_dic['SCANNET'] = {0:'unannotated', 1:'wall', 2:'floor', 3:'chair', 4:'table', 5:'desk',\
                                6:'bed', 7:'bookshelf', 8:'sofa', 9:'sink', 10:'bathtub', 11:'toilet',\
                                12:'curtain', 13:'counter', 14:'door', 15:'window', 16:'shower curtain',\
                                17:'refridgerator', 18:'picture', 19:'cabinet', 20:'otherfurniture'}
    g_label2color_dic = {}
    g_label2color_dic['MATTERPORT'] = MatterportMeta['label2color']
    g_label2color_dic['ETH'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],5:[10,200,100]}
    g_label2color_dic['STANFORD_INDOOR3D'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100]}
    g_label2color_dic['SCANNET'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100],
                    13: [100,200,200],14: [200,100,200],15: [100,200,100],16: [100,100,200],
                     17:[100,100,100],18:[200,200,200],19:[200,200,100],20:[200,200,100]}

    #g_easy_view_labels = [7,8,9,10,11,1]
    #g_is_labeled = True

    ## normed data channels
    normed_data_elements_candi = {}
    #normed_data_elements_candi['xyz'] = ['xyz','xyz_midnorm_block','xyz_1norm_file']
    normed_data_elements_candi['xyz'] = ['xyz']
    normed_data_elements_candi['nxnynz'] = ['nxnynz']
    normed_data_elements_candi['color'] = ['color_1norm']
    normed_data_elements_candi['intensity'] = ['intensity_1norm']
    normed_ele_idx_order = ['xyz','xyz_midnorm_block','xyz_1norm_file','color_1norm','nxnynz','intensity_1norm']
    normed_data_ele_candi_len = {'xyz':3,'xyz_midnorm_block':3,'xyz_1norm_file':3,'nxnynz':3,'color_1norm':3,'intensity_1norm':1}

    labels_order = ['label_category','label_instance','label_material']
    label_candi_eles_len = {'label_category':1,'label_instance':1,'label_material':1}
    max_rootb_num = 20000

    def __init__(self,h5f,file_name,datasource_name=None):
        '''
        DATASET_NAME = 'ETH'
        DATASET_NAME = 'STANFORD_INDOOR3D'
        '''
        self.h5f = h5f
        self.file_name = file_name
        if datasource_name == None:
            assert 'datasource_name' in self.h5f.attrs
        else:
            self.h5f.attrs['datasource_name'] = datasource_name
        assert self.h5f.attrs['datasource_name'] in DATA_SOURCE_NAME_LIST
        self.datasource_name = self.h5f.attrs['datasource_name']
        self.g_label2class = self.g_label2class_dic[self.datasource_name]
        self.g_label2color = self.g_label2color_dic[self.datasource_name]
        self.g_class2label = {cls:label for label,cls in self.g_label2class.iteritems()}
        self.g_class2color = {}
        for i in self.g_label2class:
            cls = self.g_label2class[i]
            self.g_class2color[cls] = self.g_label2color[i]
        self.num_classes = len(self.g_label2class)

        self.dataset_names = ['data','labels','raw_xyz','pred_logits']
        for dn in self.dataset_names:
            if dn in h5f:
                setattr(self,dn+'_set', h5f[dn])
        self.update_norm_eles_by_attrs()

    @staticmethod
    def get_norm_eles_by_attrs(h5f_attrs_element_names):
        norm_ele_info = {}
        norm_data_eles = [Normed_H5f.normed_data_elements_candi[de] for de in Normed_H5f.normed_data_elements_candi if de in h5f_attrs_element_names]
        norm_data_eles = [e  for e_ls in norm_data_eles for e in e_ls]
        norm_ele_info['norm_data_eles'] = [e for e in Normed_H5f.normed_ele_idx_order if e in norm_data_eles]
        norm_data_ele_lens = np.array( [Normed_H5f.normed_data_ele_candi_len[e] for e in norm_data_eles ])
        norm_ele_info['norm_data_eles_num'] = np.sum(norm_data_ele_lens)

        norm_ele_info['label_eles'] = [lb for lb in Normed_H5f.labels_order if lb in h5f_attrs_element_names]
        norm_ele_info['label_eles_num'] = len(norm_ele_info['label_eles'])

        norm_ele_info['norm_data_ele_idxs'] = Normed_H5f.get_normeddata_ele_idxs(norm_ele_info['norm_data_eles'])
        norm_ele_info['label_ele_idxs'] = Normed_H5f.get_label_ele_ids(norm_ele_info['label_eles'])

        return norm_ele_info

    def update_norm_eles_by_attrs(self):
        if 'element_names' not in self.h5f.attrs:
            return # new created file
        norm_ele_info = Normed_H5f.get_norm_eles_by_attrs(self.h5f.attrs['element_names'])

        self.norm_data_eles_num = norm_ele_info['norm_data_eles_num']
        self.label_eles_num = norm_ele_info['label_eles_num']
        self.normed_data_set_elements = norm_ele_info['norm_data_eles']
        self.label_set_elements = norm_ele_info['label_eles']
        self.normed_data_ele_idxs = norm_ele_info['norm_data_ele_idxs']
        self.label_ele_idxs = norm_ele_info['label_ele_idxs']

    def get_feed_ele_ids(self,feed_data_elements,feed_label_elements):
        if feed_data_elements==None:
            feed_data_ele_idxs = self.normed_data_ele_idxs
        else:
            feed_data_ele_idxs = self.get_normeddata_ele_idxs(feed_data_elements)
        if feed_label_elements==None:
            feed_label_ele_ids = self.label_ele_idxs
        else:
            feed_label_ele_ids = self.get_label_ele_ids(feed_label_elements)
        return feed_data_ele_idxs,feed_label_ele_ids

    def get_normed_data(self,start_block,end_blcok,feed_elements=None):
        # the data ele order store according to feed_elements
        if feed_elements==None:
            datas = self.data_set[start_block:end_blcok,...]
        else:
            normed_data_ele_idx = np.sort(list(set([k for e in feed_elements for k in self.data_set.attrs[e] ])))
            datas = self.data_set[start_block:end_blcok,...,normed_data_ele_idx]
        return datas

    @staticmethod
    def get_bidxmaps(bxmh5_fn, start_block,end_block):
        with h5py.File(bxmh5_fn,'r') as h5f:
            flatten_bidxmaps = h5f['bidxmaps_flatten'][start_block:end_block,:]
            sg_bidxmaps = h5f['bidxmaps_sample_group'][start_block:end_block,:]

        return  sg_bidxmaps, flatten_bidxmaps

    def get_label_eles(self,start_block,end_blcok,feed_label_elements=None):
        # order according to feed_label_elements
        if feed_label_elements==None:
            labels = self.labels_set[start_block:end_blcok,...]
        else:
            labels_ele_idx = np.sort(list(set( [k for e in feed_label_elements for k in self.labels_set.attrs[e]] )))
            labels = self.labels_set[start_block:end_blcok,...,labels_ele_idx]
            assert labels.ndim == self.labels_set.ndim
        return labels
    @staticmethod
    def get_normeddata_ele_idxs(normed_data_elements):
        # order according to Normed_H5f.normed_ele_idx_order
        # len of each ele according to  normed_data_ele_candi_len
        data_ele_idxs = {}
        k = 0
        for e in Normed_H5f.normed_ele_idx_order:
            if e in normed_data_elements:
                #assert e in Normed_H5f.normed_ele_idx_order,"%s not in Normed_H5f.normed_ele_idx_order"%(e)
                idx = range(k,k+Normed_H5f.normed_data_ele_candi_len[e])
                k += Normed_H5f.normed_data_ele_candi_len[e]
                data_ele_idxs[e] = idx
        return data_ele_idxs
    @staticmethod
    def get_label_ele_ids(label_elements):
        # order according to  Normed_H5f.labels_order
        label_ele_idxs = {}
        k = 0
        for e in Normed_H5f.labels_order:
            if e in label_elements:
                label_ele_idxs[e] = range(k,k+Normed_H5f.label_candi_eles_len[e])
                k += Normed_H5f.label_candi_eles_len[e]
        return label_ele_idxs

    def raw_category_idx_2_mpcat40(self,labels_with_rawcategory):
        assert labels_with_rawcategory.ndim == 2
        assert 'label_category' in self.labels_set.attrs, "no label_category"
        raw_category_idx = self.labels_set.attrs['label_category'][0]
        labels_with_rawcategory[:,raw_category_idx] = get_cat40_from_rawcat(labels_with_rawcategory[:,raw_category_idx])
        labels_without_rawcategory = labels_with_rawcategory
        return labels_without_rawcategory

    def copy_root_attrs_from_sorted(self,sortedh5f_attrs,block_sample_num,sortedh5f_IS_CHECK):
        attrs=['datasource_name','element_names','total_block_N',
               'xyz_max','xyz_min','xyz_max_aligned','xyz_min_aligned','xyz_scope_aligned',
               'block_step','block_stride','block_dims_N','total_row_N']
        for attr in attrs:
            if attr in sortedh5f_attrs:
                self.h5f.attrs[attr] = sortedh5f_attrs[attr]
        self.h5f.attrs['is_intact_nh5'] = 0
        self.h5f.attrs['sample_num'] = block_sample_num

        # - org_row_index when sortedh5f IS_CHECK=True
        self.create_dsets()

    def copy_root_attrs_from_normed(self,h5f_normed,flag=None):
       # attrs_candis=['datasource_name','element_names','total_block_N',
       #        'xyz_max','xyz_min','xyz_max_aligned','xyz_min_aligned','xyz_scope_aligned',
       #        'block_step','block_stride','block_dims_N','total_row_N']
        for attr in h5f_normed.attrs:
            self.h5f.attrs[attr] = h5f_normed.attrs[attr]
        self.h5f.attrs['is_intact_nh5'] = 0
        if flag=='MergeNormed_H5f':
            if 'total_block_N' in self.h5f.attrs:
                del self.h5f.attrs['total_block_N']
            if 'total_row_N' in self.h5f.attrs:
                del self.h5f.attrs['total_row_N']

        normed_data_shape = h5f_normed['data'].shape
        if len(normed_data_shape)==3:
            sample_num = (normed_data_shape[1],)
        elif len(normed_data_shape)==4:
            sample_num = (normed_data_shape[1],normed_data_shape[2],)
        self.h5f.attrs['sample_num'] = sample_num
        self.create_dsets()
        for attr in h5f_normed['bidxmaps_sample_group'].attrs:
            if attr != 'valid_num':
                self.h5f['bidxmaps_sample_group'].attrs[attr] = h5f_normed['bidxmaps_sample_group'].attrs[attr]

    def show_summary_info(self):
        print('\n\nsummary of file: ',self.file_name)
        show_h5f_summary_info(self.h5f)

    @staticmethod
    def show_all_colors( datasource_name ):
        from PIL import Image
        label2color = Normed_H5f.g_label2color_dic[datasource_name]
        label2class = Normed_H5f.g_label2class_dic[datasource_name]
        path = os.path.join( BASE_DIR,'label_colors' )
        if not os.path.exists(path):
            os.makedirs(path)
        for label,color in label2color.iteritems():
            if label < len( label2class ):
                cls = label2class[label]
            else:
                cls = 'empty'
            data = np.zeros((512,512,3),dtype=np.uint8)
            color_ = np.array(color,dtype=np.uint8)
            data += color_
            img = Image.fromarray(data,'RGB')
            img.save(path+'/'+str(label)+'_'+cls+'.png')
            img.show()

    def label2color(self,label):
        assert( label in self.g_label2color )
        return self.g_label2color[label]

    def get_data_shape(self):
        dset = self.h5f['data']
        return dset.shape

    def create_dsets(self):
        self.h5f.attrs['is_intact_nh5'] = 0
        if 'total_block_N' in self.h5f.attrs:
            total_block_N = self.h5f.attrs['total_block_N']
        else:
            total_block_N = 1
        chunks_n = 1
        sample_num = self.h5f.attrs['sample_num']
        if sample_num.size==1:
            sample_num_size = sample_num
            sample_num = (sample_num,)
        elif sample_num.size==2:
            sample_num_size = sample_num[0] * sample_num[1]
            sample_num = (sample_num[0],sample_num[1],)
        self.update_norm_eles_by_attrs()
        norm_data_eles_num = self.norm_data_eles_num
        label_eles_num = self.label_eles_num

        #chunks_n = math.ceil( 1024 / 1*sample_num_size*norm_data_eles_num*4 )
        data_set = self.h5f.create_dataset( 'data',shape=(total_block_N,)+sample_num+(norm_data_eles_num,),\
                maxshape=(None,)+sample_num+(norm_data_eles_num,),dtype=np.float32,compression="gzip",\
                            chunks = (chunks_n,)+sample_num+(norm_data_eles_num,)  )
        labels_set = self.h5f.create_dataset( 'labels',shape=(total_block_N,)+sample_num+(label_eles_num,),\
                maxshape=(None,)+sample_num+(label_eles_num,),dtype=np.int16,compression="gzip",\
                            chunks = (chunks_n,)+sample_num+(label_eles_num,)  )
        rootb_split_idxmap_set = self.h5f.create_dataset( 'rootb_split_idxmap',shape=(total_block_N, Normed_H5f.max_rootb_num,2),\
                maxshape=(None, Normed_H5f.max_rootb_num, 2),dtype=np.int32,compression="gzip",\
                            chunks = (chunks_n,Normed_H5f.max_rootb_num,2,)  )

        # predicted label
        pred_logits_set = self.h5f.create_dataset( 'pred_logits',shape=(total_block_N,)+sample_num+(label_eles_num,),\
                maxshape=(None,)+sample_num+(label_eles_num,),dtype=np.int16,compression="gzip",\
                chunks = (chunks_n,)+sample_num+(label_eles_num,)  )
        pred_logits_set[:] = -1

        for ele in self.normed_data_set_elements:
            data_set.attrs[ele] = self.normed_data_ele_idxs[ele]
        for ele in self.label_set_elements:
            labels_set.attrs[ele] = self.label_ele_idxs[ele]

        data_set.attrs['valid_num'] = 0
        labels_set.attrs['valid_num'] = 0
        rootb_split_idxmap_set.attrs['valid_num'] = 0
        pred_logits_set.attrs['valid_num'] = 0
        self.data_set = data_set
        self.labels_set = labels_set
        #self.bidxmap_dsets = bidxmap_dsets
        self.pred_logits_set = pred_logits_set

    def create_bidxmap_dsets(self, gsbb_write):
        self.h5f.attrs['is_intact_nh5'] = 0
        chunks_n = 1
        total_block_N = 1
        sg_block_shape = gsbb_write.get_sg_bidxmaps_fixed_shape()
        sg_bidxmap_dset = self.h5f.create_dataset('bidxmaps_sample_group',shape=(total_block_N,)+sg_block_shape, dtype=np.int32,
                            maxshape=(None,)+sg_block_shape,chunks = (chunks_n,)+sg_block_shape  )
        sg_bidxmap_dset.attrs['valid_num'] = 0
        flatten_bidxmap_shape = gsbb_write.get_flatten_bidxmaps_shape()
        flatten_bidxmap_dset = self.h5f.create_dataset('bidxmaps_flatten',shape=(total_block_N,)+flatten_bidxmap_shape,dtype=np.int32,
                            maxshape=(None,)+flatten_bidxmap_shape, chunks = (chunks_n,)+flatten_bidxmap_shape  )
        flatten_bidxmap_dset.attrs['valid_num'] = 0

    def raw_xyz_set(self):
        return self.data_set[...,self.data_set.attrs['xyz']]

    def create_areano_dset(self,total_block_N,sample_num):
        chunks_n = 4
        area_no_set = self.h5f.create_dataset( 'area_no',shape=(total_block_N,sample_num),\
                maxshape=(None,sample_num),dtype=np.int16,compression="gzip",\
                chunks = (chunks_n,sample_num)  )
        area_no_set.attrs['valid_num'] = 0

    def append_to_dset(self,dset_name,data_i,vacant_size=0,IsLabelWithRawCategory=False):
        dset = self.h5f[dset_name]
        valid_num = dset.attrs['valid_num']
        if data_i.ndim == len(dset.shape) -1:
            for i in range(1,len(dset.shape)):
                assert(dset.shape[i] == data_i.shape[i-1]), "in Normed_H5f.append_to_dset: data shape not match dataset"
            new_valid_num = valid_num + 1
        else:
            assert(dset.shape[1:] == data_i.shape[1:]), "in Normed_H5f.append_to_dset: data shape not match dataset"
            new_valid_num = valid_num + data_i.shape[0]
            print('%s  %d -> %d'%(dset_name,valid_num,new_valid_num) )

        if new_valid_num > dset.shape[0]:
            dset.resize( (new_valid_num + vacant_size,)+dset.shape[1:] )

        if IsLabelWithRawCategory and dset_name == 'labels':
            data_i = self.raw_category_idx_2_mpcat40(data_i)
        dset[valid_num : new_valid_num,...] = data_i
        dset.attrs['valid_num'] = new_valid_num
        self.h5f.flush()

    def set_dset_value(self,dset_name,data_i,start_idx,end_idx):
        if dset_name not in self.h5f:
            return
        dset = self.h5f[dset_name]
        if dset.shape[0] < end_idx:
            dset.resize( (end_idx,) + dset.shape[1:] )
        if data_i.shape[1] < dset.shape[1]:
            dset[start_idx:end_idx,data_i.shape[1]:] = -1
        dset[start_idx:end_idx,0:data_i.shape[1]] = data_i
        if dset.attrs['valid_num'] < end_idx:
            dset.attrs['valid_num'] = end_idx

    def merge_file(self,another_file_name):
        # merge all the data from another_file intto self
        with h5py.File(another_file_name,'r') as f:
            ano_normed_h5f = Normed_H5f(f,another_file_name)
            for dset_name in ano_normed_h5f.h5f:
                self.append_to_dset(dset_name,ano_normed_h5f.h5f[dset_name])
                # set area no
            if self.datasource_name == 'STANFORD_INDOOR3D':
                base_name = os.path.basename(another_file_name)
                tmp = base_name.split('Area_')[1].split('_')[0]
                area_no = int(tmp)

                num_blocks,num_sample = ano_normed_h5f.h5f['label'].shape
                area_data = np.ones((num_blocks,num_sample)) * area_no
                self.append_to_dset('area_no',area_data)

    def create_done(self):
        self.rm_invalid_data()
        self.add_label_histagram()
        self.h5f.attrs['is_intact_nh5'] = 1
    @staticmethod
    def check_nh5_intact( file_name ):
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.nh5' or f_format == '.prh5' or f_format == '.bxmh5'
        if not os.path.exists(file_name):
            return False, "%s not exist"%(file_name)

       # if os.path.getsize( file_name ) / 1000.0 < 100:
       #     return False,"file too small < 100 K"
        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        #print('checking nh5: %s'%(file_name))
        with h5py.File(file_name,'r') as h5f:
            if 'intact_void_file' in h5f.attrs:
                return True,"void file"
            if 'is_intact_nh5' not in h5f.attrs:
                return False,"no is_intact_nh5 attr"
            IsIntact = h5f.attrs['is_intact_nh5'] == 1
            return IsIntact,"is_intact_nh5=1"

    def rm_invalid_data(self):
        for dset_name_i in self.h5f:
            def rm_invalid_dset(dset):
                valid_n = dset.attrs['valid_num']
                if dset.shape[0] > valid_n:
                    #print('resizing block %s from %d to %d'%(dset_name_i,dset.shape[0],valid_n))
                    dset.resize( (valid_n,)+dset.shape[1:] )
            rm_invalid_dset( self.h5f[dset_name_i] )
           # else:
           #     grp = self.h5f[dset_name_i]
           #     for dset_name_ij in grp:
           #         rm_invalid_dset( grp[dset_name_ij] )


    def add_label_histagram(self):
        label_name = 'label_category'
        if not hasattr(self,'labels_set'):
            return
        if label_name in self.labels_set.attrs:
            label_hist,_ = np.histogram(self.labels_set[:,:,self.labels_set.attrs[label_name]],range(self.num_classes+1))
            label_hist_1norm = label_hist / np.sum(label_hist).astype(np.float)
            self.labels_set.attrs[label_name+'_hist'] = label_hist
            self.labels_set.attrs[label_name+'_hist1norm'] = label_hist_1norm

    def Get_file_accuracies(self,IsWrite=False,out_path=None):
        # get the accuracy of each file by the pred data in hdf5
        if self.pred_logits_set.shape[0] != self.labels_set.shape[0]:
            return ''
        class_num = len(self.g_class2label)
        class_TP = np.zeros(shape=(class_num))
        class_FN = np.zeros(shape=(class_num))
        class_FP = np.zeros(shape=(class_num))
        total_num = self.raw_xyz_set.size

        for j in range(0,self.raw_xyz_set.shape[0]):
            xyz_block = self.raw_xyz_set[j,:]
            label_gt = self.labels_set[j,:]
            label_pred = self.pred_logits_set[j,:]
            for i in range(xyz_block.shape[0]):
                # calculate accuracy
                if (label_gt[i]==label_pred[i]):
                    class_TP[label_gt[i]] += 1
                else:
                    class_FN[label_gt[i]] += 1
                    class_FP[label_pred[i]] += 1
        acc_str,ave_acc_str = Normed_H5f.cal_accuracy(class_TP,class_FN,class_FP,total_num)
        return class_TP,class_FN,class_FP,total_num,acc_str,ave_acc_str

    @staticmethod
    def cal_accuracy(TP,FN,FP,total_num):
        precision = np.nan_to_num(TP/(TP+FP))
        recall = np.nan_to_num(TP/(TP+FN))
        IOU = np.nan_to_num(TP/(TP+FN+FP))
        # weighted ave
        real_Pos = TP+FN
        normed_real_TP = real_Pos/np.sum(real_Pos)
        ave_4acc = np.zeros(shape=(4),dtype=np.float)
        ave_4acc[0] = np.sum( precision*normed_real_TP )
        ave_4acc[1] = np.sum( recall*normed_real_TP )
        ave_4acc[2] = np.sum( IOU*normed_real_TP )
        ave_4acc[3] = np.sum(TP)/total_num
        ave_4acc_name = ['ave_class_pre','ave_class_rec','ave_class_IOU','ave_point_accu']
        # gen str
        delim = '' # ','
        def getstr(array,mean=None,str_format='%0.3g'):
            if mean!=None:
                mean_str = '%9s'%(str_format%mean) + delim
            else:
                mean_str = '%9s'%('  ')
                if delim != '': mean_str = mean_str + ' '

            return mean_str + delim.join(['%9s'%(str_format%v) for v in array])
        ave_acc_str = 'point average:  %0.3f,  class ave pre/rec/IOU: %0.3f/ %0.3f/ %0.3f    N = %f M'% \
            ( ave_4acc[0],  ave_4acc[1], ave_4acc[2], ave_4acc[3],total_num/1000000.0)
        acc_str = ave_acc_str + '\n\t       average'+delim  + delim.join(['%9s'%c for c in Normed_H5f.g_class2label])+'\n'
        acc_str += 'class_pre:   '+getstr(precision,ave_4acc[0])+'\n'
        acc_str += 'class_rec:   '+getstr(recall,ave_4acc[1])+'\n'
        acc_str += 'class_IOU:   '+getstr(IOU,ave_4acc[2])+'\n'
        acc_str += 'number(K):   '+getstr(np.trunc(real_Pos/1000.0),str_format='%d')+'\n'
        return acc_str,ave_acc_str

    def gen_gt_pred_obj_examples(self,config_flag = ['None'],out_path=None):
        #all_catigories = ['ceiling','floor','wall','beam','column','window','door','table','chair','sofa','bookcase','board','clutter']
        all_catigories = [key for key in self.g_class2label]
        #config_flag = ['Z','building_6_no_ceiling']
        #config_flag = ['all_single','Y']
        #config_flag = ['ALL','Y']
        #config_flag = ['Y']
        def get_config(config_flag):
            if config_flag == 'all_single':
                show_categaries_ls = [[c] for c in all_catigories]
                visu_flag = all_catigories
                return  [None]*len(all_catigories), show_categaries_ls, visu_flag
            else:
                if config_flag =='ALL':
                    xyz_cut_rate=None
                    show_categaries=None
                elif config_flag =='Y':
                    xyz_cut_rate=[0,0.92,1]
                    show_categaries=None
                elif config_flag =='Z':
                    xyz_cut_rate=[0,0,0.93]
                    show_categaries=None
                elif config_flag =='XZ':
                    xyz_cut_rate=[0.95,0,0.93]
                    show_categaries=None
                elif config_flag == 'wall':
                    show_categaries = ['wall']
                    xyz_cut_rate = None
                elif config_flag == 'all_single':
                    show_categaries = all_catigories
                    xyz_cut_rate = [None]*len(all_catigories)
                elif config_flag =='building_7':
                    xyz_cut_rate=None
                    show_categaries=['ceiling','floor','wall','beam','column','window','door']
                elif config_flag =='building_6_no_ceiling':
                    xyz_cut_rate=None
                    show_categaries=['floor','wall','beam','column','window','door']
                elif config_flag =='void':
                    xyz_cut_rate=None
                    show_categaries=['void']
                else:
                    return None, None
                return [xyz_cut_rate],[show_categaries],[config_flag]

        num_blocks = self.data_set.shape[0]
        vis_block_ids = range(num_blocks)

        for flag in config_flag:
            xyz_cut_rate_ls,show_categaries,visu_flags = get_config(flag)
            for i in range(len(xyz_cut_rate_ls)):
                for vis_block_id in vis_block_ids:
                    self.gen_gt_pred_obj(out_path,xyz_cut_rate_ls[i],show_categaries[i],
                                        pre_fn=str(visu_flags[i]),visu_flag=str(flag),block_id=vis_block_id )
           # self.Get_file_accuracies(IsWrite=True)

    def gen_gt_pred_obj(self,out_path=None,xyz_cut_rate=None,show_categaries=None,
                        pre_fn='',visu_flag=None,block_id=None):
        '''
            (1)xyz_cut_rate:
                # when rate < 0.5: cut small
                # when rate >0.5: cut big
            (2) show_categaries:  ['ceiling']
                the categaries to show, if None  show all
        '''
        if show_categaries != None:
            show_categaries = [self.g_class2label[c] for c in show_categaries]
        #if self.pred_logits_set.shape[0] ==0:
        #    print('File: %s \n   has no pred data'%(self.file_name))
        #    return
        base_fn = os.path.basename(self.file_name)
        base_fn = os.path.splitext(base_fn)[0]
        folder_path = os.path.dirname(self.file_name)
        if out_path == None:
            obj_folder = os.path.join(folder_path,'obj_file',base_fn)
            if visu_flag != None:
                obj_folder = os.path.join(obj_folder,visu_flag)
            if block_id != None:
                pre_fn += 'b'+str(block_id)+'_'
        else:
            obj_folder = os.path.join(out_path,base_fn)
        print('obj_folder=',obj_folder)
        if not os.path.exists(obj_folder):
            os.makedirs(obj_folder)

        raw_obj_fn = os.path.join(obj_folder, 'raw_'+pre_fn+'.obj')
        raw_colored_obj_fn = os.path.join(obj_folder, 'raw_color_'+pre_fn+'.obj')
        gt_obj_fn = os.path.join(obj_folder, 'gt_'+pre_fn+'.obj')
        pred_obj_fn = os.path.join(obj_folder,'pred_'+pre_fn+'.obj')
        dif_FN_obj_fn = os.path.join(obj_folder, 'dif_FN_'+pre_fn+'.obj')
        dif_FP_obj_fn = os.path.join(obj_folder, 'dif_FP_'+pre_fn+'.obj')
        correct_obj_fn = os.path.join(obj_folder,'correct_'+pre_fn+'.obj')
        correct_num = 0
        pred_num = 0
        raw_xyz_set = self.raw_xyz_set()
        if raw_xyz_set.ndim == 4:
            raw_xyz_set = np.reshape(raw_xyz_set,(raw_xyz_set.shape[0],-1,raw_xyz_set.shape[-1]))
        file_size = raw_xyz_set.shape[0] * raw_xyz_set.shape[1]

        if xyz_cut_rate != None:
            # when rate < 0.5: cut small
            # when rate >0.5: cut big
            xyz_max = np.array([np.max(raw_xyz_set[:,:,i]) for i in range(3)])
            xyz_min = np.array([np.min(raw_xyz_set[:,:,i]) for i in range(3)])
            xyz_scope = xyz_max - xyz_min
            xyz_thres = xyz_scope * xyz_cut_rate + xyz_min
            print('xyz_thres = ',str(xyz_thres))
        cut_num = 0

        with open(gt_obj_fn,'w') as gt_f, open(raw_obj_fn,'w') as raw_f, open(raw_colored_obj_fn,'w') as raw_colored_f:
          with open(pred_obj_fn,'w') as pred_f,open(dif_FN_obj_fn,'w') as dif_FN_f,open(dif_FP_obj_fn,'w') as dif_FP_f:
            with open(correct_obj_fn,'w') as correct_f:
                for j in range(0,raw_xyz_set.shape[0]):
                    if block_id != None and block_id != j:
                        continue
                    xyz_block = raw_xyz_set[j,...]
                    label_gt = np.reshape( self.labels_set[j,...],(-1,3)  )[:,self.label_ele_idxs['label_category'][0]]
                    color_block = (np.reshape( self.data_set[...,self.data_set.attrs['color_1norm']],(-1,3) )*255).astype(np.uint8)
                    if self.pred_logits_set.shape[0] !=0 and  j < self.pred_logits_set.shape[0]:
                        IsGenPred = True
                        label_pred = np.reshape( self.pred_logits_set[j,:], (-1,3) )[:,self.label_ele_idxs['label_category'][0]]
                    else:
                        IsGenPred = False
                    for i in range(xyz_block.shape[0]):

                        # cut parts by xyz or label
                        is_cut_this_point = False
                        if show_categaries!=None and label_gt[i] not in show_categaries:
                                #label_pred[i] not in show_categaries:
                            # cut by category
                            is_cut_this_point = True
                        elif xyz_cut_rate!=None:
                            # cut by position
                            for xyz_j in range(3):
                                if (xyz_cut_rate[xyz_j] >0.5 and xyz_block[i,xyz_j] > xyz_thres[xyz_j]) or \
                                    (xyz_cut_rate[xyz_j]<=0.5 and xyz_block[i,xyz_j] < xyz_thres[xyz_j]):
                                    is_cut_this_point =  True
                        if is_cut_this_point:
                            cut_num += 1
                            continue

                        color_gt = self.label2color( label_gt[i] )
                        str_xyz = 'v ' + ' '.join( ['%0.3f'%(d) for d in  xyz_block[i,:] ])
                        str_xyz = str_xyz + ' \t'
                        str_raw_color = ' '.join( ['%d'%(d) for d in  color_block[i,:]]) + '\n'
                        str_color_gt = ' '.join( ['%d'%(d) for d in  color_gt]) + '\n'
                        str_gt = str_xyz + str_color_gt

                        if show_categaries == None or label_gt[i] in show_categaries:
                            raw_f.write(str_xyz+'\n')
                            raw_colored_f.write(str_xyz+str_raw_color)
                            gt_f.write( str_gt )

                        if IsGenPred and label_pred[i] in self.g_label2color:
                            color_pred = self.label2color( label_pred[i] )
                            str_color_pred = ' '.join( ['%d'%(d) for d in  color_pred]) + '\n'
                            str_pred = str_xyz + str_color_pred
                            if show_categaries==None or label_pred[i] in show_categaries:
                                pred_f.write( str_pred )
                            if label_gt[i] != label_pred[i]:
                                if show_categaries == None or label_gt[i] in show_categaries:
                                    dif_FN_f.write(str_pred)
                                if show_categaries == None or label_pred[i] in show_categaries:
                                    dif_FP_f.write(str_gt)
                            else:
                                correct_f.write(str_pred)
                                correct_num += 1
                            pred_num += 1
                    if j%20 ==0: print('batch %d / %d'%(j,raw_xyz_set.shape[0]))


                print('gen gt obj file (%d): \n%s'%(file_size,gt_obj_fn) )
                if pred_num > 0:
                     print('gen pred obj file (%d,%f): \n%s '%(pred_num,1.0*pred_num/file_size,pred_obj_fn) )
                     print('gen correct obj file (%d,%f),: \n%s '%(correct_num,1.0*correct_num/pred_num,correct_obj_fn) )
                print('gen dif obj file: ',pred_obj_fn)
                print('cut roof ponit num = %d, xyz_cut_rate = %s'%(cut_num,str(xyz_cut_rate)) )


def MergeNormed_H5f(in_filename_ls,merged_filename, Always_CreateNew = False, IsShowSummaryFinished=False):
    if len(in_filename_ls) == 0:
        print('no .nh5/.prh5 file in the list')
        return
    if not Always_CreateNew:
        IsIntact,_ = Normed_H5f.check_nh5_intact(merged_filename)
        if IsIntact:
            print('nh5/prh5 file intact: %s'%(merged_filename))
            return
    if not os.path.exists( os.path.dirname(merged_filename) ):
        os.makedirs( os.path.dirname(merged_filename) )
    print('start generating merged file: %s'%(merged_filename))
    with h5py.File(merged_filename,'w') as merged_h5f:
        for k,fn in enumerate(in_filename_ls):
            print('merging %s'%(fn))
            with h5py.File(fn,'r') as in_h5f:
                if 'intact_void_file' in in_h5f.attrs:
                    continue
                if k == 0:
                    merged_normed_h5f = Normed_H5f(merged_h5f,merged_filename,in_h5f.attrs['datasource_name'])
                    merged_normed_h5f.copy_root_attrs_from_normed(in_h5f,'MergeNormed_H5f')
                else:
                    merged_normed_h5f.h5f['bidxmaps_sample_group'].attrs['sum_sg_bidxmap_sample_num'] += in_h5f['bidxmaps_sample_group'].attrs['sum_sg_bidxmap_sample_num']
                    merged_normed_h5f.h5f['bidxmaps_sample_group'].attrs['sum_flatten_bmap_sample_num'] += in_h5f['bidxmaps_sample_group'].attrs['sum_flatten_bmap_sample_num']

                in_normed_h5f = Normed_H5f(in_h5f,fn)
                merged_normed_h5f.append_to_dset('data',in_normed_h5f.data_set)
                merged_normed_h5f.append_to_dset('labels',in_normed_h5f.labels_set)
                merged_normed_h5f.append_to_dset('bidxmaps_sample_group',in_normed_h5f.h5f['bidxmaps_sample_group'])
                merged_normed_h5f.append_to_dset('bidxmaps_flatten',in_normed_h5f.h5f['bidxmaps_flatten'])
        merged_normed_h5f.create_done()
        if IsShowSummaryFinished:
            merged_normed_h5f.show_summary_info()
        print('merged h5f OK: %s'%(merged_filename))

def Write_all_file_accuracies(normed_h5f_file_list=None,out_path=None,pre_out_fn=''):
    if normed_h5f_file_list == None:
        normed_h5f_file_list = glob.glob( GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096 +
                            '/Area_2_office_1*' )
    if out_path == None: out_path = os.path.join(GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096,
                                    'pred_accuracy')
    if not os.path.exists(out_path):
        os.makedirs(out_path)
    all_acc_fn = os.path.join(out_path,pre_out_fn+'accuracies.txt')
    all_ave_acc_fn = os.path.join(out_path,pre_out_fn+'average_accuracies.txt')
    class_TP = class_FN = class_FP = np.zeros(shape=(len(Normed_H5f.g_class2label)))
    total_num = 0
    average_class_accu_ls = []
    with open(all_acc_fn,'w') as all_acc_f,open(all_ave_acc_fn,'w') as all_ave_acc_f:
        for i,fn in enumerate(normed_h5f_file_list):
            h5f = h5py.File(fn,'r')
            norm_h5f = Normed_H5f(h5f,fn)
            class_TP_i,class_FN_i,class_FP_i,total_num_i,acc_str_i,ave_acc_str_i = norm_h5f.Get_file_accuracies(
                IsWrite=False, out_path = out_path)
            class_TP = class_TP_i + class_TP
            class_FN = class_FN_i + class_FN
            class_FP = class_FP_i + class_FP
            total_num = total_num_i +  total_num

            if acc_str_i != '':
                all_acc_f.write('File: '+os.path.basename(fn)+'\n')
                all_acc_f.write(acc_str_i+'\n')
                all_ave_acc_f.write(ave_acc_str_i+'\t: '+os.path.basename(fn)+'\n')

        acc_str,ave_acc_str = Normed_H5f.cal_accuracy(class_TP,class_FN,class_FP,total_num)
        ave_str = 'Throughout All %d files.\n'%(i+1) +  acc_str
        all_acc_f.write('\n'+ave_str)
        all_ave_acc_f.write('\n'+ave_str)
    print('accuracy file: '+all_acc_fn)
    print('average accuracy file: '+all_ave_acc_fn)
    return ave_str,out_path,class_TP,class_FN,class_FP,total_num

def Write_Area_accuracies():
    ave_str_areas = ''
    class_TP = class_FN = class_FP = np.zeros(shape=(len(Normed_H5f.g_class2label)))
    total_num = 0
    for i in range(6):
        glob_i = 'Area_%d'%(i+1)
        normed_h5f_file_list = glob.glob( os.path.join(GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096,
                                glob_i+'*') )
        ave_str,out_path,class_TP_i,class_FN_i,class_FP_i,total_num_i = Write_all_file_accuracies(normed_h5f_file_list,pre_out_fn=glob_i+'_')
        class_TP = class_TP_i + class_TP
        class_FN = class_FN_i + class_FN
        class_FP = class_FP_i + class_FP
        total_num = total_num_i + total_num

        ave_str_areas += '\nArea%d\n'%i
        ave_str_areas += ave_str
    acc_str,ave_acc_str = Normed_H5f.cal_accuracy(class_TP,class_FN,class_FP,total_num)
    all_area_str = '\nThrough %d areas.\n'%(i+1)+acc_str
    with open(os.path.join(out_path,'areas_accuracies.txt'),'w' ) as area_acc_f:
        area_acc_f.write(ave_str_areas)
        area_acc_f.write(all_area_str)



#-------------------------------------------------------------------------------
# Test above codes
#-------------------------------------------------------------------------------
def Gen_raw_label_color_obj():
    base_fn = os.path.join(GLOBAL_PARA.ETH_raw_partA,'untermaederbrunnen_station1_xyz_intensity_rgb')
    data_fn = base_fn + '.txt'
    label_fn = base_fn + '.labels'
    obj_fn = base_fn + '.obj'
    obj_labeled_fn = base_fn + '_labeled.obj'
    obj_unlabeled_fn = base_fn + '_unlabeled.obj'
    labeled_N = 0
    unlabeled_N = 0
    with open(data_fn,'r') as data_f:
     with open(label_fn,'r') as label_f:
      with open(obj_fn,'w') as obj_f:
       with open(obj_labeled_fn,'w') as obj_labeled_f:
        with open(obj_unlabeled_fn,'w') as obj_unlabeled_f:
            data_label_fs = itertools.izip(data_f,label_f)
            for k,data_label_line in enumerate(data_label_fs):
                data_k =np.fromstring( data_label_line[0].strip(),dtype=np.float32,sep=' ' )
                label_k = np.fromstring( data_label_line[1].strip(),dtype=np.int16,sep=' ' )
                color_k = Normed_H5f.g_label2color[label_k[0]]
                str_k = 'v ' + ' '.join( [str(d) for d in data_k[0:3] ] ) + ' \t' +\
                    ' '.join( [str(c) for c in color_k] ) + '\n'
                obj_f.write(str_k)
                if label_k == 0:
                    unlabeled_N += 1
                    obj_unlabeled_f.write(str_k)
                else:
                    labeled_N += 1
                    obj_labeled_f.write(str_k)
                if k%(1000*100) == 0:
                    print('gen raw obj %d'%(k))
                if k > 1000*1000*1:
                    break
    total_N = k+1
    print('total_N = %d, labeled_N = %d (%0.3f), unlabeled_N = %d (%0.3f)'%\
          (total_N,labeled_N,1.0*labeled_N/total_N,unlabeled_N,1.0*unlabeled_N/total_N))

def Do_Norm(file_list):
    for fn in file_list:
        with h5py.File(fn,'r') as f:
            sf = Sorted_H5f(f,fn)
            sf.file_normalize_to_NormedH5F()

def Do_sample(file_list):
    #h5f_name = os.path.join(GLOBAL_PARA.ETH_A_stride_8_step_8,\
                      #'bildstein_station5_sub_m80_m5_stride_8_step_8.h5')
    for h5f_name in file_list:
        sample_num =  4096
        sample_method = 'random'
        with h5py.File(h5f_name,'r') as h5f:
            sh5f = Sorted_H5f(h5f,h5f_name)
            sh5f.file_random_sampling(sample_num)


def Test_get_block_data_of_new_stride_step():
    folder_base = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1'
    base_h5f_name = os.path.join(folder_base,'region9.sh5')

    xyz1norm_k = [0.2,0.3,0.1]
    new_stride = np.array([0.1,0.1,0.1])*10
    new_step = np.array([0.1,0.1,0.1])*20
    feed_data_elements=['xyz_midnorm','color_1norm']
    feed_label_elements=['label_category','label_instance']
    sample_num=8

    with h5py.File(base_h5f_name,'r') as base_h5f:
        GlobalSubBaseBLOCK.save_bmap_between_dif_stride_step(base_h5f,base_h5f_name)
        #GlobalSubBaseBLOCK.show_all(base_h5f,base_h5f_name)

        base_sh5f = Sorted_H5f(base_h5f,base_h5f_name)
        base_sh5f.file_saveas_pyramid_feed(True)
        #base_sh5f.load_blockids(new_stride,new_step)
        #base_sh5f.get_block_data_of_new_stride_step_byxyz1norm(xyz1norm_k,new_stride,new_step,feed_data_elements,feed_label_elements,sample_num)
        #base_sh5f.show_summary_info()

def Do_normalize_sorted_to_self():
    folder_base = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1'
    base_h5f_name = os.path.join(folder_base,'region0.sh5')
    with h5py.File(base_h5f_name,'r') as base_h5f:
        base_sh5f = Sorted_H5f(base_h5f,base_h5f_name)
        base_sh5f.file_normalize_to_self()

def Test_sub_block_ks_():
    folder_base = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1'
    folder_new = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1_testtmp'
    if not os.path.exists(folder_new):
        os.makedirs(folder_new)
    base_h5f_name = os.path.join(folder_base,'region0.sh5')
    new_h5f_name = os.path.join(folder_new,'region0_testtmp.sh5')

    xyz1norm_k = [0.2,0.3,0.1]
    new_stride = [0.1,0.1,0.1]
    new_step = [0.1,0.1,0.1]

    with h5py.File(base_h5f_name,'r') as base_h5f:
        with h5py.File(new_h5f_name,'w') as new_h5f:
            base_sh5f = Sorted_H5f(base_h5f,base_h5f_name)
            new_sh5f = Sorted_H5f(new_h5f,new_h5f_name)
            new_sh5f.copy_root_summaryinfo_from_another(base_h5f,'new_stride')
            new_sh5f.set_step_stride(new_step,new_stride)
            #base_sh5f.show_summary_info()

            new_block_id_ls,i_xyz_new_ls,org_blockid0 = base_sh5f.get_blockids_of_dif_stride_step_byxyz(xyz1norm_k, new_stride, new_step )
            #print(new_block_id_ls)
            #print(i_xyz_new_ls)
            for new_block_id in new_block_id_ls:
                org_block_id_ls,org_i_xyz_ls = Sorted_H5f.get_blockids_of_dif_stride_step(new_block_id, base_attrs = new_h5f.attrs, aim_attrs= base_h5f.attrs)
                assert org_blockid0 in org_block_id_ls
                #print(org_block_id_ls)

    print('check get_sub_block_ks OK')



def Do_Check_xyz():
    #fnl = glob.glob(os.path.join(folder,'*.hdf5'))
    #for fn in fnl:
        raw_fn = os.path.join(GLOBAL_PARA.ETH_A_rawh5,'bildstein_station5_xyz_intensity_rgb.hdf5')
        fn_s = os.path.join( GLOBAL_PARA.ETH_A_stride_1_step_1,'bildstein_station5_sub_m80_m5_stride_2_step_4.h5')
        fn_s = os.path.join( GLOBAL_PARA.ETH_A_stride_1_step_1,'bildstein_station5_sub_m80_m5_stride_4_step_8.h5')
        print('checking equal and  xyz scope of file: ',fn_s)
        with h5py.File(raw_fn,'r') as h5f:
            with h5py.File(fn_s,'r') as sh5f:
                sorted_h5f = Sorted_H5f(sh5f,raw_fn)
                #sorted_h5f.show_summary_info()
               # flag1 = sorted_h5f.check_equal_to_raw(h5f)
               # if flag1:
               #     print('equal check passed')
               # else:
               #     print('equal check failed')
                flag2 = sorted_h5f.check_xyz_scope()
                if flag2:
                    print('xyz scope check passed')
                else:
                    print('xyz scope check failed')


def Do_extract_sub_area():
    folder = GLOBAL_PARA.ETH_A_rawh5
    fnl = glob.glob(os.path.join(folder,'bildstein_station5_stride_1_step_1.h5'))
    #sub_xyz_scope = np.array([[-30,-30,-20],[0,0,50]])
    #new_flag = '_sub_m30_0'
    sub_xyz_scope = np.array([[-80,-80,-20],[-5,-5,50]])
    new_flag = '_sub_m80_m5'
    print('sub_scope:\n',sub_xyz_scope)
    for fn in fnl:
        fn_parts =  os.path.splitext(fn)
        new_name = fn_parts[0]+new_flag+fn_parts[1]
        print('sub file name: ',new_name)
        with h5py.File(fn,'r') as s_h5f:
            sorted_h5f = Sorted_H5f(s_h5f,fn)
            sorted_h5f.extract_sub_area(sub_xyz_scope,new_name)


def Add_sorted_total_row_block_N_onefile(fn):
        print('calculating row_N block_N of: ',fn)
        with h5py.File(fn,'a') as h5f:
            sorted_h5f = Sorted_H5f(h5f,fn)
            rN,bN = sorted_h5f.add_total_row_block_N()
            print('rn= ',rN, '  bN= ',bN,'\n')
def Add_sorted_total_row_block_N():
    folder = GLOBAL_PARA.ETH_A_step_20_stride_10
    fnl = glob.glob(os.path.join(folder,'*.hdf5'))
    IsMulti_aN = False
    if not IsMulti_aN:
        for fn in fnl:
            Add_sorted_total_row_block_N_onefile(fn)
    else:
        p = mp.Pool(3)
        p.map(Add_sorted_total_row_block_N_onefile,fnl)
        p.close()
        p.join()


def Do_gen_raw_obj():
    ETH_training_partAh5_folder =  GLOBAL_PARA.ETH_A_rawh5
    folder_path = ETH_training_partAh5_folder
    file_list = glob.glob( os.path.join(folder_path,'b*.hdf5') )
    IsLabelColor = True
    for fn in file_list:
        print(fn)
        if IsLabelColor:
            meta_fn = '_labeledColor'
        else:
            meta_fn = ''
        obj_fn = os.path.splitext(fn)[0]+meta_fn+'.obj'
        with h5py.File(fn,'r') as  raw_h5_f:
            raw_h5f = Raw_H5f(raw_h5_f)
            raw_h5f.generate_objfile(obj_fn,IsLabelColor)

def Do_gen_sorted_block_obj(file_list):
    for fn in file_list:
        with  h5py.File(fn,'r') as h5f:
            sorted_h5f = Sorted_H5f(h5f,fn)
            sorted_h5f.gen_file_obj(True)

def Do_gen_normed_obj(file_list):
    for fn in file_list:
        with  h5py.File(fn,'r') as h5f:
            norm_h5f = Normed_H5f(h5f,fn)
            norm_h5f.gen_gt_pred_obj()

def Do_gen_gt_pred_objs(file_list=None):
    if file_list == None:
        folder = GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096
        # many chairs and tables
        #file_list = glob.glob(os.path.join(folder,'Area_1_office_16_stride_0.5_step_1_random_4096_globalnorm.nh5'))
        # simple only one table
        #file_list = glob.glob(os.path.join(folder,'Area_6_pantry_1_stride_0.5_step_1_random_4096_globalnorm.nh5'))

        # wall good 0.88M
        fn_glob_good = 'Area_6_office_25_stride_0.5_step_1_random_4096_globalnorm.nh5'
        # all poor ave 0.5  1.5M
        fn_glob_poor = 'Area_5_storage_1_stride_0.5_step_1_random_4096_globalnorm.nh5'
        # test wall recall low
        fn_glob_test = 'Area_1_office_10_stride_0.5_step_1_random_4096_globalnorm.nh5'
        fn_glob = [fn_glob_good,fn_glob_poor,fn_glob_test]
        file_list = [ os.path.join(folder,fn)  for fn in fn_glob]

    for fn in file_list:
        with h5py.File(fn,'r') as h5f:
            norm_h5f = Normed_H5f(h5f,fn)
            norm_h5f.gen_gt_pred_obj_examples()

def gen_file_list(folder,format='h5'):
    file_list = glob.glob( os.path.join(folder,'*.'+format) )
    print(file_list)
    with open(os.path.join(folder,'all_files.txt'),'w') as f:
        for fn in file_list:
            base_filename = os.path.basename(fn)
            base_dirname = os.path.basename( os.path.dirname(fn) )
            base_dir_file_name = os.path.join(base_dirname,base_filename)
            f.write( base_dir_file_name )
            print(base_dir_file_name)
    print('all file list file write OK ')



def main(file_list):

    outdoor_prep = MAIN_DATA_PREP()
    actions = ['merge','sample_merged','obj_sampled_merged','norm_sampled_merged']
    actions = ['merge','sample_merged','norm_sampled_merged']
    outdoor_prep.main(file_list,actions,sample_num=4096,sample_method='random',\
                      stride=[8,8,-1],step=[8,8,-1])

    #outdoor_prep.Do_sort_to_blocks()
    #Do_extract_sub_area()
    #outdoor_prep.test_sub_block_ks()
    #outdoor_prep.DO_add_geometric_scope_file()
    #outdoor_prep.DO_gen_rawETH_to_h5()

def show_h5f_file():
    fn = '/home/y/Research/dynamic_pointnet/data/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1/region2.sh5'
    fn = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1_pyramid-1_2-512_128_64_16-0d2_0d4_0d8_16/region2.prh5'
    with h5py.File(fn,'r') as h5f:
        show_h5f_summary_info(h5f)

if __name__ == '__main__':
 #   file_list = glob.glob( os.path.join(GLOBAL_PARA.ETH_A_stride_1_step_1, \
 #               '*_m5.h5') )
    #file_list = glob.glob( os.path.join(GLOBAL_PARA.ETH_A_stride_8_step_8, \
                #'*_4096.h5') )
   # file_list = glob.glob( os.path.join(GLOBAL_PARA.seg_train_path, \
   #             '*.h5') )
    #main(file_list)
    #Do_gen_raw_obj()
    #Add_sorted_total_row_block_N()
    #Do_Check_xyz()
    #Test_sub_block_ks_()
    #Do_sample()
    #Do_gen_sorted_block_obj(file_list)
    #Do_gen_normed_obj(file_list)
    #Do_Norm(file_list)
    #gen_file_list(GLOBAL_PARA.seg_train_path)
    #Do_gen_gt_pred_objs()


    #Write_Area_accuracies()
    #Write_all_file_accuracies()
    #Test_get_block_data_of_new_stride_step()
    #Do_normalize_sorted_to_self()
    #Normed_H5f.show_all_colors()
    #Gen_raw_label_color_obj()

    show_h5f_file()
    T = time.time() - START_T
    print('exit main, T = ',T)

#xyz Sep 2017
'''
Data preparation for datsets: stanford_indoor, scannet, ETH_semantic3D
Core idea: store all the information in hdf5 file itself

# The workflow to use this tool:
Raw_H5f -> Sorted_H5f -> merge block to get new block size -> randomnly select n points
    -> Normed_H5f -> Net_Provider

## Raw_H5f store the raw data of dataset, which contains several datasets: xyz, label, color.... Each dataset
    stores the whole data for one dtype data.
    (.rh5)
## Sorted_H5f contains lots of lots of dataset. Each dataset stores all types of data within a spacial block.
    The point number of each block/dataset can be fix or not.
    (.sh5) Use class Sort_RawH5f to generate sorted file with unfixed point num in each block, and a small stride / step size.
    Then merge .sh5 file with small stride/step size to get larger size block.
    (.rsh5) Randomly sampling .sh5 file to get Sorted_H5f file with fixed point number in each block.
## Normed_H5f includes 4 datasets: data, label, raw_xyz, pred_logit
    (.nh5) This file is directly used to feed data for deep learning models.
    .nh5 file is generated by Sorted_H5f.file_normalization()
## For all three files, show_h5f_summary_info() can use to show the info summary.
## scannet_block_sample.py is the basic usage for these classes.
'''

from __future__ import print_function
import os
import sys
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(BASE_DIR)
#from plyfile import (PlyData, PlyElement, make2d, PlyParseError, PlyProperty)
import numpy as np
import h5py
import glob
import time
import multiprocessing as mp
import itertools
#import argparse
from global_para import GLOBAL_PARA

START_T = time.time()

g_h5_num_row_1M = 50*1000
ROOT_DIR = os.path.dirname(BASE_DIR)
UPER_DIR = os.path.dirname(ROOT_DIR)
DATA_DIR = os.path.join(ROOT_DIR,'data')

DATA_SOURCE_NAME_LIST = ['ETH','STANFORD_INDOOR3D','SCANNET']
def rm_file_name_midpart(fn,rm_part):
    base_name = os.path.basename(fn)
    parts = base_name.split(rm_part)
    if len(parts)>1:
        new_bn = parts[0] + parts[1]
    else:
        new_bn = parts[0]
    new_fn = os.path.join(os.path.dirname(fn),new_bn)
    return new_fn


def show_h5f_summary_info(h5f):
    root_attrs = [attr for attr in h5f.attrs]
    print('*** The root_attr: ',root_attrs)
    key_root_attrs = ['datasource_name','element_names','total_row_N','total_block_N','block_step',\
                    'block_stride','block_dims_N','xyz_min_aligned','xyz_max_aligned',\
                    'xyz_scope_aligned']

    for attr in key_root_attrs:
        if attr in h5f.attrs:
            print(attr+':',h5f.attrs[attr] )

    print('\n*** The datasets')
    for k, dset_n in enumerate(h5f):
        dset = h5f[dset_n]
        if k < 10:
            print('# dataset %d: '%(k),dset_n,'  shape=',dset.shape)
        if k < 5:
            for attr in dset.attrs:
                print(attr,' = ',dset.attrs[attr])
            print(dset[0:min(2,dset.shape[0]),:])
            if dset.shape[0] > 3:
                print(dset[-1,:])
            print('\n')
    print('%d datasets totally'%(k+1))

class Raw_H5f():
    '''
    * raw data:unsorted points,all the time in one dataset
    * Each data type as a hdf5 dataset: xyz, intensity, label, color
    * class "Sorted_H5f" will sort data to blocks based on this class
    '''
    file_flag = 'RAW_H5F'
    h5_num_row_1M = 50*1000
    dtypes = { 'xyz':np.float32, 'intensity':np.int32, 'color':np.uint8,'label':np.uint8 }
    num_channels = {'xyz':3,'intensity':1,'color':3,'label':1}
    def __init__(self,raw_h5_f,file_name,datasource_name=None):
        self.raw_h5f = raw_h5_f
        if datasource_name == None:
            assert 'datasource_name' in self.raw_h5f.attrs
        else:
            self.raw_h5f.attrs['datasource_name'] = datasource_name
        assert self.raw_h5f.attrs['datasource_name'] in DATA_SOURCE_NAME_LIST
        self.get_summary_info()
        self.file_name = file_name
        self.num_default_row = 0

    def show_h5f_summary_info(self):
        print('\n\nsummary of file: ',self.file_name)
        show_h5f_summary_info(self.h5f)

    def set_num_default_row(self,N):
        self.num_default_row = N

    def get_dataset(self,data_name):
        if data_name in self.raw_h5f:
            return self.raw_h5f[data_name]
        assert(data_name in self.dtypes)
        nc = self.num_channels[data_name]
        dset = self.raw_h5f.create_dataset(data_name,shape=(self.num_default_row,nc),\
                                    maxshape=(None,nc),dtype=self.dtypes[data_name],\
                                    chunks = (self.h5_num_row_1M,nc),\
                                    compression = "gzip")
        dset.attrs['valid_num'] = 0
        setattr(self,data_name+'_dset',dset)
        if 'element_names' not in self.raw_h5f.attrs:
            self.raw_h5f.attrs['element_names'] = [data_name]
        else:
            self.raw_h5f.attrs['element_names'] = [data_name]+[e for e in self.raw_h5f.attrs['element_names']]
        return dset
    def get_total_num_channels_name_list(self):
        total_num_channels = 0
        data_name_list = [str(dn) for dn in self.raw_h5f]
        for dn in data_name_list:
            total_num_channels += self.num_channels[dn]

        return total_num_channels,data_name_list

    def append_to_dset(self,dset_name,new_data):
       self.add_to_dset(dset_name,new_data,None,None)

    def get_all_dsets(self,start_idx,end_idx):
        out_dset_order = ['xyz','color','label','intensity']
        data_list = []
        for dset_name in out_dset_order:
            if dset_name in self.raw_h5f:
                data_k = self.raw_h5f[dset_name][start_idx:end_idx,:]
                data_list.append(data_k)
        data = np.concatenate(data_list,1)
        return data

    def add_to_dset(self,dset_name,new_data,start,end):
        dset = self.get_dataset(dset_name)
        valid_n  = dset.attrs['valid_num']
        if start == None:
            start = valid_n
            end = start + new_data.shape[0]
        if dset.shape[0] < end:
            dset.resize((end,dset.shape[1:]))
        if valid_n < end:
            dset.attrs['valid_num'] = end
        if new_data.ndim==1 and dset.ndim==2 and dset.shape[1]==1:
            new_data = np.expand_dims(new_data,1)
        dset[start:end,:] = new_data

    def rm_invalid(self):
        for dset_name in self.raw_h5f:
            dset = self.raw_h5f[dset_name]
            if 'valid_num' in dset.attrs:
                valid_num = dset.attrs['valid_num']
                if valid_num < dset.shape[0]:
                    dset.resize( (valid_num,dset.shape[1:]) )

    def get_summary_info(self):
        for dset_name in self.raw_h5f:
            setattr(self,dset_name+'_dset',self.raw_h5f[dset_name])
        if 'xyz' in self.raw_h5f:
            self.total_row_N = self.xyz_dset.shape[0]
            self.xyz_max = self.xyz_dset.attrs['max']
            self.xyz_min = self.xyz_dset.attrs['min']
            self.xyz_scope = self.xyz_max - self.xyz_min


    def generate_objfile(self,obj_file_name,IsLabelColor):
        with open(obj_file_name,'w') as out_obj_file:
            xyz_dset = self.xyz_dset
            color_dset = self.color_dset
            label_dset = self.label_dset

            row_step = self.h5_num_row_1M * 10
            row_N = xyz_dset.shape[0]
            for k in range(0,row_N,row_step):
                end = min(k+row_step,row_N)
                xyz_buf_k = xyz_dset[k:end,:]
                color_buf_k = color_dset[k:end,:]
                buf_k = np.hstack((xyz_buf_k,color_buf_k))
                label_k = label_dset[k:end,0]
                for j in range(0,buf_k.shape[0]):
                    if not IsLabelColor:
                        str_j = 'v   ' + '\t'.join( ['%0.5f'%(d) for d in  buf_k[j,0:3]]) + '  \t'\
                        + '\t'.join( ['%d'%(d) for d in  buf_k[j,3:6]]) + '\n'
                    else:
                        label = label_k[j]
                        label_color = Normed_H5f.g_label2color[label]
                        str_j = 'v   ' + '\t'.join( ['%0.5f'%(d) for d in  buf_k[j,0:3]]) + '  \t'\
                        + '\t'.join( ['%d'%(d) for d in  label_color ]) + '\n'
                    out_obj_file.write(str_j)

                rate = int(100.0 * end / row_N)
                e = row_step / row_N
                if rate > 3 and rate % 3 <= e:
                    print('gen raw obj: %d%%'%(rate))
                if rate > 3:
                    break

    def create_done(self):
        self.rm_invalid()
        self.add_geometric_scope()

    def add_geometric_scope(self,line_num_limit=None):
        ''' calculate the geometric scope of raw h5 data, and add the result to attrs of dset'''
        #begin = time.time()
        max_xyz = -np.ones((3))*1e10
        min_xyz = np.ones((3))*1e10

        xyz_dset = self.xyz_dset
        row_step = self.h5_num_row_1M
        print('File: %s   %d lines'\
              %(os.path.basename(self.file_name),xyz_dset.shape[0]) )
        #print('read row step = %d'%(row_step))

        for k in range(0,xyz_dset.shape[0],row_step):
            end = min(k+row_step,xyz_dset.shape[0])
            xyz_buf = xyz_dset[k:end,:]
            xyz_buf_max = xyz_buf.max(axis=0)
            xyz_buf_min = xyz_buf.min(axis=0)
            max_xyz = np.maximum(max_xyz,xyz_buf_max)
            min_xyz = np.minimum(min_xyz,xyz_buf_min)

            if line_num_limit!=None and k > line_num_limit:
                print('break at k = ',line_num_limit)
                break
        xyz_dset.attrs['max'] = max_xyz
        xyz_dset.attrs['min'] = min_xyz
        self.raw_h5f.attrs['xyz_max'] = max_xyz
        self.raw_h5f.attrs['xyz_min'] = min_xyz
        max_str = '  '.join([ str(e) for e in max_xyz ])
        min_str = '  '.join([ str(e) for e in min_xyz ])
        print('max_str=%s\tmin_str=%s'%(max_str,min_str) )
        #print('T=',time.time()-begin)


class Sorted_H5f():
    '''
    (1) sorted: sort Raw_H5f by position to blocks, each block in one dataset.
        The dataset name is the voxel index.
    (2) store all types of data (xyz,color,intensity,label..) together (float32) in one dataset
    (3) All the information are stored in self.h5f.attrs,like:
[u'xyz_max', u'xyz_min', u'element_names', u'block_step', u'block_stride', u'block_dims_N', u'xyz_min_aligned', u'xyz_max_aligned', u'xyz_scope_aligned']
    '''
    file_flag = 'SORTED_H5F'
    data_name_list_candidate = ['xyz','color','label','intensity','org_row_index']
    data_channels = {'xyz':3,'color':3,'label':1,'intensity':1,'org_row_index':1}
    IS_CHECK = False # when true, store org_row_index
    data_idxs = {}
    total_num_channels = 0

    actions = ''
    h5_num_row_1M = g_h5_num_row_1M

    def __init__(self,h5f,file_name=None):
        self.h5f = h5f
        if file_name != None:
            self.file_name = file_name
        else:
            self.file_name = None
        self.reduced_num = 0
        self.update_data_index_by_elementnames()
        #self.show_summary_info()


    def show_summary_info(self):
        print('\n\nsummary of file: ',self.file_name)
        show_h5f_summary_info(self.h5f)

    def update_data_index_by_elementnames(self):
        # update by self.h5f.attrs['element_names']
        data_index = {}
        last_index = 0
        if 'element_names' in self.h5f.attrs:
            element_names = self.h5f.attrs['element_names']
        else:
            element_names = []
        if self.IS_CHECK and 'org_row_index' not in element_names:
            element_names += ['org_row_index']
        element_names = set(element_names)
        for dn in self.data_name_list_candidate:
            if dn in element_names:
                data_index[dn] = range(last_index,last_index+self.data_channels[dn])
                last_index += self.data_channels[dn]
        self.data_idxs = data_index
        self.total_num_channels = last_index

    def set_step_stride(self,block_step,block_stride,stride_to_align=1):
        self.h5f.attrs['block_step'] = block_step
        self.h5f.attrs['block_stride'] = block_stride
        self.h5f.attrs['stride_to_align'] = stride_to_align
        self.update_align_scope_by_stridetoalign()

    def update_align_scope_by_stridetoalign(self):
        if 'xyz_min' not in self.h5f.attrs:
            return
        xyz_min = self.h5f.attrs['xyz_min']
        xyz_max = self.h5f.attrs['xyz_max']
        xyz_min_aligned = xyz_min - xyz_min % self.h5f.attrs['stride_to_align'] - [0,0,1]
        xyz_max_aligned = xyz_max - xyz_max % 1 + 1
        xyz_scope_aligned =  xyz_max_aligned - xyz_min_aligned

        # step or stride ==-1 means one step/stride the whole scene
        if 'block_step' in self.h5f.attrs:
            block_step = self.h5f.attrs['block_step']
            block_stride = self.h5f.attrs['block_stride']
            for i in range(0,len(block_step)):
                if block_step[i]  == -1:
                    block_step[i] = xyz_scope_aligned[i]
                if block_stride[i]  == -1:
                    block_stride[i] = xyz_scope_aligned[i]
            self.h5f.attrs['block_step']  = block_step
            self.h5f.attrs['block_stride'] = block_stride
            self.h5f.attrs['block_dims_N'] = np.ceil(xyz_scope_aligned / self.h5f.attrs['block_stride']).astype(np.int64)
        self.h5f.attrs['xyz_min_aligned'] = xyz_min_aligned
        self.h5f.attrs['xyz_max_aligned'] = xyz_max_aligned
        self.h5f.attrs['xyz_scope_aligned'] = xyz_scope_aligned


    def add_total_row_block_N(self):
        total_row_N = 0
        n = -1
        for n,dn in enumerate( self.h5f ):
            total_row_N += self.h5f[dn].shape[0]

        self.h5f.attrs['total_row_N']=total_row_N
        self.h5f.attrs['total_block_N']=n+1
        print('add_total_row_block_N:  file: %s \n   total_row_N = %d,  total_block_N = %d'%(
            os.path.basename(self.file_name),total_row_N,n+1))
        return total_row_N, n+1

    def copy_root_summaryinfo_from_another(self,h5f0,copy_flag):
        attrs = ['datasource_name','xyz_max','xyz_min','element_names','stride_to_align']   # 'new_stride'
        if copy_flag == 'sub' or copy_flag == 'sample':
            attrs += ['block_step','block_stride','block_dims_N','total_block_N']

        for attr in attrs:
            if attr in h5f0.attrs:
                self.h5f.attrs[attr] = h5f0.attrs[attr]
        self.update_align_scope_by_stridetoalign()
        self.update_data_index_by_elementnames()

    def copy_root_attrs_from_raw(self,h5f_raw):
        attrs=['datasource_name','element_names','xyz_max','xyz_min']
        for attr in attrs:
            if attr in h5f_raw.attrs:
                self.h5f.attrs[attr] = h5f_raw.attrs[attr]
        self.update_data_index_by_elementnames()

    def normalize_dset(self,block_k_str,xyz_1norm_method='global'):
        '''
        (1) xyz/max
        (2) xy-min-block_size/2  (only xy)
        (3) color / 255
        '''
        raw_dset_k = self.h5f[block_k_str]

        norm_data_dic = {}
        raw_xyz = raw_dset_k[:,self.data_idxs['xyz']]
        #  xyz_1norm
        if xyz_1norm_method == 'global': # used by QI
            # 1norm within the whole scene
            # use by QI in indoor. Since room scale is not large, this is fine.
            # For outdoor,a scene could be too large, maybe not a good choice
            IsUseAligned = False
            if IsUseAligned:
                file_scene_zero = self.h5f.attrs['xyz_min_aligned']
                file_scene_scope = self.h5f.attrs['xyz_max_aligned'] - self.h5f.attrs['xyz_min_aligned']
            else:
                file_scene_zero = self.h5f.attrs['xyz_min']
                file_scene_scope = self.h5f.attrs['xyz_max'] - self.h5f.attrs['xyz_min']
            xyz_1norm = (raw_xyz - file_scene_zero) / file_scene_scope
        elif xyz_1norm_method == 'local':
            # 1norm within the block
            block_scope = raw_dset_k.attrs['xyz_max'] - raw_dset_k.attrs['xyz_min']
            xyz_1norm = (raw_xyz-raw_dset_k.attrs['xyz_min']) / block_scope

        # xyz_midnorm
        xyz_midnorm = raw_xyz+0 # as a new variable, not a reference
        # only norm x,y. Keep z be the raw value
        #xyz_min_real = np.min(raw_xyz,axis=0)
        #xyz_midnorm[:,0:2] -= (xyz_min_real[0:2] + self.block_step[0:2]/2)  # used by QI
        block_mid = (raw_dset_k.attrs['xyz_min'] + raw_dset_k.attrs['xyz_max'] ) / 2
        xyz_midnorm[:,0:2] -= block_mid[0:2]  # I think is better
        # for z, just be positive
        xyz_midnorm[:,2] -= self.h5f.attrs['xyz_min'][2]

        norm_data_dic['xyz_midnorm'] = xyz_midnorm
        norm_data_dic['xyz_1norm'] = xyz_1norm

        # color_1norm
        if 'color' in self.data_idxs:
            color_1norm = raw_dset_k[:,self.data_idxs['color']] / 255.0
            norm_data_dic['color_1norm']=color_1norm


        # intensity_1norm
        if 'intensity' in self.data_idxs:
            # ETH senmantic3D intensity range from -2047 to 2048
            intensity = raw_dset_k[:,self.data_idxs['intensity']]
            intensity_1norm = (intensity+2047)/(2048+2047)
            norm_data_dic['intensity_1norm']=intensity_1norm

        norm_data_list = []
        for data_name in Normed_H5f.normed_ele_idx_order:
            if data_name in norm_data_dic:
                norm_data_list.append(norm_data_dic[data_name])
        data_norm = np.concatenate( norm_data_list,1 )

        label = raw_dset_k[:,self.data_idxs['label'][0]]
        return data_norm,label,raw_xyz

    def block_index_to_ixyz(self,block_k):
        i_xyz = np.zeros(3,np.int64)
        if 'block_dims_N' not in self.h5f.attrs:
            import pdb; pdb.set_trace()  # XXX BREAKPOINT
            print('e')
        block_dims_N = self.h5f.attrs['block_dims_N']
        i_xyz[2] = block_k % block_dims_N[2]
        k = int( block_k / block_dims_N[2] )
        i_xyz[1] = k % block_dims_N[1]
        k = int( k / block_dims_N[1] )
        i_xyz[0] = k % block_dims_N[0]
        return i_xyz

    def ixyz_to_block_index(self,i_xyz):
        i_xyz = i_xyz.astype(np.uint64)
        block_dims_N = self.h5f.attrs['block_dims_N']
        block_k = int( i_xyz[0]*block_dims_N[1]*block_dims_N[2] + i_xyz[1]*block_dims_N[2] + i_xyz[2] )
        return block_k
    def xyz_to_block_index(self,xyz_k):
        assert((self.h5f.attrs['block_step'] == self.h5f.attrs['block_stride']).all()),"step != stride,the out k is not unique"

        #i_xyz = ( (xyz_k - self.raw_h5f.xyz_min)/block_step ).astype(np.int64)
        i_xyz = ( (xyz_k - self.h5f.attrs['xyz_min_aligned'])/self.h5f.attrs['block_stride'] ).astype(np.int64)
        block_k = self.ixyz_to_block_index(i_xyz)
       # i_xyz_test = self.block_index_to_ixyz(block_k)
       # if (i_xyz_test != i_xyz).any():
       #     print('get i_xyz ERROR!')
        return block_k


    def get_sub_block_ks(self,block_k,new_sorted_h5f):
        '''
        For the space k in current file,
        return the corresponding block_ks in a new file with new step and stride
        block_ks is a list
        '''
        i_xyz = self.block_index_to_ixyz(block_k)
        i_xyz_new_start = i_xyz * self.h5f.attrs['block_stride'] / new_sorted_h5f.h5f.attrs['block_stride']
        i_xyz_new_start = (i_xyz_new_start).astype(np.int)
        #print( self.xyz_min_aligned )
        #print( new_sorted_h5f.xyz_min_aligned )
        i_xyz_new_list = []
        block_k_new_list = []

        # for check
        IsCheck_Scope =  False
        if IsCheck_Scope:
            min_k,max_k,_ = self.get_block_scope_from_k(block_k)

        if (self.h5f.attrs['block_step'] > new_sorted_h5f.h5f.attrs['block_step']).any():
            '''
            find all the small(out) blocks within the large input block
            The out dataset is a base dataset in which: block_step_out == block_stride_out
            '''
            assert((self.h5f.attrs['block_step'] > new_sorted_h5f.h5f.attrs['block_step'] ).all())
            assert((new_sorted_h5f.h5f.attrs['block_step'] == new_sorted_h5f.h5f.attrs['block_stride']).all())

            search = np.ceil(self.block_step / new_sorted_h5f.block_step).astype(np.int64)
            for i_x in range(0,search[0]):
                for i_y in range(0,search[1]):
                    for i_z in range(0,search[2]):
                        i_xyz_new = ( i_xyz_new_start + np.array([i_x,i_y,i_z]) ).astype(np.uint64)
                        block_k_new = new_sorted_h5f.ixyz_to_block_index(i_xyz_new)

                        #check
                        if IsCheck_Scope:
                            min_k_new,max_k_new,_ = new_sorted_h5f.get_block_scope_from_k(block_k_new)
                            min_check = (min_k_new >= min_k).all()
                            max_check = (max_k_new <= max_k).all()
                        else:
                            min_check = True
                            max_check = True
                        if not min_check & max_check:
                            print('new=small failed i_xyz=',[i_x,i_y,i_z])
                            if not min_check:
                                print('\nmin check failed in get_sub_blcok_ks')
                                print('new min = ',min_k_new,'\norg min = ',min_k)
                            if not max_check:
                                print('\nmax check failed in get_sub_blcok_ks')
                                print('new max = ',max_k_new,'\norg max = ',max_k)

                        else:
                            i_xyz_new_list.append(i_xyz_new)
                            block_k_new_list.append(block_k_new)
                            #print('both min and max check passed')

        else:
            '''
            find all the large(out) blocks contains the small input block
            check: all xyz_scope_k_new contain xyz_scope_k
            '''
            assert( (self.h5f.attrs['block_step'] <= new_sorted_h5f.h5f.attrs['block_step']).all() )
            assert( (new_sorted_h5f.h5f.attrs['block_step'] % self.h5f.attrs['block_step'] == 0).all() )
            assert( (new_sorted_h5f.h5f.attrs['block_stride'] >= self.h5f.attrs['block_step']).all() )
            assert( (new_sorted_h5f.h5f.attrs['block_stride'] % self.h5f.attrs['block_step'] == 0).all() )

            search = ( new_sorted_h5f.h5f.attrs['block_step'] / new_sorted_h5f.h5f.attrs['block_stride'] ).astype(np.float64)
            if ( search%1*new_sorted_h5f.h5f.attrs['block_stride'] >= self.h5f.attrs['block_step']).all() :
                search = np.ceil(search).astype(np.int64)
            else:
                search = np.trunc(search).astype(np.int64)
            for i_x in range( -search[0]+1,1 ):
                for i_y in range(  -search[1]+1,1  ):
                    for i_z in range(  -search[2]+1,1 ):
                        i_xyz_new = ( i_xyz_new_start + np.array([i_x,i_y,i_z]) ).astype(np.int64)
                        if ( (i_xyz_new < 0).any() or (i_xyz_new > new_sorted_h5f.h5f.attrs['block_dims_N']).any() ):
                            continue

                        block_k_new = new_sorted_h5f.ixyz_to_block_index(i_xyz_new)
                        # check
                        if IsCheck_Scope:
                            min_k_new,max_k_new,_ = new_sorted_h5f.get_block_scope_from_k(block_k_new)
                            min_check = (min_k_new <= min_k).all()
                            max_check = (max_k_new >= max_k).all()
                        else:
                            min_check = True
                            max_check = True

                        if not min_check & max_check:
                            print('new=large failed i_xyz=',[i_x,i_y,i_z])
                            if not min_check:
                                print('\nmin check failed in get_sub_blcok_ks')
                                print('new min = ',min_k_new,'\norg min = ',min_k)
                            if not max_check:
                                print('\nmax check failed in get_sub_blcok_ks')
                                print('new max = ',max_k_new,'\norg max = ',max_k)

                        else:
                            #print('both min and max check passed, i_xyz= ',[i_x,i_y,i_z])
                            i_xyz_new_list.append(i_xyz_new)
                            block_k_new_list.append(block_k_new)
        return block_k_new_list,i_xyz_new_list

    def get_blocked_dset(self,block_k,new_set_default_rows=None,column_N = 9):
        if not type(block_k) is int:
            block_k = int(block_k)

        dset_name = str(block_k)
        if dset_name in self.h5f:
            return self.h5f[dset_name]
        if new_set_default_rows==None:
            new_set_default_rows = self.h5_num_row_1M
        #dset = self.h5f_blocked.create_dataset( dset_name,shape=(new_set_default_rows,n),\
                #maxshape=(None,n),dtype=np.float32,chunks=(self.h5_num_row_1M/5,n) )
        dset = self.h5f.create_dataset( dset_name,shape=(new_set_default_rows,column_N),\
                maxshape=(None,column_N),dtype=np.float32,compression="gzip"  )
        dset.attrs['valid_num']=0
        block_min, block_max,i_xyz = self.get_block_scope_from_k(block_k)
        dset.attrs['i_xyz'] = i_xyz
        dset.attrs['xyz_min'] = block_min
        dset.attrs['xyz_max'] = block_max
        #print('block %s min = %s  max = %s '%(dset_name,block_min,block_max))
        return dset
    def rm_invalid_data(self):
        for dset_name_i in self.h5f:
            dset_i = self.h5f[dset_name_i]
            valid_n = dset_i.attrs['valid_num']
            if dset_i.shape[0] > valid_n:
                #print('resizing block %s from %d to %d'%(dset_name_i,dset_i.shape[0],valid_n))
                dset_i.resize( (valid_n,dset_i.shape[1]) )

    def get_block_scope_from_k(self,block_k):
        i_xyz = self.block_index_to_ixyz(block_k)
        block_dims_N = self.h5f.attrs['block_dims_N']
        block_k = int( i_xyz[0]*block_dims_N[1]*block_dims_N[2] + i_xyz[1]*block_dims_N[2] + i_xyz[2] )
        block_min = i_xyz * self.h5f.attrs['block_stride'] + self.h5f.attrs['xyz_min_aligned']
        block_max = block_min + self.h5f.attrs['block_step']
        return block_min,block_max,i_xyz

    def check_xyz_scope_k(self,block_k):
        '''
        (1) anno-scope == scope_from_k
        (2) xyz data is in scope
        '''
        dset = self.h5f[str(block_k)]
        min_anno = dset.attrs['xyz_min']
        max_anno = dset.attrs['xyz_max']
        min_k,max_k,i_xyz = self.get_block_scope_from_k(block_k)

        e_min = min_anno-min_k
        e_max = max_anno-max_k
        e = np.linalg.norm(e_min) + np.linalg.norm(e_max)
        if e > 1e-5:
            print('block %d scope anno error! '%(block_k),'\nscope_k=\n',[min_k,max_k],'scope_anno=\n',[min_anno,max_anno])
            return False

        xyz = dset[:,0:3]
        xyz_max = xyz.max(axis=0)
        xyz_min = xyz.min(axis=0)
        if (max_k >= xyz_max).all() and (min_k <= xyz_min).all():
            #print('scope checked OK')
            return True
        else:
            if not (min_k <= xyz_min).all():
                print('\nmin check failed')
            if not (max_k >= xyz_max).all():
                print('\nmax check failed')
            print('scope_min=\n',min_k,'\nreal_min=\n',xyz_min)
            print('scope_max=\n',max_k,'\nreal_max=\n',xyz_max)
            print('stride=\n',self.block_stride,'\nstep=\n',self.block_step)
            return False
    def check_xyz_scope(self):
        step = int(self.total_block_N/20)+1
        Flag = True
        n=0
        for i,dset_n in enumerate(self.h5f):
            block_k = int(dset_n)
            if i%step == 0:
                flag = self.check_xyz_scope_k(block_k)
                if not flag:
                    Flag = False
                    print('dset: %s xyz scope check                   failed'%(dset_n))
                else:
                    n += 1
                    pass
                    #print('dset: %s xyz scope check passed'%(dset_n))
        if Flag:
            print('\nall %d dsets  xyz scope check passed\n'%(n))
        return Flag

    def check_equal_to_raw(self,raw_h5f):
        check_flag = True
        for k,block_k in enumerate(self.h5f):
            #print('checing block %s'%(block_k))
            dset_k = self.h5f[block_k]
            step = max(int(dset_k.shape[0]/30),1)
            for i in range(0,dset_k.shape[0],step):
                sorted_d_i = dset_k[i,0:-1]
                raw_k = int(dset_k[i,-1])
                if raw_k < 0 or raw_k > 16777215: # for float32, it is not accurate again
                    continue
                #raw_d_i = np.concatenate(  [raw_xyz_set[raw_k,:],raw_color_set[raw_k,:],raw_label_set[raw_k,:],raw_intensity_set[raw_k,:]] )
                raw_d_i = raw_h5f.get_all_dsets(raw_k,raw_k+1)
                error = raw_d_i - sorted_d_i
                err = np.linalg.norm( error )
                if err != 0:
                    check_flag = False
                    print('\nsorted error:raw_k=%d  block_k=%s,i=%d'%(raw_k,block_k,i))
                    print('raw_data = \n',raw_d_i,'\nsorted_data = \n',sorted_d_i)
                    break
                else:
                    pass
                    #print('equal check passed: block_k=%s,i=%d'%(block_k,i))
#            if flag_k:
#                    print('equal check passed: block_k=%s '%(block_k))
#            else:
#                    print('equal check failed: block_k=%s '%(block_k))
        return check_flag

    def append_to_dset(self,aim_block_k,source_dset,vacant_size=0,IsSample=False,sample_num=None):
        '''
        if append frequently to one dataset, vacant_size > 0 to avoid frequent resize
        '''
        source_N = source_dset.shape[0]
        if IsSample:
            sample_choice = self.sample(source_N,sample_num)
            sample_choice = np.sort(sample_choice)
            new_row_N = sample_choice.size
        else:
            new_row_N = source_N

        aim_dset = self.get_blocked_dset(aim_block_k,vacant_size,self.total_num_channels)
        assert aim_dset.shape[-1] == source_dset.shape[-1], "The num_channels may be wrong (in append_to_dset)"
        row_step = self.h5_num_row_1M * 10
        org_row_N = aim_dset.attrs['valid_num']
        aim_dset.resize((org_row_N+new_row_N+vacant_size,aim_dset.shape[1]))
        for k in range(0,new_row_N,row_step):
            end = min(k+row_step,new_row_N)
            if IsSample == False:
                aim_dset[org_row_N+k:org_row_N+end,:] = source_dset[k:end,:]
            else:
                choice_k = sample_choice[k:end]
                dset_buf = source_dset[choice_k.min():choice_k.max()+1,:]
                aim_dset[org_row_N+k:org_row_N+end,:] = dset_buf[choice_k-choice_k.min(),:]
            aim_dset.attrs['valid_num'] = end + org_row_N

    def generate_one_block_to_object(self,block_k,out_obj_file,IsLabelColor=False):
        row_step = self.h5_num_row_1M * 10
        dset_k = self.get_blocked_dset(block_k)
        row_N = dset_k.shape[0]
        for k in range(0,row_N,row_step):
            end = min(k+row_step,row_N)
            buf_k = dset_k[k:end,:]
            #buf_k[:,0:3] -= middle
            for j in range(0,buf_k.shape[0]):
                if not IsLabelColor:
                    str_j = 'v ' + ' '.join( ['%0.3f'%(d) for d in  buf_k[j,0:3]]) + ' \t'\
                    + ' '.join( ['%d'%(d) for d in  buf_k[j,3:6]]) + '\n'
                else:
                    label = buf_k[j,self.data_idxs['label'][0]]
                  #  if label == 0:
                  #      continue
                    label_color = Normed_H5f.g_label2color_dic[self.h5f.attrs['datasource_name']][label]
                    str_j = 'v ' + ' '.join( ['%0.3f'%(d) for d in  buf_k[j,0:3]]) + ' \t'\
                    + ' '.join( ['%d'%(d) for d in  label_color ]) + '\n'

                out_obj_file.write(str_j)

    def gen_file_obj(self,IsLabelColor=False):
        if self.file_name == None:
            print('set file_name (gen_file_obj)')
            return
        base_fn = os.path.basename(self.file_name)
        base_fn = os.path.splitext(base_fn)[0]
        folder_path = os.path.dirname(self.file_name)
        obj_folder = os.path.join(folder_path,base_fn)
        if not os.path.exists(obj_folder):
            os.makedirs(obj_folder)

        aim_scope = np.array([[-30,-30,-20],[20,20,50]])
        aim_scope = None
        n = 0
        last_rate = -20
        out_info_fn = os.path.join(obj_folder,'info.txt')
        with open(out_info_fn,'w') as info_f:
            for dset_name in self.h5f:
                row_N = self.h5f[dset_name].shape[0]

                min_i = self.h5f[dset_name].attrs['xyz_min']
                max_i = self.h5f[dset_name].attrs['xyz_max']
                if aim_scope == None:
                    IsInScope = True
                else:
                    IsInScope = (min_i > aim_scope[0,:]).all() and ( max_i < aim_scope[1,:]).all()
                if not IsInScope:
                    continue
                if IsLabelColor:
                    name_meta = 'labeled_'
                else:
                    name_meta = ''
                out_fn = os.path.join(obj_folder,name_meta+dset_name+'_'+str(row_N)+'.obj')
                with open(out_fn,'w') as out_f:
                    self.generate_one_block_to_object(dset_name,out_f,IsLabelColor)
                n += row_N
                rate = 100.0 * n / self.h5f.attrs['total_row_N']
                if int(rate) % 2 == 0 and rate - last_rate > 3:
                    last_rate = rate
                    print('%0.2f%% generating file: %s'%(rate,os.path.basename(out_fn)) )

                info_str = 'dset: %s \tN= %d   \tmin=%s   \tmax=%s \n'%(dset_name,self.h5f[dset_name].shape[0], np.array_str(min_i), np.array_str(max_i)  )
                info_f.write(info_str)
                #print(info_str)
                #if rate > 30:
                    #break
    def extract_sub_area(self,sub_xyz_scope,sub_file_name):
        with h5py.File(sub_file_name,'w') as sub_h5f:
            sub_f = Sorted_H5f(sub_h5f,sub_file_name)

            sub_f.copy_root_summaryinfo_from_another(self.h5f,'sub')
            sub_f.set_step_stride(self.block_step,self.block_stride)
            for dset_name_i in self.h5f:
                xyz_min_i = self.h5f[dset_name_i].attrs['xyz_min']
                xyz_max_i = self.h5f[dset_name_i].attrs['xyz_max']
                if (xyz_min_i > sub_xyz_scope[0,:]).all() and (xyz_max_i < sub_xyz_scope[1,:]).all():
                    sub_f.get_blocked_dset(dset_name_i,0)
                    sub_f.append_to_dset(dset_name_i,self.h5f[dset_name_i])
            sub_f.add_total_row_block_N()

    def sample(self,org_N,sample_N):
        sample_method='random'
        if sample_method == 'random':
            if org_N == sample_N:
                sample_choice = np.arange(sample_N)
            elif org_N > sample_N:
                sample_choice = np.random.choice(org_N,sample_N)
                self.reduced_num += org_N - sample_N
            else:
                #sample_choice = np.arange(org_N)
                new_samp = np.random.choice(org_N,sample_N-org_N)
                sample_choice = np.concatenate( (np.arange(org_N),new_samp) )
            #str = '%d -> %d  %d%%'%(org_N,sample_N,100.0*sample_N/org_N)
            #print(str)
        return sample_choice

    def file_random_sampling(self,sample_num,gen_norm=False,gen_obj=False):
        '''
        automatically create a folder in uper directory to store sampled files
        '''
        # randomly select n points
        out_folder = os.path.dirname(self.file_name)+'_'+str(sample_num)
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)
        file_name_base = os.path.splitext(os.path.basename(self.file_name))[0]
        sampled_filename = os.path.join(out_folder,file_name_base+'.rsh5')

        print('start genrating sampled file: ',sampled_filename)
        ave_dset_num = self.h5f.attrs['total_row_N'] /  self.h5f.attrs['total_block_N']
        print('ave_org_num = ',ave_dset_num)
        print('sample_num = %d   %d%%'%(sample_num,100.0*sample_num/ave_dset_num) )
        with h5py.File(sampled_filename,'w') as sampled_h5f:
            sampled_sh5f = Sorted_H5f(sampled_h5f,sampled_filename)
            sampled_sh5f.copy_root_summaryinfo_from_another(self.h5f,'sample')
            #sampled_sh5f.set_root_attr('sample_num',sample_num)
            for i, k_str in enumerate( self.h5f ):
                dset_k = self.h5f[k_str]
                #if dset_k.shape[0] < sample_num*0.3:
                if dset_k.shape[0] < 100:
                    continue
                sampled_sh5f.append_to_dset(int(k_str),dset_k,vacant_size=0,\
                                            IsSample=True,sample_num=sample_num)
            sampled_sh5f.add_total_row_block_N()
            print('reduced_num = %d  %d%%'%(sampled_sh5f.reduced_num,100.0*sampled_sh5f.reduced_num/self.h5f.attrs['total_row_N'] ))
            reduced_block_N = self.h5f.attrs['total_block_N'] - sampled_sh5f.h5f.attrs['total_block_N']
            print('reduced block num = %d  %d%%'%(reduced_block_N,100*reduced_block_N/self.h5f.attrs['total_block_N']))

            if gen_obj:
               sampled_sh5f.gen_file_obj()
            if gen_norm:
                sampled_sh5f.file_normalization()


    def get_sample_shape(self):
            for i,k_str in  enumerate(self.h5f):
                dset = self.h5f[k_str]
                return dset.shape

    def file_normalization(self):
        '''
        automatically create a folder in uper directory to store sampled files
        '''
        xyz_1norm_method = 'global'

        out_folder = os.path.dirname(self.file_name)+'_normed'
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)
        file_name_base = os.path.splitext(os.path.basename(self.file_name))[0]
        normalized_filename = os.path.join(out_folder,file_name_base+'.nh5')

        print('start gen normalized file: ',normalized_filename)
        with h5py.File(normalized_filename,'w') as h5f:
            normed_h5f = Normed_H5f(h5f,normalized_filename,self.h5f.attrs['datasource_name'])
            normed_h5f.copy_root_attrs_from_sorted(self.h5f,self.IS_CHECK)

            for i,k_str in  enumerate(self.h5f):
                normed_data_i,normed_label_i,raw_xyz_i = self.normalize_dset(k_str,xyz_1norm_method)
                normed_h5f.append_to_dset('data',normed_data_i)
                normed_h5f.append_to_dset('label',normed_label_i)
                normed_h5f.append_to_dset('raw_xyz',raw_xyz_i)
            normed_h5f.rm_invalid_data()
            print('normalization finished: data shape: %s'%(str(normed_h5f.data_set.shape)) )

    def merge_to_new_step(self,larger_stride,larger_step,out_folder,more_actions_config=None):
        '''merge blocks of sorted raw h5f to get new larger step / stride
        '''
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)
        new_name = os.path.join(out_folder,os.path.basename(self.file_name))
        print('new file: ',new_name)
        #if os.path.exists(new_name):
        #    print('already exists, skip')
        #    return
        with  h5py.File(self.file_name,'r') as base_h5f:
            with h5py.File(new_name,'w') as new_h5f:
                new_sh5f = Sorted_H5f(new_h5f,new_name)
                new_sh5f.copy_root_summaryinfo_from_another(base_h5f,'new_stride')
                new_sh5f.set_step_stride(larger_step,larger_stride)

                read_row_N = 0
                rate_last = -10
                print('%d rows and %d blocks to merge'%(self.h5f.attrs['total_row_N'],self.h5f.attrs['total_block_N']))
                for dset_name in  base_h5f:
                    block_i_base = int(dset_name)
                    base_dset_i = base_h5f[dset_name]
                    block_k_new_ls,i_xyz_new_ls = self.get_sub_block_ks(block_i_base,new_sh5f)

                    read_row_N += base_dset_i.shape[0]
                    rate = 100.0 * read_row_N / self.h5f.attrs['total_row_N']
                    if int(rate)%10 < 1 and rate-rate_last>5:
                        rate_last = rate
                        print(str(rate),'%   ','  dset_name = ',dset_name, '  new_k= ',block_k_new_ls,'   id= ',os.getpid())
                        new_sh5f.h5f.flush()

                    for block_k_new in block_k_new_ls:
                        new_sh5f.append_to_dset(block_k_new,base_dset_i)
                    #if rate > 5:
                        #break
                if read_row_N != self.h5f.attrs['total_row_N']:
                    print('ERROR!!!  total_row_N = %d, but only read %d'%( self.h5f.attrs['total_row_N'],read_row_N))

                total_block_N = 0
                total_row_N = 0
                for total_block_N,dn in enumerate(new_sh5f.h5f):
                    total_row_N += new_sh5f.h5f[dn].shape[0]
                total_block_N += 1
                new_sh5f.h5f.attrs['total_row_N']=total_row_N
                new_sh5f.h5f.attrs['total_block_N']=total_block_N
                print('total_row_N = ',total_row_N)
                print('total_block_N = ',total_block_N)
                new_sh5f.h5f.flush()

                #new_sh5f.check_xyz_scope()

                if more_actions_config != None:
                    actions = more_actions_config['actions']
                    if 'obj_merged' in actions:
                        new_sh5f.gen_file_obj(True)
                        new_sh5f.gen_file_obj(False)
                    if 'sample_merged' in actions:
                        Is_gen_obj = 'obj_sampled_merged' in actions
                        Is_gen_norm = 'norm_sampled_merged' in actions
                        new_sh5f.file_random_sampling(more_actions_config['sample_num'],\
                                            gen_norm=Is_gen_norm,gen_obj = Is_gen_obj)


class Sort_RawH5f():
    '''
    (1) Do sort: from "Raw_H5f" to "Sorted_H5f"
    unsampled: .sh5
    sampled: .rsh5  (fix number in each block)
    block_step_xyz=[0.5,0.5,0.5]
    '''
    def __init__(self,raw_file_list,block_step_xyz,out_folder):
        self.out_folder = out_folder
        self.Do_sort_to_blocks(raw_file_list,block_step_xyz)

    def Do_sort_to_blocks(self,raw_file_list,block_step_xyz = [0.5,0.5,0.5]):
        IsMulti = False
        if not IsMulti:
            for fn in raw_file_list:
                self.sort_to_blocks(fn,block_step_xyz)
        else:
            #pool = mp.Pool( max(mp.cpu_count()/2,1) )
            print('cpu_count= ',mp.cpu_count())
            pool = mp.Pool()
            for fn in raw_file_list:
                pool.apply_async(self.sort_to_blocks(fn,block_step_xyz))
            pool.close()
            pool.join()

    def sort_to_blocks(self,file_name,block_step_xyz=[1,1,1]):
        '''
        split th ewhole scene to space sorted small blocks
        The whole scene is a group. Each block is one dataset in the group.
        The block attrs represents the field.
        '''
        print('start sorting file to blocks: %s'%file_name)
        block_step = np.array( block_step_xyz )
        print('block step = ',block_step)
        self.row_num_limit = None

        if not os.path.exists(self.out_folder):
            os.makedirs(self.out_folder)
        basefn = os.path.splitext(os.path.basename(file_name))[0]
        blocked_file_name = os.path.join(self.out_folder,basefn)+'.sh5'
        with h5py.File(blocked_file_name,'w') as h5f_blocked:
            with h5py.File(file_name,'r') as h5_f:
                self.raw_h5f = Raw_H5f(h5_f,file_name)
                self.s_h5f = Sorted_H5f(h5f_blocked,blocked_file_name)

                self.s_h5f.copy_root_attrs_from_raw( self.raw_h5f.raw_h5f )
                self.s_h5f.set_step_stride(block_step,block_step)

                #self.row_num_limit = int(self.raw_h5f.total_row_N/1000)

                row_step = GLOBAL_PARA.h5_num_row_1M*8
                sorted_buf_dic = {}
                raw_row_N = self.raw_h5f.xyz_dset.shape[0]

                for k in range(0,raw_row_N,row_step):
                    end = min(k+row_step,raw_row_N)
                    _,data_name_list = self.raw_h5f.get_total_num_channels_name_list()
                    raw_buf = np.zeros((end-k,self.s_h5f.total_num_channels))
                    for dn in data_name_list:
                        raw_buf[:,self.s_h5f.data_idxs[dn] ] = self.raw_h5f.raw_h5f[dn][k:end,:]
                    if self.s_h5f.IS_CHECK:
                        if end < 16777215: # this is the largest int float32 can acurately present
                            org_row_index = np.arange(k,end)
                        else:
                            org_row_index = -1
                        raw_buf[:,self.s_h5f.data_idxs['org_row_index'][0]] = org_row_index

                    sorted_buf_dic={}
                    self.sort_buf(raw_buf,k,sorted_buf_dic)

                    self.h5_write_buf(sorted_buf_dic)

                    if int(k/row_step) % 1 == 0:
                        print('%%%.1f  line[ %d:%d ] block_N = %d'%(100.0*end/self.raw_h5f.total_row_N, k,end,len(sorted_buf_dic)))
                         #print('line: [%d,%d] blocked   block_T=%f s, read_T=%f ms, cal_t = %f ms, write_t= %f ms'%\
                               #(k,end,time.time()-t0_k,(t1_k-t0_k)*1000,(t2_1_k-t2_0_k)*1000, (t2_2_k-t2_1_k)*1000 ))
                    if hasattr(self,'row_num_limit') and self.row_num_limit!=None and  end>=self.row_num_limit:
                    #if k /row_step >3:
                        print('break read at k= ',end)
                        break

                total_row_N,total_block_N = self.s_h5f.add_total_row_block_N()

                if total_row_N != self.raw_h5f.total_row_N:
                    print('ERROR: blocked total_row_N= %d, raw = %d'%(total_row_N,self.raw_h5f.total_row_N))
                print('total_block_N = ',total_block_N)

                if self.s_h5f.IS_CHECK:
                    check = self.s_h5f.check_equal_to_raw(self.raw_h5f) & self.s_h5f.check_xyz_scope()
                    print('overall check of equal and scope:')
                    if check:
                        print('both passed')
                    else:
                        print('somewhere check failed')
                #self.s_h5f.show_summary_info()

    def sort_buf(self,raw_buf,buf_start_k,sorted_buf_dic):
        #t0 = time.time()
        IsMulti = False
        if IsMulti:
            block_ks = self.get_block_index_multi(raw_buf)
        else:
            block_ks = np.zeros(raw_buf.shape[0],np.int64)
            for j in range(raw_buf.shape[0]):
                block_ks[j] = self.s_h5f.xyz_to_block_index(raw_buf[j,0:3])

        #t1 = time.time()
        for i in range(raw_buf.shape[0]):
            block_k = block_ks[i]
            row = raw_buf[i,:].reshape(1,-1)
            if not block_k in sorted_buf_dic:
                sorted_buf_dic[block_k]=[]
            sorted_buf_dic[block_k].append(row)
        #t2 = time.time()
        #print('t1 = %d ms, t2 = %d ms'%( (t1-t0)*1000,(t2-t1)*1000 ))

    def h5_write_buf(self,sorted_buf_dic):
        for key in sorted_buf_dic:
            sorted_buf_dic[key] = np.concatenate(sorted_buf_dic[key],axis=0)
        for block_k in sorted_buf_dic:
            self.s_h5f.append_to_dset(block_k,sorted_buf_dic[block_k],vacant_size=GLOBAL_PARA.h5_num_row_1M)

#            dset_k =  self.s_h5f.get_blocked_dset(block_k)
#            valid_n = dset_k.attrs['valid_num']
#            new_valid_n = valid_n + sorted_buf_dic[block_k].shape[0]
#            while dset_k.shape[0] < new_valid_n:
#                dset_k.resize(( dset_k.shape[0]+self.h5_num_row_1M,dset_k.shape[1]))
#            dset_k[valid_n:new_valid_n,:] = sorted_buf_dic[block_k]
#            dset_k.attrs['valid_num'] = new_valid_n
        self.s_h5f.rm_invalid_data()
        self.s_h5f.h5f.flush()

    def get_block_index_multi(self,raw_buf):
        block_ks = mp.Array('i',raw_buf.shape[0])
        num_workers = 2
        step = int(raw_buf.shape[0]/num_workers)
        pool = []
        for i in range(0,raw_buf.shape[0],step):
            end = min( (i+1)*step, raw_buf.shape[0])
            p = mp.Process(target=self.get_block_index_subbuf,args=(raw_buf[i:end,0:3],block_ks,i) )
            p.start()
            pool.append(p)
        for p in pool:
            p.join()
        return block_ks

    def get_block_index_subbuf(self,sub_buf_xyz,block_ks,i_start):
        for i in range(sub_buf_xyz.shape[0]):
            block_ks[i+i_start] = self.s_h5f.xyz_to_block_index(sub_buf_xyz[i,0:3])



class Normed_H5f():
    '''
    format: .nhf5
    (1) There are 4 datasets:
        'data' data_set store all normalized data, shape: N*(H*W)*C, like [N,4096,9]
        'label'
        'pred_logit'
        'raw_xyz'
    (2) The root attrs are inherited from Sorted_H5f, just record the raw data information. Not all attrs are meaningful here.
        Especially, 'element_names' means the raw elements in Sorted_H5f.
        The normed data elements are stored in attrs of dataset "data".
    (3) The elements to be stored are flexible, the idx order responds to self.normed_ele_idx_order
    '''
    # -----------------------------------------------------------------------------
    # CONSTANTS
    # -----------------------------------------------------------------------------
    g_label2class_dic = {}
    g_label2class_dic['ETH'] = {0: 'unlabeled points', 1: 'man-made terrain', 2: 'natural terrain',\
                     3: 'high vegetation', 4: 'low vegetation', 5: 'buildings', \
                     6: 'hard scape', 7: 'scanning artefacts', 8: 'cars'}

    g_label2class_dic['STANFORD_INDOOR3D'] = \
                    {0:'ceiling', 1:'floor', 2:'wall', 3:'beam', 4:'column', 5:'window', 6:'door', 7:'table',
                     8:'chair', 9:'sofa', 10:'bookcase', 11:'board', 12:'clutter'}

    g_label2class_dic['SCANNET'] = {0:'unannotated', 1:'wall', 2:'floor', 3:'chair', 4:'table', 5:'desk',\
                                6:'bed', 7:'bookshelf', 8:'sofa', 9:'sink', 10:'bathtub', 11:'toilet',\
                                12:'curtain', 13:'counter', 14:'door', 15:'window', 16:'shower curtain',\
                                17:'refridgerator', 18:'picture', 19:'cabinet', 20:'otherfurniture'}
    g_label2color_dic = {}
    g_label2color_dic['ETH'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],5:[10,200,100]}
    g_label2color_dic['STANFORD_INDOOR3D'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100]}
    g_label2color_dic['SCANNET'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100],
                    13: [100,200,200],14: [200,100,200],15: [100,200,100],16: [100,100,200],
                     17:[100,100,100],18:[200,200,200],19:[200,200,100],20:[200,200,100]}

    #g_easy_view_labels = [7,8,9,10,11,1]
    #g_is_labeled = True

    ## normed data channels
    normed_data_elements_candi = {}
    normed_data_elements_candi['xyz'] = ['xyz_midnorm','xyz_1norm']
    normed_data_elements_candi['color'] = ['color_1norm']
    normed_data_elements_candi['intensity'] = ['intensity_1norm']
    normed_ele_idx_order = ['xyz_midnorm','xyz_1norm','color_1norm','intensity_1norm']
    normed_data_ele_candi_len = {'xyz_midnorm':3,'xyz_1norm':3,'color_1norm':3,'intensity_1norm':1}

    def __init__(self,h5f,file_name,datasource_name=None):
        '''
        DATASET_NAME = 'ETH'
        DATASET_NAME = 'STANFORD_INDOOR3D'
        '''
        self.h5f = h5f
        self.file_name = file_name
        if datasource_name == None:
            assert 'datasource_name' in self.h5f.attrs
        else:
            self.h5f.attrs['datasource_name'] = datasource_name
        assert self.h5f.attrs['datasource_name'] in DATA_SOURCE_NAME_LIST
        self.datasource_name = datasource_name
        self.g_label2class = self.g_label2class_dic[datasource_name]
        self.g_label2color = self.g_label2color_dic[datasource_name]
        self.g_class2label = {cls:label for label,cls in self.g_label2class.iteritems()}
        self.g_class2color = {}
        for i in self.g_label2class:
            cls = self.g_label2class[i]
            self.g_class2color[cls] = self.g_label2color[i]
        self.NUM_CLASSES = len(self.g_label2class)

        self.dataset_names = ['data','label','raw_xyz','pred_logit']
        for dn in self.dataset_names:
            if dn in h5f:
                setattr(self,dn+'_set', h5f[dn])

    def copy_root_attrs_from_sorted(self,h5f_sorted,sortedh5f_IS_CHECK):
        attrs=['datasource_name','element_names','total_block_N',
               'xyz_max','xyz_min','xyz_max_aligned','xyz_min_aligned','xyz_scope_aligned',
               'block_step','block_stride','block_dims_N','total_row_N']
        for attr in attrs:
            if attr in h5f_sorted.attrs:
                self.h5f.attrs[attr] = h5f_sorted.attrs[attr]

        for i,k_str in  enumerate(h5f_sorted):
            dset = h5f_sorted[k_str]
            h5f_sorted_shape = dset.shape
            break
        sample_num,raw_channels = h5f_sorted_shape
        self.h5f.attrs['sample_num'] = sample_num
        # - label channel
        # - org_row_index when sortedh5f IS_CHECK=True
        rawdata_num_channels = raw_channels -1 - sortedh5f_IS_CHECK

        self.create_4dsets(rawdata_num_channels)

    def show_summary_info(self):
        print('\n\nsummary of file: ',self.file_name)
        show_h5f_summary_info(self.h5f)

    @staticmethod
    def show_all_colors():
        from PIL import Image
        for label,color in Normed_H5f.g_label2color.iteritems():
            if label < len(Normed_H5f.g_label2class):
                cls = Normed_H5f.g_label2class[label]
            else:
                cls = 'empty'
            data = np.zeros((512,512,3),dtype=np.uint8)
            color_ = np.array(color,dtype=np.uint8)
            data += color_
            img = Image.fromarray(data,'RGB')
            img.save('colors/'+str(label)+'_'+cls+'.png')
            img.show()

    def label2color(self,label):
        assert( label in self.g_label2color )
        return self.g_label2color[label]

    def get_data_shape(self):
        dset = self.h5f['data']
        return dset.shape

    def update_normeddata_ele_idxs(self,normed_data_elements):
        data_ele_idxs = {}
        k = 0
        for e in self.normed_ele_idx_order:
            if e in normed_data_elements:
                idx = range(k,k+self.normed_data_ele_candi_len[e])
                k += self.normed_data_ele_candi_len[e]
                data_ele_idxs[e] = idx
        self.normed_data_ele_idxs = data_ele_idxs

    def create_4dsets(self,rawdata_num_channels):
        chunks_n = 1
        total_block_N = self.h5f.attrs['total_block_N']
        sample_num = self.h5f.attrs['sample_num']

        normed_data_num_channels = rawdata_num_channels + 3 # xyz_midnorm, xyz_1norm
        data_set = self.h5f.create_dataset( 'data',shape=(total_block_N,sample_num,normed_data_num_channels),\
                maxshape=(None,sample_num,normed_data_num_channels),dtype=np.float32,compression="gzip",\
                chunks = (chunks_n,sample_num,normed_data_num_channels)  )
        label_set = self.h5f.create_dataset( 'label',shape=(total_block_N,sample_num),\
                maxshape=(None,sample_num),dtype=np.int16,compression="gzip",\
                chunks = (chunks_n,sample_num)  )

        # record the original xyz for gen obj
        raw_xyz_set = self.h5f.create_dataset( 'raw_xyz',shape=(total_block_N,sample_num,3),\
                maxshape=(None,sample_num,3),dtype=np.float32,compression="gzip",\
                chunks = (chunks_n,sample_num,3)  )
        # predicted label
        pred_logit_set = self.h5f.create_dataset( 'pred_logit',shape=(total_block_N,sample_num),\
                maxshape=(None,sample_num),dtype=np.int16,compression="gzip",\
                chunks = (chunks_n,sample_num)  )
        pred_logit_set[:] = -1

        normed_data_set_elements = []
        for e in self.h5f.attrs['element_names']:
            if e != 'label':
                normed_data_set_elements += self.normed_data_elements_candi[e]
        self.update_normeddata_ele_idxs(normed_data_set_elements)

        for ele in normed_data_set_elements:
            data_set.attrs[ele] = self.normed_data_ele_idxs[ele]
        data_set.attrs['valid_num'] = 0
        label_set.attrs['valid_num'] = 0
        raw_xyz_set.attrs['valid_num'] = 0
        pred_logit_set.attrs['valid_num'] = 0
        self.data_set = data_set
        self.label_set  =label_set
        self.raw_xyz_set = raw_xyz_set
        self.pred_logit_set = pred_logit_set

    def create_areano_dset(self,total_block_N,sample_num):
        chunks_n = 4
        area_no_set = self.h5f.create_dataset( 'area_no',shape=(total_block_N,sample_num),\
                maxshape=(None,sample_num),dtype=np.int16,compression="gzip",\
                chunks = (chunks_n,sample_num)  )
        area_no_set.attrs['valid_num'] = 0

    def append_to_dset(self,dset_name,data_i,vacant_size=0):
        dset = self.h5f[dset_name]
        valid_num = dset.attrs['valid_num']
        if data_i.ndim == len(dset.shape) -1:
            for i in range(1,len(dset.shape)):
                assert(dset.shape[i] == data_i.shape[i-1]), "in Normed_H5f.append_to_dset: data shape not match dataset"
            new_valid_num = valid_num + 1
            #print('append 2d to 3d')
        else:
            assert(dset.shape[1:] == data_i.shape[1:]), "in Normed_H5f.append_to_dset: data shape not match dataset"
            new_valid_num = valid_num + data_i.shape[0]
            print('%s  %d -> %d'%(dset_name,valid_num,new_valid_num) )

        if new_valid_num > dset.shape[0]:
            dset.resize( (new_valid_num + vacant_size,)+dset.shape[1:] )
        dset[valid_num : new_valid_num,...] = data_i
        dset.attrs['valid_num'] = new_valid_num
        self.h5f.flush()

    def set_dset_value(self,dset_name,data_i,start_idx,end_idx):
        if dset_name not in self.h5f:
            return
        dset = self.h5f[dset_name]
        if dset.shape[0] < end_idx:
            dset.resize( (end_idx,) + dset.shape[1:] )
        if data_i.shape[1] < dset.shape[1]:
            dset[start_idx:end_idx,data_i.shape[1]:] = -1
        dset[start_idx:end_idx,0:data_i.shape[1]] = data_i
        if dset.attrs['valid_num'] < end_idx:
            dset.attrs['valid_num'] = end_idx

    def merge_file(self,another_file_name):
        # merge all the data from another_file intto self
        with h5py.File(another_file_name,'r') as f:
            ano_normed_h5f = Normed_H5f(f,another_file_name)
            for dset_name in ano_normed_h5f.h5f:
                self.append_to_dset(dset_name,ano_normed_h5f.h5f[dset_name])
                # set area no
            if self.datasource_name == 'STANFORD_INDOOR3D':
                base_name = os.path.basename(another_file_name)
                tmp = base_name.split('Area_')[1].split('_')[0]
                area_no = int(tmp)

                num_blocks,num_sample = ano_normed_h5f.h5f['label'].shape
                area_data = np.ones((num_blocks,num_sample)) * area_no
                self.append_to_dset('area_no',area_data)

    def rm_invalid_data(self):
        for dset_name_i in self.h5f:
            dset_i = self.h5f[dset_name_i]
            valid_n = dset_i.attrs['valid_num']
            if dset_i.shape[0] > valid_n:
                #print('resizing block %s from %d to %d'%(dset_name_i,dset_i.shape[0],valid_n))
                dset_i.resize( (valid_n,)+dset_i.shape[1:] )


    def Get_file_accuracies(self,IsWrite=False,out_path=None):
        # get the accuracy of each file by the pred data in hdf5
        if self.pred_logit_set.shape[0] != self.label_set.shape[0]:
            return ''
        class_num = len(self.g_class2label)
        class_TP = np.zeros(shape=(class_num))
        class_FN = np.zeros(shape=(class_num))
        class_FP = np.zeros(shape=(class_num))
        total_num = self.raw_xyz_set.size

        for j in range(0,self.raw_xyz_set.shape[0]):
            xyz_block = self.raw_xyz_set[j,:]
            label_gt = self.label_set[j,:]
            label_pred = self.pred_logit_set[j,:]
            for i in range(xyz_block.shape[0]):
                # calculate accuracy
                if (label_gt[i]==label_pred[i]):
                    class_TP[label_gt[i]] += 1
                else:
                    class_FN[label_gt[i]] += 1
                    class_FP[label_pred[i]] += 1
        acc_str,ave_acc_str = Normed_H5f.cal_accuracy(class_TP,class_FN,class_FP,total_num)
        return class_TP,class_FN,class_FP,total_num,acc_str,ave_acc_str

    @staticmethod
    def cal_accuracy(TP,FN,FP,total_num):
        precision = np.nan_to_num(TP/(TP+FP))
        recall = np.nan_to_num(TP/(TP+FN))
        IOU = np.nan_to_num(TP/(TP+FN+FP))
        # weighted ave
        real_Pos = TP+FN
        normed_real_TP = real_Pos/np.sum(real_Pos)
        ave_4acc = np.zeros(shape=(4),dtype=np.float)
        ave_4acc[0] = np.sum( precision*normed_real_TP )
        ave_4acc[1] = np.sum( recall*normed_real_TP )
        ave_4acc[2] = np.sum( IOU*normed_real_TP )
        ave_4acc[3] = np.sum(TP)/total_num
        ave_4acc_name = ['ave_class_pre','ave_class_rec','ave_class_IOU','ave_point_accu']
        # gen str
        delim = '' # ','
        def getstr(array,mean=None,str_format='%0.3g'):
            if mean!=None:
                mean_str = '%9s'%(str_format%mean) + delim
            else:
                mean_str = '%9s'%('  ')
                if delim != '': mean_str = mean_str + ' '

            return mean_str + delim.join(['%9s'%(str_format%v) for v in array])
        ave_acc_str = 'point average:  %0.3f,  class ave pre/rec/IOU: %0.3f/ %0.3f/ %0.3f    N = %f M'% \
            ( ave_4acc[0],  ave_4acc[1], ave_4acc[2], ave_4acc[3],total_num/1000000.0)
        acc_str = ave_acc_str + '\n\t       average'+delim  + delim.join(['%9s'%c for c in Normed_H5f.g_class2label])+'\n'
        acc_str += 'class_pre:   '+getstr(precision,ave_4acc[0])+'\n'
        acc_str += 'class_rec:   '+getstr(recall,ave_4acc[1])+'\n'
        acc_str += 'class_IOU:   '+getstr(IOU,ave_4acc[2])+'\n'
        acc_str += 'number(K):   '+getstr(np.trunc(real_Pos/1000.0),str_format='%d')+'\n'
        return acc_str,ave_acc_str

    def gen_gt_pred_obj_examples(self,config_flag = ['None'],out_path=None):
        #all_catigories = ['ceiling','floor','wall','beam','column','window','door','table','chair','sofa','bookcase','board','clutter']
        all_catigories = [key for key in self.g_class2label]
        config_flag = ['Z','building_6_no_ceiling']
        config_flag = ['all_single','Y']
        #config_flag = ['ALL','Y']
        #config_flag = ['Y']
        def get_config(config_flag):
            if config_flag == 'all_single':
                show_categaries_ls = [[c] for c in all_catigories]
                visu_flag = all_catigories
                return  [None]*len(all_catigories), show_categaries_ls, visu_flag
            else:
                if config_flag =='ALL':
                    xyz_cut_rate=None
                    show_categaries=None
                elif config_flag =='Y':
                    xyz_cut_rate=[0,0.92,1]
                    show_categaries=None
                elif config_flag =='yZ':
                    xyz_cut_rate=[0,0.06,0.93]
                    show_categaries=None
                elif config_flag =='XZ':
                    xyz_cut_rate=[0.95,0,0.93]
                    show_categaries=None
                elif config_flag == 'wall':
                    show_categaries = ['wall']
                    xyz_cut_rate = None
                elif config_flag == 'all_single':
                    show_categaries = all_catigories
                    xyz_cut_rate = [None]*len(all_catigories)
                elif config_flag =='building_7':
                    xyz_cut_rate=None
                    show_categaries=['ceiling','floor','wall','beam','column','window','door']
                elif config_flag =='building_6_no_ceiling':
                    xyz_cut_rate=None
                    show_categaries=['floor','wall','beam','column','window','door']
                else:
                    return None, None
                return [xyz_cut_rate],[show_categaries],[config_flag]

        for flag in config_flag:
            xyz_cut_rate,show_categaries,visu_flags = get_config(flag)
            for i in range(len(xyz_cut_rate)):
                self.gen_gt_pred_obj(out_path,xyz_cut_rate[i],show_categaries[i],
                                     pre_fn=str(visu_flags[i]),visu_flag=str(flag))
           # self.Get_file_accuracies(IsWrite=True)

    def gen_gt_pred_obj(self,out_path=None,xyz_cut_rate=None,show_categaries=None,
                        pre_fn='',visu_flag=None):
        '''
            (1)xyz_cut_rate:
                # when rate < 0.5: cut small
                # when rate >0.5: cut big
            (2) show_categaries:  ['ceiling']
                the categaries to show, if None  show all
        '''
        if show_categaries != None:
            show_categaries = [self.g_class2label[c] for c in show_categaries]
        if self.pred_logit_set.shape[0] ==0:
            print('File: %s \n   has no pred data'%(self.file_name))
            return
        base_fn = os.path.basename(self.file_name)
        base_fn = os.path.splitext(base_fn)[0]
        folder_path = os.path.dirname(self.file_name)
        if out_path == None:
            obj_folder = os.path.join(folder_path,'obj_file',base_fn)
            if visu_flag != None:
                obj_folder = os.path.join(obj_folder,visu_flag)
        else:
            obj_folder = os.path.join(out_path,base_fn)
        print('obj_folder=',obj_folder)
        if not os.path.exists(obj_folder):
            os.makedirs(obj_folder)

        raw_obj_fn = os.path.join(obj_folder, 'raw_'+pre_fn+'.obj')
        raw_colored_obj_fn = os.path.join(obj_folder, 'raw_color_'+pre_fn+'.obj')
        gt_obj_fn = os.path.join(obj_folder, 'gt_'+pre_fn+'.obj')
        pred_obj_fn = os.path.join(obj_folder,'pred_'+pre_fn+'.obj')
        dif_FN_obj_fn = os.path.join(obj_folder, 'dif_FN_'+pre_fn+'.obj')
        dif_FP_obj_fn = os.path.join(obj_folder, 'dif_FP_'+pre_fn+'.obj')
        correct_obj_fn = os.path.join(obj_folder,'correct_'+pre_fn+'.obj')
        correct_num = 0
        pred_num = 0
        file_size = self.raw_xyz_set.shape[0] * self.raw_xyz_set.shape[1]

        if xyz_cut_rate != None:
            # when rate < 0.5: cut small
            # when rate >0.5: cut big
            xyz_max = np.array([np.max(self.raw_xyz_set[:,:,i]) for i in range(3)])
            xyz_min = np.array([np.min(self.raw_xyz_set[:,:,i]) for i in range(3)])
            xyz_scope = xyz_max - xyz_min
            xyz_thres = xyz_scope * xyz_cut_rate + xyz_min
            print('xyz_thres = ',str(xyz_thres))
        cut_num = 0

        with open(gt_obj_fn,'w') as gt_f, open(raw_obj_fn,'w') as raw_f, open(raw_colored_obj_fn,'w') as raw_colored_f:
          with open(pred_obj_fn,'w') as pred_f,open(dif_FN_obj_fn,'w') as dif_FN_f,open(dif_FP_obj_fn,'w') as dif_FP_f:
            with open(correct_obj_fn,'w') as correct_f:
                for j in range(0,self.raw_xyz_set.shape[0]):
                    xyz_block = self.raw_xyz_set[j,:]
                    label_gt = self.label_set[j,:]
                    if j < self.pred_logit_set.shape[0]:
                        IsGenPred = True
                        label_pred = self.pred_logit_set[j,:]
                    else:
                        IsGenPred = False
                    for i in range(xyz_block.shape[0]):

                        # cut parts by xyz or label
                        is_cut_this_point = False
                        if show_categaries!=None and label_gt[i] not in show_categaries and \
                                label_pred[i] not in show_categaries:
                            # cut by category
                            is_cut_this_point = True
                        elif xyz_cut_rate!=None:
                            # cut by position
                            for xyz_j in range(3):
                                if (xyz_cut_rate[xyz_j] >0.5 and xyz_block[i,xyz_j] > xyz_thres[xyz_j]) or \
                                    (xyz_cut_rate[xyz_j]<=0.5 and xyz_block[i,xyz_j] < xyz_thres[xyz_j]):
                                    is_cut_this_point =  True
                        if is_cut_this_point:
                            cut_num += 1
                            continue

                        color_gt = self.label2color( label_gt[i] )
                        str_xyz = 'v ' + ' '.join( ['%0.3f'%(d) for d in  xyz_block[i,:] ])
                        str_xyz = str_xyz + ' \t'
                        str_raw_color = ' '.join( ['%d'%(d) for d in  256*self.data_set[j,i,self.elements_idxs['color_1norm']]]) + '\n'
                        str_color_gt = ' '.join( ['%d'%(d) for d in  color_gt]) + '\n'
                        str_gt = str_xyz + str_color_gt

                        if show_categaries == None or label_gt[i] in show_categaries:
                            raw_f.write(str_xyz+'\n')
                            raw_colored_f.write(str_xyz+str_raw_color)
                            gt_f.write( str_gt )

                        if IsGenPred and label_pred[i] in self.g_label2color:
                            color_pred = self.label2color( label_pred[i] )
                            str_color_pred = ' '.join( ['%d'%(d) for d in  color_pred]) + '\n'
                            str_pred = str_xyz + str_color_pred
                            if show_categaries==None or label_pred[i] in show_categaries:
                                pred_f.write( str_pred )
                            if label_gt[i] != label_pred[i]:
                                if show_categaries == None or label_gt[i] in show_categaries:
                                    dif_FN_f.write(str_pred)
                                if show_categaries == None or label_pred[i] in show_categaries:
                                    dif_FP_f.write(str_gt)
                            else:
                                correct_f.write(str_pred)
                                correct_num += 1
                            pred_num += 1
                    if j%20 ==0: print('batch %d / %d'%(j,self.raw_xyz_set.shape[0]))


                print('gen gt obj file (%d): \n%s'%(file_size,gt_obj_fn) )
                if pred_num > 0:
                     print('gen pred obj file (%d,%f): \n%s '%(pred_num,1.0*pred_num/file_size,pred_obj_fn) )
                     print('gen correct obj file (%d,%f),: \n%s '%(correct_num,1.0*correct_num/pred_num,correct_obj_fn) )
                print('gen dif obj file: ',pred_obj_fn)
                print('cut roof ponit num = %d, xyz_cut_rate = %s'%(cut_num,str(xyz_cut_rate)) )

def Write_all_file_accuracies(normed_h5f_file_list=None,out_path=None,pre_out_fn=''):
    if normed_h5f_file_list == None:
        normed_h5f_file_list = glob.glob( GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096 +
                            '/Area_2_office_1*' )
    if out_path == None: out_path = os.path.join(GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096,
                                    'pred_accuracy')
    if not os.path.exists(out_path):
        os.makedirs(out_path)
    all_acc_fn = os.path.join(out_path,pre_out_fn+'accuracies.txt')
    all_ave_acc_fn = os.path.join(out_path,pre_out_fn+'average_accuracies.txt')
    class_TP = class_FN = class_FP = np.zeros(shape=(len(Normed_H5f.g_class2label)))
    total_num = 0
    average_class_accu_ls = []
    with open(all_acc_fn,'w') as all_acc_f,open(all_ave_acc_fn,'w') as all_ave_acc_f:
        for i,fn in enumerate(normed_h5f_file_list):
            h5f = h5py.File(fn,'r')
            norm_h5f = Normed_H5f(h5f,fn)
            class_TP_i,class_FN_i,class_FP_i,total_num_i,acc_str_i,ave_acc_str_i = norm_h5f.Get_file_accuracies(
                IsWrite=False, out_path = out_path)
            class_TP = class_TP_i + class_TP
            class_FN = class_FN_i + class_FN
            class_FP = class_FP_i + class_FP
            total_num = total_num_i +  total_num

            if acc_str_i != '':
                all_acc_f.write('File: '+os.path.basename(fn)+'\n')
                all_acc_f.write(acc_str_i+'\n')
                all_ave_acc_f.write(ave_acc_str_i+'\t: '+os.path.basename(fn)+'\n')

        acc_str,ave_acc_str = Normed_H5f.cal_accuracy(class_TP,class_FN,class_FP,total_num)
        ave_str = 'Throughout All %d files.\n'%(i+1) +  acc_str
        all_acc_f.write('\n'+ave_str)
        all_ave_acc_f.write('\n'+ave_str)
    print('accuracy file: '+all_acc_fn)
    print('average accuracy file: '+all_ave_acc_fn)
    return ave_str,out_path,class_TP,class_FN,class_FP,total_num

def Write_Area_accuracies():
    ave_str_areas = ''
    class_TP = class_FN = class_FP = np.zeros(shape=(len(Normed_H5f.g_class2label)))
    total_num = 0
    for i in range(6):
        glob_i = 'Area_%d'%(i+1)
        normed_h5f_file_list = glob.glob( os.path.join(GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096,
                                glob_i+'*') )
        ave_str,out_path,class_TP_i,class_FN_i,class_FP_i,total_num_i = Write_all_file_accuracies(normed_h5f_file_list,pre_out_fn=glob_i+'_')
        class_TP = class_TP_i + class_TP
        class_FN = class_FN_i + class_FN
        class_FP = class_FP_i + class_FP
        total_num = total_num_i + total_num

        ave_str_areas += '\nArea%d\n'%i
        ave_str_areas += ave_str
    acc_str,ave_acc_str = Normed_H5f.cal_accuracy(class_TP,class_FN,class_FP,total_num)
    all_area_str = '\nThrough %d areas.\n'%(i+1)+acc_str
    with open(os.path.join(out_path,'areas_accuracies.txt'),'w' ) as area_acc_f:
        area_acc_f.write(ave_str_areas)
        area_acc_f.write(all_area_str)



#-------------------------------------------------------------------------------
# Test above codes
#-------------------------------------------------------------------------------
class MAIN_DATA_PREP():

    def __init__(self):
        print('Init Class MAIN_DATA_PREP')

    def Do_merge_blocks(self,file_list,stride=[4,4,4],step=[8,8,8]):
        #file_list = glob.glob( os.path.join(GLOBAL_PARA.ETH_A_step_0d5_stride_0d5,   '*_step_0d5_stride_0d5.h5') )
        #file_list = glob.glob( os.path.join(GLOBAL_PARA.ETH_A_stride_1_step_1,   '*_4096.h5') )
        #file_list = glob.glob( os.path.join(GLOBAL_PARA.ETH_A_step_10_stride_10,   '*_blocked.h5_sorted_step_10_stride_10.hdf5') )
        block_step = (np.array(step)).astype(np.int)
        block_stride = (np.array(stride)).astype(np.int)
        #block_stride = (block_step*0.5).astype(np.int)
        print('step = ',block_step)
        print('stride = ',block_stride)

        IsMulti_merge = True
        if not IsMulti_merge:
            for file_name in file_list:
                merged_name = self.merge_blocks_to_new_step(file_name,block_step,block_stride)
                merged_names.append(merged_name)
        else:
            pool = []
            for file_name in file_list:
                p = mp.Process( target=self.merge_blocks_to_new_step, args=(file_name,block_step,block_stride,) )
                p.start()
                pool.append(p)
            for p in pool:
                p.join()

    def merge_blocks_to_new_step(self,base_file_name,larger_step,larger_stride):
        '''merge blocks of sorted raw h5f to get new larger step
        '''
        #new_name = base_file_name.split('_xyz_intensity_rgb')[0] + '_step_' + str(larger_step[0]) + '_stride_' + str(larger_stride[0]) + '.hdf5'
        tmp = rm_file_name_midpart(base_file_name,'_stride_1_step_1')
        new_part = '_stride_' + str(larger_stride[0])+ '_step_' + str(larger_step[0])
        if larger_step[2] != larger_step[0]:
            if larger_step[2]>0:
                new_part += '_z' + str(larger_step[2])
            else:
                new_part += '_zall'

        new_name = os.path.splitext(tmp)[0]  + new_part + '.h5'
        print('new file: ',new_name)
        print('id = ',os.getpid())
        with h5py.File(new_name,'w') as new_h5f:
                base_sh5f = Sorted_H5f(base_h5f,base_file_name)
                new_sh5f = Sorted_H5f(new_h5f,new_name)
                new_sh5f.copy_root_summaryinfo_from_another(self.h5f,'new_stride')
                new_sh5f.set_step_stride(larger_step,larger_stride)

                read_row_N = 0
                rate_last = -10
                print('%d rows and %d blocks to merge'%(base_sh5f.total_row_N,base_sh5f.total_block_N))
                for dset_name in  self.h5f:
                    block_i_base = int(dset_name)
                    base_dset_i = self.h5f[dset_name]
                    block_k_new_ls,i_xyz_new_ls = base_sh5f.get_sub_block_ks(block_i_base,new_sh5f)

                    read_row_N += base_dset_i.shape[0]
                    rate = 100.0 * read_row_N / base_sh5f.total_row_N
                    if int(rate)%10 < 1 and rate-rate_last>5:
                        rate_last = rate
                        print(str(rate),'%   ','  dset_name = ',dset_name, '  new_k= ',block_k_new_ls,'   id= ',os.getpid())
                        new_sh5f.h5f.flush()

                    for block_k_new in block_k_new_ls:
                        new_sh5f.append_to_dset(block_k_new,base_dset_i)
                    #if rate > 5:
                        #break
                if read_row_N != base_sh5f.total_row_N:
                    print('ERROR!!!  total_row_N = %d, but only read %d'%(base_sh5f.total_row_N,read_row_N))

                total_block_N = 0
                total_row_N = 0
                for total_block_N,dn in enumerate(new_sh5f.h5f):
                    total_row_N += new_sh5f.h5f[dn].shape[0]
                total_block_N += 1
                new_sh5f.h5f.attrs['total_row_N']=total_row_N
                new_sh5f.h5f.attrs['total_block_N']=total_block_N
                print('total_row_N = ',total_row_N)
                print('total_block_N = ',total_block_N)
                new_sh5f.h5f.flush()

                #new_sh5f.check_xyz_scope()

                if 'sample_merged' in self.actions:
                    Is_gen_obj = 'obj_sampled_merged' in self.actions
                    Is_gen_norm = 'norm_sampled_merged' in self.actions
                    new_sh5f.file_random_sampling(self.sample_num,self.sample_method,\
                                         gen_norm=Is_gen_norm,gen_obj = Is_gen_obj)


    def gen_rawETH_to_h5(self,label_files_glob,line_num_limit=None):
        '''
        transform the data and label to h5 format
        put every dim to a single dataset
            to speed up search and compare of a single dim
        data is large, chunk to speed up slice
        '''

        label_files_list = glob.glob(label_files_glob)
        data_files_list, h5_files_list = self.clean_label_files_list(label_files_list)
        print('%d data-label files detected'%(len(label_files_list)))
        for lf in label_files_list:
            print('\t%s'%(lf))

        for i,label_fn in enumerate(label_files_list):
            data_fn = data_files_list[i]
            h5_fn = h5_files_list[i]
            with open(data_fn,'r') as data_f:
                with open(label_fn,'r') as label_f:
                    with h5py.File(h5_fn,'w') as h5_f:
                        raw_h5f = Raw_H5f(h5_f,h5_fn)
                        raw_h5f.set_num_default_row(GLOBAL_PARA.h5_num_row_1G)
                        data_label_fs = itertools.izip(data_f,label_f)
                        buf_rows = GLOBAL_PARA.h5_num_row_10M*5
                        data_buf = np.zeros((buf_rows,7),np.float32)
                        label_buf = np.zeros((buf_rows,1),np.int8)
                        for k,data_label_line in enumerate(data_label_fs):
                            k_buf = k%buf_rows
                            data_buf[k_buf,:] =np.fromstring( data_label_line[0].strip(),dtype=np.float32,sep=' ' )
                            label_buf[k_buf,:] = np.fromstring( data_label_line[1].strip(),dtype=np.float32,sep=' ' )
                            if k_buf == buf_rows-1:
                                start = int(k/buf_rows)*buf_rows
                                end = k+1
                                print('start = %d, end = %d in file: %s'%(start,end,data_fn))
                                raw_h5f.add_to_dset('xyz',data_buf[:,0:3],start,end)
                                raw_h5f.add_to_dset('intensity',data_buf[:,3:4],start,end)
                                raw_h5f.add_to_dset('color',data_buf[:,4:7],start,end)
                                raw_h5f.add_to_dset('label',label_buf[:,0:1],start,end)
                                h5_f.flush()

                            if line_num_limit != None and k+1 >= line_num_limit:
                                print('break at k= ',k)
                                break

                        self.add_to_dset_all(raw_h5f,data_buf,label_buf,k,buf_rows)
                        raw_h5f.create_done()

                        print('having read %d lines from %s \n'%(k+1,data_fn))
                        #print('h5 file line num = %d'%(xyz_dset.shape[0]))

    def add_to_dset_all(self,raw_h5f,data_buf,label_buf,k,buf_rows):
        k_buf = k%buf_rows
        start = int(k/buf_rows)*buf_rows
        end = k+1
        #print( 'start = %d, end = %d'%(start,end))
        raw_h5f.add_to_dset('xyz',data_buf[0:k_buf+1,0:3],start,end)
        raw_h5f.add_to_dset('intensity',data_buf[0:k_buf+1,3:4],start,end)
        raw_h5f.add_to_dset('color',data_buf[0:k_buf+1,4:7],start,end)
        raw_h5f.add_to_dset('label',label_buf[0:k_buf+1,0:1],start,end)
        raw_h5f.raw_h5f.flush()
        #print('flushing k = ',k)

    def clean_label_files_list(self,label_files_list):
        data_files_list = []
        h5_files_list = []
        for i,label_file_name in enumerate(label_files_list):
            no_format_name = os.path.splitext(label_file_name)[0]
            data_file_name = no_format_name + '.txt'
            h5_file_name = no_format_name + '.hdf5'
            if not os.path.exists(data_file_name):
                label_files_list.pop(i)
                print('del label_files_list[%d]:%s'%(i,label_file_name))
            else:
                data_files_list.append(data_file_name)
                h5_files_list.append(h5_file_name)
        return data_files_list, h5_files_list


    def DO_add_geometric_scope_file(self):
        files_glob = os.path.join(self.ETH_training_partBh5_folder,'*.hdf5')
        #files_glob = os.path.join(self.ETH_training_partAh5_folder,'*.hdf5')
        #files_glob = os.path.join(self.Local_training_partAh5_folder,'*.hdf5')
        files_list = glob.glob(files_glob)
        print('%d files detected'%(len(files_list)))

        IsMultiProcess = False
        line_num_limit = 1000*100
        line_num_limit = None

        if not IsMultiProcess:
            for file_name in files_list:
                with h5py.File(file_name,'a') as h5f:
                    raw_h5f = Raw_H5f(h5f,file_name)
                    raw_h5f.add_geometric_scope(line_num_limit)
                #self.add_geometric_scope_file(file_name,line_num_limit)
        else:
            mp_n = min(len(files_list),mp.cpu_count())
            pool = mp.Pool(mp_n)
            pool.imap_unordered(self.add_geometric_scope_file,files_list)


    def DO_gen_rawETH_to_h5(self,ETH_raw_labels_glob=None):
        if ETH_raw_labels_glob == None:
            labels_folder = '/home/x/Research/Dataset/ETH_Semantic3D_Dataset/training/part_A'
            labels_folder = '/short/dh01/yx2146/Dataset/ETH_Semantic3D_Dataset/training/part_B'
            #labels_folder = '/other/ETH_Semantic3D_Dataset/training/part_A
            ETH_raw_labels_glob = os.path.join(labels_folder,'*.labels')
        line_num_limit = None
        self.gen_rawETH_to_h5(ETH_raw_labels_glob)


    def main(self,file_list,actions,sample_num=4096,sample_method='random',stride=[4,4,100],step=[8,8,100]):
        # self.actions: [
        # 'merge','sample_merged','obj_sampled_merged','norm_sampled_merged' ]
        self.actions = actions
        self.sample_num = sample_num
        self.sample_method = sample_method
        self.stride = stride
        self.step = step
        if 'merge' in self.actions:
            self.Do_merge_blocks(file_list,self.stride,self.step)

#-------------------------------------------------------------------------------
#-------------------------------------------------------------------------------



def Gen_raw_label_color_obj():
    base_fn = os.path.join(GLOBAL_PARA.ETH_raw_partA,'untermaederbrunnen_station1_xyz_intensity_rgb')
    data_fn = base_fn + '.txt'
    label_fn = base_fn + '.labels'
    obj_fn = base_fn + '.obj'
    obj_labeled_fn = base_fn + '_labeled.obj'
    obj_unlabeled_fn = base_fn + '_unlabeled.obj'
    labeled_N = 0
    unlabeled_N = 0
    with open(data_fn,'r') as data_f:
     with open(label_fn,'r') as label_f:
      with open(obj_fn,'w') as obj_f:
       with open(obj_labeled_fn,'w') as obj_labeled_f:
        with open(obj_unlabeled_fn,'w') as obj_unlabeled_f:
            data_label_fs = itertools.izip(data_f,label_f)
            for k,data_label_line in enumerate(data_label_fs):
                data_k =np.fromstring( data_label_line[0].strip(),dtype=np.float32,sep=' ' )
                label_k = np.fromstring( data_label_line[1].strip(),dtype=np.int16,sep=' ' )
                color_k = Normed_H5f.g_label2color[label_k[0]]
                str_k = 'v ' + ' '.join( [str(d) for d in data_k[0:3] ] ) + ' \t' +\
                    ' '.join( [str(c) for c in color_k] ) + '\n'
                obj_f.write(str_k)
                if label_k == 0:
                    unlabeled_N += 1
                    obj_unlabeled_f.write(str_k)
                else:
                    labeled_N += 1
                    obj_labeled_f.write(str_k)
                if k%(1000*100) == 0:
                    print('gen raw obj %d'%(k))
                if k > 1000*1000*1:
                    break
    total_N = k+1
    print('total_N = %d, labeled_N = %d (%0.3f), unlabeled_N = %d (%0.3f)'%\
          (total_N,labeled_N,1.0*labeled_N/total_N,unlabeled_N,1.0*unlabeled_N/total_N))

def Do_Norm(file_list):
    for fn in file_list:
        with h5py.File(fn,'r') as f:
            sf = Sorted_H5f(f,fn)
            sf.file_normalization()

def Do_sample(file_list):
    #h5f_name = os.path.join(GLOBAL_PARA.ETH_A_stride_8_step_8,\
                      #'bildstein_station5_sub_m80_m5_stride_8_step_8.h5')
    for h5f_name in file_list:
        sample_num =  4096
        sample_method = 'random'
        with h5py.File(h5f_name,'r') as h5f:
            sh5f = Sorted_H5f(h5f,h5f_name)
            sh5f.file_random_sampling(sample_num)


def Test_sub_block_ks():
    h5f_name0 = os.path.join(GLOBAL_PARA.ETH_A_stride_1_step_1,'bildstein_station5_stride_1_step_1_sub_m80_m5.h5')
    h5f_name1 = os.path.join(GLOBAL_PARA.ETH_A_stride_1_step_1,'t_bildstein_station5_stride_1_step_1_sub_m80_m5.h5')
    with h5py.File(h5f_name0,'r') as h5f0:
      with h5py.File(h5f_name1,'w') as h5f1:
        sh5f0 = Sorted_H5f(h5f0)
        sh5f1 = Sorted_H5f(h5f1)

        block_step1 = np.array([1,1,1])*4
        block_stride1 = np.array([1,1,1])*2

        sh5f1.copy_root_summaryinfo_from_another(h5f0,'new_stride')
        sh5f1.set_step_stride(block_step1,block_stride1)

        for i,block_k0_str in enumerate( sh5f0.h5f ):
            if i > 3:
                break
            block_k0 = int(block_k0_str)
            check_flag = True
            #print('block_k0 = ',block_k0)
            block_k1s,i_xyz_1s = sh5f0.get_sub_block_ks(block_k0,sh5f1)
            print('block_k1s = ',len(block_k1s),'   ',block_k1s,'\n')
            for block_k1 in block_k1s:
                # all the space block_k1 should contain block_k0
                block_k0s,i_xyz_0s = sh5f1.get_sub_block_ks(block_k1,sh5f0)
                print('k1 = ',block_k1,'  block_k0 = ',block_k0s,'\nlen = ',len(block_k0s),'\n')
                if block_k0 not in block_k0s:
                    check_flag = False

                for block_k0_ in block_k0s:
                    # all the scope block_k0_ should constain
                    block_k1s_,i_xyz_1s_ = sh5f0.get_sub_block_ks(block_k0_,sh5f1)
                    if block_k1 not in block_k1s_:
                        check_flag = False
            if check_flag:
                print('all check passed')
            else:
                print('check failed')

def Do_Check_xyz():
    #fnl = glob.glob(os.path.join(folder,'*.hdf5'))
    #for fn in fnl:
        raw_fn = os.path.join(GLOBAL_PARA.ETH_A_rawh5,'bildstein_station5_xyz_intensity_rgb.hdf5')
        fn_s = os.path.join( GLOBAL_PARA.ETH_A_stride_1_step_1,'bildstein_station5_sub_m80_m5_stride_2_step_4.h5')
        fn_s = os.path.join( GLOBAL_PARA.ETH_A_stride_1_step_1,'bildstein_station5_sub_m80_m5_stride_4_step_8.h5')
        print('checking equal and  xyz scope of file: ',fn_s)
        with h5py.File(raw_fn,'r') as h5f:
            with h5py.File(fn_s,'r') as sh5f:
                sorted_h5f = Sorted_H5f(sh5f,raw_fn)
                #sorted_h5f.show_summary_info()
               # flag1 = sorted_h5f.check_equal_to_raw(h5f)
               # if flag1:
               #     print('equal check passed')
               # else:
               #     print('equal check failed')
                flag2 = sorted_h5f.check_xyz_scope()
                if flag2:
                    print('xyz scope check passed')
                else:
                    print('xyz scope check failed')


def Do_extract_sub_area():
    folder = GLOBAL_PARA.ETH_A_rawh5
    fnl = glob.glob(os.path.join(folder,'bildstein_station5_stride_1_step_1.h5'))
    #sub_xyz_scope = np.array([[-30,-30,-20],[0,0,50]])
    #new_flag = '_sub_m30_0'
    sub_xyz_scope = np.array([[-80,-80,-20],[-5,-5,50]])
    new_flag = '_sub_m80_m5'
    print('sub_scope:\n',sub_xyz_scope)
    for fn in fnl:
        fn_parts =  os.path.splitext(fn)
        new_name = fn_parts[0]+new_flag+fn_parts[1]
        print('sub file name: ',new_name)
        with h5py.File(fn,'r') as s_h5f:
            sorted_h5f = Sorted_H5f(s_h5f,fn)
            sorted_h5f.extract_sub_area(sub_xyz_scope,new_name)


def Add_sorted_total_row_block_N_onefile(fn):
        print('calculating row_N block_N of: ',fn)
        with h5py.File(fn,'a') as h5f:
            sorted_h5f = Sorted_H5f(h5f,fn)
            rN,bN = sorted_h5f.add_total_row_block_N()
            print('rn= ',rN, '  bN= ',bN,'\n')
def Add_sorted_total_row_block_N():
    folder = GLOBAL_PARA.ETH_A_step_20_stride_10
    fnl = glob.glob(os.path.join(folder,'*.hdf5'))
    IsMulti_aN = False
    if not IsMulti_aN:
        for fn in fnl:
            Add_sorted_total_row_block_N_onefile(fn)
    else:
        p = mp.Pool(3)
        p.map(Add_sorted_total_row_block_N_onefile,fnl)
        p.close()
        p.join()


def Do_gen_raw_obj():
    ETH_training_partAh5_folder =  GLOBAL_PARA.ETH_A_rawh5
    folder_path = ETH_training_partAh5_folder
    file_list = glob.glob( os.path.join(folder_path,'b*.hdf5') )
    IsLabelColor = True
    for fn in file_list:
        print(fn)
        if IsLabelColor:
            meta_fn = '_labeledColor'
        else:
            meta_fn = ''
        obj_fn = os.path.splitext(fn)[0]+meta_fn+'.obj'
        with h5py.File(fn,'r') as  raw_h5_f:
            raw_h5f = Raw_H5f(raw_h5_f)
            raw_h5f.generate_objfile(obj_fn,IsLabelColor)

def Do_gen_sorted_block_obj(file_list):
    for fn in file_list:
        with  h5py.File(fn,'r') as h5f:
            sorted_h5f = Sorted_H5f(h5f,fn)
            sorted_h5f.gen_file_obj(True)

def Do_gen_normed_obj(file_list):
    for fn in file_list:
        with  h5py.File(fn,'r') as h5f:
            norm_h5f = Normed_H5f(h5f,fn)
            norm_h5f.gen_gt_pred_obj()

def Do_gen_gt_pred_objs(file_list=None):
    if file_list == None:
        folder = GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096
        # many chairs and tables
        #file_list = glob.glob(os.path.join(folder,'Area_1_office_16_stride_0.5_step_1_random_4096_globalnorm.nh5'))
        # simple only one table
        #file_list = glob.glob(os.path.join(folder,'Area_6_pantry_1_stride_0.5_step_1_random_4096_globalnorm.nh5'))

        # wall good 0.88M
        fn_glob_good = 'Area_6_office_25_stride_0.5_step_1_random_4096_globalnorm.nh5'
        # all poor ave 0.5  1.5M
        fn_glob_poor = 'Area_5_storage_1_stride_0.5_step_1_random_4096_globalnorm.nh5'
        # test wall recall low
        fn_glob_test = 'Area_1_office_10_stride_0.5_step_1_random_4096_globalnorm.nh5'
        fn_glob = [fn_glob_good,fn_glob_poor,fn_glob_test]
        file_list = [ os.path.join(folder,fn)  for fn in fn_glob]

    for fn in file_list:
        with h5py.File(fn,'r') as h5f:
            norm_h5f = Normed_H5f(h5f,fn)
            norm_h5f.gen_gt_pred_obj_examples()

def gen_file_list(folder,format='h5'):
    file_list = glob.glob( os.path.join(folder,'*.'+format) )
    print(file_list)
    with open(os.path.join(folder,'all_files.txt'),'w') as f:
        for fn in file_list:
            base_filename = os.path.basename(fn)
            base_dirname = os.path.basename( os.path.dirname(fn) )
            base_dir_file_name = os.path.join(base_dirname,base_filename)
            f.write( base_dir_file_name )
            print(base_dir_file_name)
    print('all file list file write OK ')



def main(file_list):

    outdoor_prep = MAIN_DATA_PREP()
    actions = ['merge','sample_merged','obj_sampled_merged','norm_sampled_merged']
    actions = ['merge','sample_merged','norm_sampled_merged']
    outdoor_prep.main(file_list,actions,sample_num=4096,sample_method='random',\
                      stride=[8,8,-1],step=[8,8,-1])

    #outdoor_prep.Do_sort_to_blocks()
    #Do_extract_sub_area()
    #outdoor_prep.test_sub_block_ks()
    #outdoor_prep.DO_add_geometric_scope_file()
    #outdoor_prep.DO_gen_rawETH_to_h5()

if __name__ == '__main__':
 #   file_list = glob.glob( os.path.join(GLOBAL_PARA.ETH_A_stride_1_step_1, \
 #               '*_m5.h5') )
    #file_list = glob.glob( os.path.join(GLOBAL_PARA.ETH_A_stride_8_step_8, \
                #'*_4096.h5') )
   # file_list = glob.glob( os.path.join(GLOBAL_PARA.seg_train_path, \
   #             '*.h5') )
    #main(file_list)
    #Do_gen_raw_obj()
    #Add_sorted_total_row_block_N()
    #Do_Check_xyz()
    #Test_sub_block_ks()
    #Do_sample()
    #Do_gen_sorted_block_obj(file_list)
    #Do_gen_normed_obj(file_list)
    #Do_Norm(file_list)
    #gen_file_list(GLOBAL_PARA.seg_train_path)
    #Do_gen_gt_pred_objs()

    #Write_Area_accuracies()
    #Write_all_file_accuracies()

    #Normed_H5f.show_all_colors()
    #Gen_raw_label_color_obj()
    T = time.time() - START_T
    print('exit main, T = ',T)

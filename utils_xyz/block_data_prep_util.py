#xyz Sep 2017
'''
Data preparation for datsets: stanford_indoor, scannet, ETH_semantic3D
Core idea: store all the information in hdf5 file itself

# The workflow to use this tool:
Raw_H5f -> Sorted_H5f -> merge block to get new block size -> randomnly select n points
    -> Normed_H5f -> Net_Provider

## Raw_H5f store the raw data of dataset, which contains several datasets: xyz, label, color.... Each dataset
    stores the whole data for one dtype data.
    (.rh5)
## Sorted_H5f contains lots of lots of dataset. Each dataset stores all types of data within a spacial block.
    The point number of each block/dataset can be fix or not.
    (.sh5) Use class Sort_RawH5f to generate sorted file with unfixed point num in each block, and a small stride / step size.
    Then merge .sh5 file with small stride/step size to get larger size block.
    (.rsh5) Randomly sampling .sh5 file to get Sorted_H5f file with fixed point number in each block.
## Normed_H5f includes 4 datasets: data, label, raw_xyz, pred_logit
    (.sph5) This file is directly used to feed data for deep learning models.
    .sph5 file is generated by Sorted_H5f.file_normalize_to_NormedH5F()
## For all three files, show_h5f_summary_info() can use to show the info summary.
## scannet_block_sample.py is the basic usage for these classes.
'''

from __future__ import print_function
import os
import sys
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(BASE_DIR)
#from plyfile import (PlyData, PlyElement, make2d, PlyParseError, PlyProperty)
import math
import numpy as np
import h5py
import glob
import time
import multiprocessing as mp
import itertools
import ply_util
#from global_para import GLOBAL_PARA
sys.path.append(BASE_DIR+'/matterport_metadata')
from get_mpcat40 import MatterportMeta,get_cat40_from_rawcat
import csv,pickle
from configs import get_gsbb_config, NETCONFIG
import magic

'''                         Def key words list
Search with "name:" to find the definition.

    rootb_split_idxmap
    bxmh5
    flatten_bidxmap
    sg_bidxmap
    baseb_exact_flat_num
    global_step
'''
'''    Important functions
    get_blockids_of_dif_stride_step
    get_bidxmap
    get_all_bidxmaps
    gsbb naming: get_pyramid_flag
    file_saveas_pyramid_feed
'''
'''     step, stride Configuration
    (1) set_whole_scene_stride_step: limit stride, step of every cascade by whole scene scope. By calling update_align_scope_by_stridetoalign_
    (2) IsLimitStrideStepCascades_Inbxmap : Always limit step and stride larger than last cascade in bxmh5
'''

SHOW_ONLY_ERR = False
DEBUGTMP=True
ENABLECHECK = True
START_T = time.time()

g_h5_num_row_1M = 5*1000
ROOT_DIR = os.path.dirname(BASE_DIR)
UPER_DIR = os.path.dirname(ROOT_DIR)
DATA_DIR = os.path.join(ROOT_DIR,'data')

DATA_SOURCE_NAME_LIST = ['ETH','STANFORD_INDOOR3D','SCANNET','MATTERPORT','KITTI']
FLOAT_BIAS = 1e-8

def isin_sorted( a,v ):
    i = np.searchsorted(a,v)
    if i>=a.size: return False
    r = a[i] == v
    return r

def get_stride_step_name(block_stride,block_step):
    assert block_step[0] == block_step[1]
    assert block_stride[0] == block_stride[1]
    #assert (block_step[0] == block_step[2] and block_stride[0] == block_stride[2]) or (block_step[2]<0 and block_stride[2]<0)

    def get_str(v):
        assert (v*100) % 1 < 1e-8, "v=%s"%(str(v))
        if v%1!=0:
            if (v*10)%1 < 1e-8: return '%dd%d'%(v,v%1*10)
            else: return '%dd%d%d'%(v,v%1*10, v*10%1*10)
        else: return str(int(v))
    if block_stride[2] == -1:
        return 'stride-%s-step-%s'%(get_str(block_stride[0]),get_str(block_step[0]))
    else:
        return 'stride_%s_step_%s'%(get_str(block_stride[0]),get_str(block_step[0]))
def rm_file_name_midpart(fn,rm_part):
    base_name = os.path.basename(fn)
    parts = base_name.split(rm_part)
    if len(parts)>1:
        new_bn = parts[0] + parts[1]
    else:
        new_bn = parts[0]
    new_fn = os.path.join(os.path.dirname(fn),new_bn)
    return new_fn


def copy_h5f_attrs(h5f_attrs):
    attrs = {}
    for e in h5f_attrs:
        attrs[e] = h5f_attrs[e]
    return attrs
def get_mean_sg_sample_rate(sum_sg_bidxmap_sample_num):
    global_block_num = sum_sg_bidxmap_sample_num[0,4]
    subblock_num = sum_sg_bidxmap_sample_num[:,-1]
    mean_sg_bidxmap_sample_num = np.copy(sum_sg_bidxmap_sample_num)
    for i in range(sum_sg_bidxmap_sample_num.shape[0]):
        mean_sg_bidxmap_sample_num[i,0:5] /= mean_sg_bidxmap_sample_num[i,4]
        mean_sg_bidxmap_sample_num[i,5:8] /= mean_sg_bidxmap_sample_num[i,7]
    return mean_sg_bidxmap_sample_num,global_block_num,subblock_num
def get_mean_flatten_sample_rate(sum_flatten_bmap_sample_num):
    global_block_num = sum_flatten_bmap_sample_num[0,2]
    mean_flatten_bmap_sample_num = np.copy(sum_flatten_bmap_sample_num)
    for i in range(sum_flatten_bmap_sample_num.shape[0]):
        mean_flatten_bmap_sample_num[i,0:3] /= mean_flatten_bmap_sample_num[i,2]
    return mean_flatten_bmap_sample_num,global_block_num

def get_attrs_str(attrs):
    attrs_str = ''
    for a in attrs:
        elenames = ''
        if type(attrs[a])==str:
            a_str = attrs[a]
        else:
            a_val = attrs[a]
            if a == "sum_sg_bidxmap_sample_num":
                a_val,global_block_num,subblock_num = get_mean_sg_sample_rate(a_val)
                elenames = str(GlobalSubBaseBLOCK.get_sg_bidxmap_sample_num_elename()) + '\n' + 'global_block_num: %d'%(global_block_num) + '\tsubblock_num: %s'%(subblock_num)  + '\n'
            if a == "sum_flatten_bmap_sample_num":
                a_val,global_block_num = get_mean_flatten_sample_rate(a_val)
                elenames = str(GlobalSubBaseBLOCK.get_flatten_bidxmaps_sample_num_elename()) +'\n' + 'global_block_num: %d'%(global_block_num) + '\n'

            a_str = np.array2string(a_val,precision=2,separator=',',suppress_small=True)
        attrs_str += ( a+':\n'+elenames+a_str+'\n' )
    return attrs_str

def show_h5f_summary_info(h5f):
    root_attrs = [attr for attr in h5f.attrs]
    summary_str = ''
    summary_str += '--------------------------------------------------------------------------\n'
    summary_str += 'The root_attr: %s'%(root_attrs) + '\n'
    summary_str += get_attrs_str(h5f.attrs) + '\n'

    summary_str += '\n--------------------------------------------------------------------------\n'
    summary_str += 'The elements in h5f\n'
    def show_dset(dset_name,id):
        dset_str = ''
        if id>10: return dset_str
        dset = h5f[dset_name]
        dset_str += '# dataset %d: %s shape=%s\n'%(id,dset_name,dset.shape)
        if id>6: return dset_str
        dset_str += get_attrs_str(dset.attrs) + '\n'
        if len(dset.shape)==2:
            dset_str += str( dset[0:min(10,dset.shape[0]),:]) + '\n'
        if len(dset.shape)==3:
           dset_str += str( dset[0:min(2,dset.shape[0]),:] ) + '\n'
        elif len(dset.shape)==4:
            var = dset[0:min(1,dset.shape[0]),0,0:min(2,dset.shape[2]),:]
            dset_str += np.array2string(var,formatter={'float_kind':lambda var:"%0.2f"%var}) + '\n'
        dset_str += '\n'
        return dset_str

    def show_root_ele(ele_name,id):
        root_ele_str = ''
        ele = h5f[ele_name]
        if type(ele) == h5py._hl.group.Group:
            root_ele_str += 'The group: %s'%(ele_name) + '\n'
            root_ele_str += get_attrs_str(ele.attrs) + '\n'
            for dset_name in ele:
                root_ele_str += show_dset(ele_name+'/'+dset_name,id)
        else:
            root_ele_str += show_dset(ele_name,id)
        return root_ele_str

    k = -1
    for k, ele_name in enumerate(h5f):
        if ele_name == 'xyz':
            summary_str += show_dset(ele_name,k)
            continue
        summary_str += show_root_ele(ele_name,k)
    summary_str += '%d datasets totally'%(k+1)+'\n'
    print( summary_str )
    return summary_str

def get_sample_choice(org_N,sample_N,random_sampl_pro=None):
    '''
    all replace with random_choice laer
    '''
    sample_method='random'
    if sample_method == 'random':
        if org_N == sample_N:
            sample_choice = np.arange(sample_N)
        elif org_N > sample_N:
            sample_choice = np.random.choice(org_N,sample_N,replace=False,p=random_sampl_pro)
        else:
            #sample_choice = np.arange(org_N)
            new_samp = np.random.choice(org_N,sample_N-org_N)
            sample_choice = np.concatenate( (np.arange(org_N),new_samp) )
        reduced_num = org_N - sample_N
        #str = '%d -> %d  %d%%'%(org_N,sample_N,100.0*sample_N/org_N)
        #print(str)
    return sample_choice,reduced_num
def random_choice(org_vector,sample_N,random_sampl_pro=None, keeporder=True, only_tile_last_one=False):
    assert org_vector.ndim == 1
    org_N = org_vector.size
    if org_N == sample_N:
        sampled_vector = org_vector
    elif org_N > sample_N:
        sampled_vector = np.random.choice(org_vector,sample_N,replace=False,p=random_sampl_pro)
        if keeporder:
            sampled_vector = np.sort(sampled_vector)
    else:
        if only_tile_last_one:
            new_vector = np.array( [ org_vector[-1] ]*(sample_N-org_N) ).astype(org_vector.dtype)
        else:
            new_vector = np.random.choice(org_vector,sample_N-org_N,replace=True)
        sampled_vector = np.concatenate( [org_vector,new_vector] )
    #str = '%d -> %d  %d%%'%(org_N,sample_N,100.0*sample_N/org_N)
    #print(str)
    return sampled_vector

def index_in_sorted(sorted_vector,values):
    if values.ndim==0:
        values = np.array([values])
    assert values.ndim<=1 and sorted_vector.ndim==1
    #values_valid = values[np.isin(values,sorted_vector)]
    indexs = np.searchsorted(sorted_vector,values)
    indexs_valid = []
    for j,index in enumerate(indexs):
        if index<sorted_vector.size and  sorted_vector[index] == values[j]:
            indexs_valid.append( index )
    indexs_valid = np.array(indexs_valid)
    assert indexs_valid.size <= values.size
    #assert indexs.size==0 or np.max(indexs) < sorted_vector.size, 'err in index_in_sorted'
    return indexs_valid

def check_h5fs_intact(file_name):
    if not os.path.exists(file_name):
        return False,"file not exist: %s"%(file_name)
    f_format = os.path.splitext(file_name)[-1]
    if f_format == '.rh5':
        return Raw_H5f.check_rh5_intact(file_name)
    elif f_format == '.sh5' or f_format == '.rsh5':
        return Sorted_H5f.check_sh5_intact(file_name)
    elif f_format == '.sph5' or f_format == '.prh5':
        return Normed_H5f.check_sph5_intact(file_name)
    elif f_format == '.bmh5':
        return GlobalSubBaseBLOCK.check_bmh5_intact(file_name)
    else:
        return False, "file format not recognized %s"%(f_format)

def float_exact_division( A, B ):
    C = A / B
    r = np.isclose( C, np.rint(C) )
    R = r.all()
    return R

def my_fix(orgvar):
    # why do not use np.fix() directly: np.fix(2.999999) = 2.0
    assert orgvar.ndim == 1
    rint_var = np.rint(orgvar)
    zero_gap = rint_var - orgvar
    fix_var = np.copy(orgvar).astype(np.int64)
    for i in range(orgvar.size):
        if np.isclose(zero_gap[i],0):
            fix_var[i] = rint_var[i].astype(np.int64)
        else:
            fix_var[i] = np.fix(orgvar[i]).astype(np.int64)
    return fix_var

def my_ceil(orgvar):
    # why do not use np.ceil: np.ceil(12.0000000000000001)=13
    assert orgvar.ndim == 1
    rint_var = np.rint(orgvar)
    zero_gap = rint_var - orgvar
    ceil_var = np.copy(orgvar).astype(np.int64)
    for i in range(orgvar.size):
        if np.isclose(zero_gap[i],0):
            ceil_var[i] = rint_var[i].astype(np.int64)
        else:
            try:
                ceil_var[i] = np.ceil(orgvar[i]).astype(np.int64)
            except:
                import pdb; pdb.set_trace()  # XXX BREAKPOINT
                pass
    return ceil_var


class GlobalSubBaseBLOCK():
    '''
        rootb_split_idxmap: (:,2) [:,0]:root_bid  [:,1]:point_index_end

    '''
    settings = {}
    settings['fix_bmap_method'] = 'num_order' # 'random'
    #settings['fix_bmap_method'] = 'random'
    IsCheck_gsbb = {}
    IsCheck_gsbb['Aim_b_index'] = False and ENABLECHECK
    IsCheck_gsbb['bidxmap_extract'] = False and ENABLECHECK
    IsCheck_gsbb['all_base_included'] = False and ENABLECHECK
    IsCheck_gsbb['IsCheckMissingAimb'] = False and ENABLECHECK
    IsCheck_gsbb['gen_ply_gsbb'] = False and ENABLECHECK

    global_para_names = ['max_global_num_point','global_num_point','global_stride','global_step']
    para_names = global_para_names + ['sub_block_stride_candis','sub_block_step_candis','nsubblock_candis','npoint_subblock_candis', 'gsbb_config' ,\
                'flatbxmap_max_nearest_num', 'flatbxmap_max_dis', 'padding']
    root_para_names = ['root_block_stride','root_block_step']
    meta_names = ['count','aimbnum_missed_add','baseb_exact_flat_num','after_fix_missed_baseb_num','around_aimb_dis_mean','around_aimb_dis_std','npointsubblock_missed_add','npoint_subblock_std','sr_count']

    def load_default_parameters( self ):
        #max_global_num_point,global_stride,global_step,global_num_point,sub_block_stride_candis,sub_block_step_candis,nsubblock_candis,npoint_subblock_candis, gsbb_config,\
        #flatbxmap_max_nearest_num, flatbxmap_max_dis, padding  = \
        gsbb_config_dic = get_gsbb_config()

        max_global_num_point= gsbb_config_dic['max_global_num_point']
        global_stride= gsbb_config_dic['global_stride']
        global_step= gsbb_config_dic['global_step']
        global_num_point= gsbb_config_dic['global_num_point']
        sub_block_stride_candis= gsbb_config_dic['sub_block_stride_candis']
        sub_block_step_candis= gsbb_config_dic['sub_block_step_candis']
        nsubblock_candis= gsbb_config_dic['nsubblock_candis']
        npoint_subblock_candis= gsbb_config_dic['npoint_subblock_candis']
        gsbb_config= gsbb_config_dic['gsbb_config']
        flatbxmap_max_nearest_num= gsbb_config_dic['flatbxmap_max_nearest_num']
        flatbxmap_max_dis= gsbb_config_dic['flatbxmap_max_dis']
        padding= gsbb_config_dic['padding']

        for pn in self.para_names:
            setattr( self, pn, eval(pn) )

    def load_para_from_file( self, bmh5_fn ):
        f_format = os.path.splitext(bmh5_fn)[-1]
        assert f_format == '.bmh5', f_format
        IsIntact,s = GlobalSubBaseBLOCK.check_bmh5_intact( bmh5_fn )
        assert IsIntact, s
        with h5py.File( bmh5_fn,'r' ) as h5f:
            #for ele_name in self.para_names + self.meta_names + self.root_para_names:
            for ele_name in self.para_names + self.root_para_names:
                setattr( self,ele_name, h5f.attrs[ele_name]  )

    def load_para_from_bxmh5( self, bxmh5_fn ):
        # only when called by MergeNormed_H5f() -> merged_normed_h5f.copy_root_attrs_from_normed()
        f_format = os.path.splitext(bxmh5_fn)[-1]
        assert f_format == '.bxmh5'
        IsIntact,s = Normed_H5f.check_sph5_intact( bxmh5_fn )
        assert IsIntact, s
        with h5py.File( bxmh5_fn,'r' ) as h5f:
            for ele_name in self.para_names + self.meta_names + self.root_para_names:
                if 'bidxmaps_flat' not in h5f:
                    if ele_name=='baseb_exact_flat_num' or ele_name=='after_fix_missed_baseb_num' or ele_name=='around_aimb_dis_mean' or ele_name=='around_aimb_dis_std' or ele_name=='sr_count':
                        if ele_name not in h5f.attrs:
                            continue
                setattr( self,ele_name, h5f.attrs[ele_name]  )
        self.update_parameters()

    def load_para_from_rootsh5( self ):
        IsIntact,s = Sorted_H5f.check_sh5_intact( self.root_s_h5f_fn )
        assert IsIntact, s
        for ele_name in self.root_para_names:
            setattr( self,ele_name, self.root_h5fattrs[ele_name[5:len(ele_name)]]  )
    #---------------------------------------------------------------------------
    def update_parameters( self ):
        self.cascade_num = cascade_num = len(self.sub_block_stride_candis)
        self.sum_sg_bidxmap_sample_num = np.zeros(shape=(cascade_num,2))
        self.sum_flatten_bmap_sample_num = np.zeros(shape=(cascade_num))

        sg_bidxmaps_extract_idx = np.zeros(shape=(cascade_num+1,2)).astype(np.int32)
        # [cascade_id+1,0] is end subblock indice of sg_bidxmaps[cascade_id]
        # [cascade_id+1,1] is npoint_subblock of sg_bidxmaps[cascade_id]
        for i in range(0,cascade_num):
            sg_bidxmaps_extract_idx[i+1,0] = sg_bidxmaps_extract_idx[i,0] + self.nsubblock_candis[i]
            sg_bidxmaps_extract_idx[i+1,1] = self.npoint_subblock_candis[i]
        self.sg_bidxmaps_extract_idx = sg_bidxmaps_extract_idx
        flatten_bidxmaps_extract_idx = np.zeros(shape=(cascade_num+1,2)).astype(np.int32)
        for i in range(1,cascade_num+1):
            if i==1:
                last_flatten_bmap_shape0 = self.global_num_point
            else:
                last_flatten_bmap_shape0 = self.nsubblock_candis[i-2]
            flatten_bidxmaps_extract_idx[i,0] = flatten_bidxmaps_extract_idx[i-1,0] + last_flatten_bmap_shape0
            flatten_bidxmaps_extract_idx[i,1] = 2
        self.flatten_bidxmaps_extract_idx = flatten_bidxmaps_extract_idx

        self.cascade_id_ls = cascade_id_ls = ['root']+range(cascade_num)+['global']
        base_cascade_ids = {}
        base_cascade_ids['global'] = 'root'
        for i in range(cascade_num):
            if i==0:
                base_cascade_ids[0] = 'root'
            else:
                base_cascade_ids[i] = i-1
        self.base_cascade_ids = base_cascade_ids

    def __init__(self,root_s_h5f = None, root_s_h5f_fn = None, bmh5_fn = None ):
        self.new_attrs = {}
        self.bm_output = {}
        self.steps = {}
        self.strides = {}

        if bmh5_fn != None:
            assert root_s_h5f_fn == None and root_s_h5f == None
            self.mode = 'load'
            # load gsbb configuration from bmh5 file
            self.bmh5_fn = bmh5_fn
            self.load_para_from_file(bmh5_fn)
        else:
            self.load_default_parameters()
            if root_s_h5f != None and root_s_h5f_fn != None:
                assert bmh5_fn == None
                self.mode = 'write'
                self.root_s_h5f = root_s_h5f
                self.root_h5fattrs = root_s_h5f.attrs
                self.root_s_h5f_fn = root_s_h5f_fn
                # load gsbb configuration from gsbb_config.py
                self.load_para_from_rootsh5()
                self.bmh5_fn = self.get_bmapfn()
            elif root_s_h5f_fn == None and root_s_h5f == None and bmh5_fn == None:
                self.mode = 'empty_use'

        self.update_parameters()

    def get_bmapfn(self):
        assert self.mode == 'write'
        datasource_name = self.root_s_h5f.attrs['datasource_name']
        if datasource_name == "MATTERPORT":
            region_name = os.path.splitext( os.path.basename(self.root_s_h5f_fn) )[0]
            house_dir_name = os.path.dirname(self.root_s_h5f_fn)
            house_name = os.path.basename(house_dir_name)
            rootsort_dirname = os.path.dirname(house_dir_name)

            out_folder = rootsort_dirname + '/ORG_bmh5/' + self.get_pyramid_flag( 'bmh5' )
            if not os.path.exists(out_folder):
                os.mkdir(out_folder)
            blockid_maps_fn = out_folder + '/' + house_name + '/' + region_name + '.bmh5'

        elif datasource_name == "SCANNET":
            scene_name = os.path.splitext( os.path.basename(self.root_s_h5f_fn) )[0]
            scannet_h5f_dir = os.path.dirname( os.path.dirname( os.path.dirname(self.root_s_h5f_fn) ))

            out_folder = scannet_h5f_dir + '/ORG_bmh5/' + self.get_pyramid_flag( 'bmh5' )
            if not os.path.exists(out_folder):
                os.makedirs(out_folder)
            blockid_maps_fn = out_folder + '/' + scene_name + '.bmh5'

        elif datasource_name == "KITTI":              ### benz_m
            region_name = os.path.splitext( os.path.basename(self.root_s_h5f_fn) )[0]
            house_dir_name = os.path.dirname(self.root_s_h5f_fn)
            house_name = os.path.basename(house_dir_name)
            rootsort_dirname = os.path.dirname(house_dir_name)

            out_folder = rootsort_dirname + '/ORG_bmh5/' + self.get_pyramid_flag( 'bmh5' ) + '/'
            if not os.path.exists(out_folder):
                os.makedirs(out_folder)
            blockid_maps_fn = out_folder + '/' + house_name + '/' + region_name + '.bmh5'
        else:
            assert False, datasource_name

        return blockid_maps_fn

    def write_gsbbattrs_to_bmh5_bxmh5(self, aim_attrs, file_format ):
        assert self.mode == 'write'
        for ele_name in self.para_names + self.meta_names + self.root_para_names:
            if hasattr( self, ele_name ):
                aim_attrs[ele_name] = getattr( self,ele_name )

        if file_format == 'bxmh5':
            attrs_all = []
            for cascade_id in range(self.cascade_num):
                attrs = self.get_new_attrs(cascade_id)
                attrs_all.append( attrs )

            attr_candicates = ['stride_to_align', 'block_dims_N', 'xyz_scope_aligned', 'xyz_min_aligned', 'total_block_N', 'xyz_max_aligned']
            for attr in attr_candicates:
                aim_attrs[attr] = attrs[attr]
            attr_candicates = [ 'block_stride','block_step' ]
            for attr in attr_candicates:
                attr_ls = [np.expand_dims(attrs[attr],0) for attrs in attrs_all]
                attr_cas = np.concatenate( attr_ls,0 )
                aim_attrs[attr+'_cascades'] = attr_cas

    def write_bxm_paras_in_txt_unused(self, bxmh5_fn, pl_sph5_filename=None ):
        assert os.path.splitext( bxmh5_fn )[1] == '.bxmh5'
        bmap_meta_filename = os.path.splitext( bxmh5_fn )[0]+'.txt'
        with open( bmap_meta_filename,'w' ) as bmap_meta_f:
            if pl_sph5_filename!=None:
                bmap_meta_f.write( 'Responding sph5 folder: %s\n'%( os.path.basename( os.path.dirname( pl_sph5_filename ) ) ))
                bmap_meta_f.write( 'Responding bmh5 folder: %s\n\n'%( os.path.basename( os.path.dirname( self.bmh5_fn ) ) ))

            par_str = 'global parameters:\n'
            for ele_name in self.para_names:
                par_str += '\t%s: %s\n'%(ele_name, getattr(self, ele_name) )
            bmap_meta_f.write( par_str+'\n' )

            meta_str = ''
            for cascade_id in range(self.cascade_num):
                meta_str += 'cascade_id %d bxmap meta:\n'%(cascade_id)
                count = self.count[cascade_id]
                for ele_name in self.meta_names:
                    if not hasattr(self,ele_name):
                        continue
                    meta_str += '\t%s: %s'%( ele_name, getattr(self, ele_name)[cascade_id] / count )
                    if ele_name == 'count':
                        meta_str += ' (%d)'%(count)
                    if ele_name == 'aimbnum_missed_add':
                        meta_str += ' \t<-- stride:%s  nsubblock:%s'%( self.sub_block_stride_candis[cascade_id], self.nsubblock_candis[cascade_id] )
                    if ele_name == 'baseb_exact_flat_num':
                        meta_str += ' \t<-- nsubblock:%s  step:%s'%( self.nsubblock_candis[cascade_id], self.sub_block_step_candis[cascade_id] )
                        baseb_exact_flat_num = getattr(self, ele_name)[cascade_id] / count
                        meta_str += '\n\t\tmissed_baseb_num:%d/%0.1f%%'%( baseb_exact_flat_num[0], 100.0*baseb_exact_flat_num[0]/np.sum(baseb_exact_flat_num) )
                    if ele_name == 'npointsubblock_missed_add':
                        meta_str += ' \t<-- npoint_subblock:%s'%( self.npoint_subblock_candis[cascade_id] )
                    meta_str += '\n'
                meta_str += '\n'
            bmap_meta_f.write( meta_str )

    def get_pyramid_flag(self, aim_format ):
        '''
        aim_format = 'sph5' 'bmh5' 'bxmh5'
        '''
        assert aim_format == 'sph5' or aim_format == 'bmh5' or aim_format == 'bxmh5'
        def my_str(s):
            if s<0:
                is_neg = True
                s = -s
            else: is_neg = False
            if s%1!=0:
                str_ = '%dd'%(int(s))+str(int((10*s)%10))
            else:
                str_ = str(int(s))
            if is_neg: str_ = '-'+str_
            return str_
        if aim_format != 'bmh5':
            flag_str = str(self.global_num_point) + '_'
        else:
            flag_str = ''
        flag_str += 'gs'+my_str(self.global_stride[0])+'_'+my_str(self.global_step[0])
        if aim_format == 'sph5':
            return flag_str
        if aim_format == 'bxmh5':
            flag_str += '_fmn'
            for n in self.flatbxmap_max_nearest_num:
                flag_str += str(n)
        flag_str += '-'
        for i,n in enumerate(self.nsubblock_candis):
            flag_str += str(n)
            if i<len(self.nsubblock_candis)-1:
                flag_str += '_'
            else:
                flag_str +='-'
        for i,n in enumerate(self.npoint_subblock_candis):
            flag_str += str(n)
            if i<len(self.npoint_subblock_candis)-1:
                flag_str += '_'
            else:
                flag_str +='-'
        for i,s in enumerate(self.sub_block_step_candis):
            flag_str += my_str(s)
            if i<len(self.sub_block_step_candis)-1:
                flag_str += '_'
            else:
                flag_str +='-'
        for i,s in enumerate(self.sub_block_stride_candis):
            flag_str += my_str(s)
            if i<len(self.sub_block_stride_candis)-1:
                flag_str += '_'
        flag_str += '-pd'+str(int(self.padding*10))
        if NETCONFIG['merge_blocks_while_fix_bmap']:
            flag_str += '-mbf'
        flag_str += '-' + self.gsbb_config
        return flag_str

    def get_block_sample_shape(self,cascade_id):
        base_casid = self.base_cascade_ids[cascade_id]
        return np.array((self.nsubblock_candis[base_casid],self.npoint_subblock_candis[base_casid],))

    def get_new_attrs(self,cascade_id) :
        assert self.mode == 'write'
        if cascade_id not in self.new_attrs:
            stride,step = self.get_stride_step_(cascade_id)
            new_attrs = Sorted_H5f.get_attrs_of_new_stride_step_(self.root_h5fattrs,stride,step)
            # add total_block_N, not add total_row_N yet
            total_block_N = self.get_block_n_of_new_stride_step_(cascade_id)
            new_attrs['total_block_N'] = total_block_N
            self.new_attrs[cascade_id] = new_attrs
        return self.new_attrs[cascade_id]

    def get_stride_step(self,cascade_id):
        return self.get_stride_step_(cascade_id)

    def get_stride_step_(self, cascade_id):
        '''
        IsLimitStrideStepCascades_Inbxmap: Always limit step and stride larger than last cascade in bxmh5
        '''
        IsLimitStrideStepCascades_Inbxmap = True
        if cascade_id == 'global':
            stride = self.global_stride
            step = self.global_step
        elif cascade_id == 'root':
            if self.mode == 'w':
                self.root_block_stride = stride = root_h5fattrs['block_stride']
                self.root_block_step = step = root_h5fattrs['block_step']
            else:
                stride = self.root_block_stride
                step = self.root_block_step
        else:
            assert cascade_id <= self.cascade_num-1 and cascade_id>=0, 'cascade_id=%s'%(str(cascade_id))
            stride  =  np.array([1.0,1.0,1.0])*self.sub_block_stride_candis[cascade_id]
            step =  np.array([1.0,1.0,1.0])*self.sub_block_step_candis[cascade_id]
            if IsLimitStrideStepCascades_Inbxmap:
                for i in range(3):
                    if cascade_id == 0:
                        if stride[i] < self.root_block_step[i]:
                            stride[i] = self.root_block_step[i]
                        if step[i] < self.root_block_step[i]:
                            step[i] = self.root_block_step[i]
                    else:
                        if stride[i] < self.strides[cascade_id-1][i]:
                            stride[i] = self.strides[cascade_id-1][i]
                        if step[i] < self.steps[cascade_id-1][i]:
                            step[i] = self.steps[cascade_id-1][i]
            self.steps[cascade_id] = step
            self.strides[cascade_id] = stride
        return stride,step

    def get_cascade_grp_name(self,cascade_id):
        aim_stride,aim_step = self.get_stride_step_(cascade_id)
        aim_name = get_stride_step_name(aim_stride,aim_step)
        if cascade_id=='root':
            group_name = 'root-'+aim_name
        else:
            base_stride,base_step = self.get_stride_step_(self.base_cascade_ids[cascade_id])
            base_name = get_stride_step_name(base_stride,base_step)
            group_name = 'BASE_'+base_name+'-AIM_'+aim_name
        return group_name

    def get_block_n_of_new_stride_step_(self,cascade_id):
        return  self.load_one_bidxmap_(cascade_id,out=['block_num'])['block_num']

    def get_all_sorted_aimbids(self,cascade_id):
        ele_name = 'all_sorted_aimbids_'+str(cascade_id)
        if ele_name not in self.bm_output:
            self.bm_output[ele_name] = self.load_one_bidxmap(cascade_id,['all_sorted_aimbids'])['all_sorted_aimbids']
        return self.bm_output[ele_name]

    def get_basebids_ina_aim(self,cascade_id,new_bid):
        ele_name = 'allbasebids_in_aim_dic-'+str(cascade_id)
        if ele_name in self.bm_output:
            return self.bm_output[ele_name][new_bid]
        else:
            return self.load_one_bidxmap(cascade_id,['basebids_ina_aim'],new_bid=new_bid)['basebids_ina_aim']
    def get_all_base_bids_in_aim_dic(self,cascade_id):
        ele_name = 'allbasebids_in_aim_dic-'+str(cascade_id)
        if ele_name not in self.bm_output:
            self.bm_output[ele_name]  = self.load_one_bidxmap(cascade_id,['allbasebids_in_aim_dic'])['allbasebids_in_aim_dic']
        return self.bm_output[ele_name]
    def get_all_aim_bids_in_base_dic(self,cascade_id):
        ele_name = 'allaimbids_in_base_dic-'+str(cascade_id)
        if ele_name not in self.bm_output:
            self.bm_output[ele_name]  = self.load_one_bidxmap(cascade_id,['allaimbids_in_base_dic'])['allaimbids_in_base_dic']
        return self.bm_output[ele_name]
    @staticmethod
    def weighted_sample_bids(sorted_aimbids,base_bids_indic,aim_nsubblock):
        if 1.0 * sorted_aimbids.size / aim_nsubblock > 1.2:
            aim_num_bids = np.array([ base_bids_indic[aim_bid].shape[0] for aim_bid in sorted_aimbids ])
            random_sampl_pro = 1.0*aim_num_bids / np.sum(aim_num_bids)
        else:
            random_sampl_pro = None
        all_sorted_aimbids_sampled = np.sort( random_choice(sorted_aimbids,aim_nsubblock,random_sampl_pro) )
        valid_aimb_num = min(sorted_aimbids.size,aim_nsubblock)
        return all_sorted_aimbids_sampled, valid_aimb_num

    @staticmethod
    def point_index_to_rootbid( rootb_split_idxmap, point_index, rootb_index_search_start=0 ):
        '''
         The original info in bmh5 file is: rootbid -> point_index
         Here to get the inverse
        '''
        last_point_index = 0
        for rootb_index in range(rootb_index_search_start, rootb_split_idxmap.shape[0]):
            if rootb_split_idxmap[rootb_index,0] < 0: break
            if rootb_index==0: last_point_index = 0
            else: last_point_index = rootb_split_idxmap[rootb_index-1,1]
            if  point_index >= last_point_index and point_index < rootb_split_idxmap[rootb_index,1]:
                rootbid = rootb_split_idxmap[rootb_index,0]
                point_idx_inroot = point_index - last_point_index
                return rootbid, rootb_index, point_idx_inroot
        print( "point_index= %d, rootbid not found"%(point_index) )
        import pdb; pdb.set_trace()  # XXX BREAKPOINT
        assert False
        return -1, -1, -1

    def get_bidxmap(self, cascade_id, valid_sorted_basebids, num_valid_basebids, debug_meta ):
        '''
        valid_sorted_basebids: (valid_base_b_nun) base blocks are sampled at last process, some ids are lost

        sg_bidxmap: (nsubblock, npoint_subblock+3)
            sg_bidxmap[ aim_bid_index,: ] = [base_bid_index_0,1,2 ....31 ] = [block_center_x,y,z]
            base_bid_index is the index of base_bid stored in valid_sorted_basebids
            aim_bid_index is the index of aim_bid stored in sorted_aimbids_fixed
            There are two options to get the grouping position: 1) mean of points grouped, 2) use the block center ( stored at the end of sg_bidxmap dim0)

        sg_bidxmaps: (sum of nsubblocks of all cascades, max of npoint_subblocks of all cascades)

        flatten_bidxmap: (npoint_subblock,self.flatbxmap_max_nearest_num,3)
                        N: base_bid_index
                    [:,:,0]: aim_b_index
                    [:,:,1]: point_index_in_aimb
                    [:,:,2]: index_distance

        flatten_bidxmaps: (sum of npoint_subblocks of all cascades, self.flatbxmap_max_nearest_num,3)

        baseb_exact_flat_num: (4,)  the histogram of the aim block num containing current base block
                              [0]: There is no aim block containing this base block because of fixing aim block groupings. The only way is by searching from around aim blocks.
        '''
        IsRecordTime = False
        cur_flatbxmap_max_nearest_num = self.flatbxmap_max_nearest_num[cascade_id]
        IsGenFlatbxmap = cur_flatbxmap_max_nearest_num > 0
        if IsRecordTime: t0 = time.time()
        if cascade_id==0:
            rootb_split_idxmap = valid_sorted_basebids
            del valid_sorted_basebids
            # remove invalid values
            valid_rootb_n = np.searchsorted( rootb_split_idxmap[:,0]==-1,1 )
            rootb_split_idxmap = rootb_split_idxmap[0:valid_rootb_n,:]
            valid_sorted_rootbids = rootb_split_idxmap[:,0]
            valid_sorted_pointids = np.arange( rootb_split_idxmap[-1,1] ) # root bids are the point indexs
            num_valid_basebids = org_baseb_num = rootb_split_idxmap[-1,1]
            assert rootb_split_idxmap.ndim == 2
        else:
            org_baseb_num = valid_sorted_basebids.size
            assert valid_sorted_basebids.ndim == 1
        # valid_sorted_basebids.size is the valid number of base blocks. Maximum value is nsubblock of last cascade.
        # Maybe less than this because of insufficient number in last one. Use valid number intead of sample number here.
        #valid_sorted_basebids = np.sort(valid_sorted_basebids)

        aim_nsubblock =  self.nsubblock_candis[cascade_id]
        aim_npoint_subblock = self.npoint_subblock_candis[cascade_id]
        #-----------------------------------------------------------------------
        # (1) Remove all the aim blocks contain no valid base blocks
        if IsRecordTime: t1 = time.time()
        all_sorted_aimbids = self.get_all_sorted_aimbids(cascade_id)
        all_base_bids_indic = self.get_all_base_bids_in_aim_dic(cascade_id)
        #all_aim_bids_in_base_dic = self.get_all_aim_bids_in_base_dic(cascade_id)
        bidxmap_dic={}
        raw_valid_base_bnum = []
        for aim_bid in all_sorted_aimbids:
            base_bids = all_base_bids_indic[aim_bid]
            # use valid_sorted_basebids, instead of
            # valid_sorted_basebids_sample. Thus, when some base bids are
            # replicated, base_bid_valid_indexs are always assigned with smaller
            # locations. (In random_choice(), replicated eles are placed at end)
            if cascade_id == 0:
                rootbid_valid_indexs = index_in_sorted( rootb_split_idxmap[:,0], base_bids )
                point_indexs = np.array([]).astype(np.int32)
                for rootb_index in rootbid_valid_indexs:
                    if rootb_index == 0: start = 0
                    else: start = rootb_split_idxmap[rootb_index-1,1]
                    point_idx = np.arange( start, rootb_split_idxmap[rootb_index,1] )
                    point_indexs = np.concatenate( [point_indexs,point_idx] )
                base_bid_valid_indexs = point_indexs
            else:
                base_bid_valid_indexs = index_in_sorted(valid_sorted_basebids,base_bids)
            if len(base_bid_valid_indexs)==0:
                continue
            raw_valid_base_bnum.append(len(base_bid_valid_indexs))
            bidxmap_dic[aim_bid] = base_bid_valid_indexs
        raw_valid_base_bnum = np.array(raw_valid_base_bnum)
        valid_sorted_aimbids = np.sort( bidxmap_dic.keys() )

        if IsRecordTime: t2a = time.time()
        aim_attrs = self.get_new_attrs(cascade_id)
        num_valid_aimbids = valid_sorted_aimbids.size
        if aim_nsubblock < valid_sorted_aimbids.size:   # Too more blocks: only select most useful blocks, or merge points in abandoned blocks to others
            #baseb_num_inaim_ls0 = [ bidxmap_dic[aimbid].size for aimbid in valid_sorted_aimbids ]
            sorted_aimbids_fixed, bidxmap_dic_fixed  = GlobalSubBaseBLOCK.fix_bmap( cascade_id, valid_sorted_aimbids, bidxmap_dic, aim_nsubblock,
                                                                                   aim_npoint_subblock, aim_attrs, debug_meta, self.IsCheck_gsbb['IsCheckMissingAimb'] )
            valid_aimb_num = aim_nsubblock
            aimbids_tile = np.array([])
            baseb_num_inaim_ls = [ bidxmap_dic[aimbid].size for aimbid in sorted_aimbids_fixed ]
        else:                                           # Too less blocks: replicate blocks with large points, randomly selection inside the block can help to avoid point missing
            valid_aimb_num = valid_sorted_aimbids.size
            #sorted_aimbids_fixed = random_choice(valid_sorted_aimbids, aim_nsubblock, only_tile_last_one=True)

            # tile the blocks with largest points to the end
            num_tile = aim_nsubblock - valid_sorted_aimbids.size
            baseb_num_inaim_ls = [ bidxmap_dic[aimbid].size for aimbid in valid_sorted_aimbids ]
            indices = np.flip( np.argsort( baseb_num_inaim_ls ),0 )
            indices = np.tile( indices,[ np.ceil(1.0*num_tile/valid_sorted_aimbids.size).astype(np.int) ] )
            indices = indices[0:num_tile]
            aimbids_tile = valid_sorted_aimbids[ indices ]
            sorted_aimbids_fixed = np.sort( np.concatenate( [valid_sorted_aimbids,aimbids_tile] ) )
            bidxmap_dic_fixed = bidxmap_dic

        if IsRecordTime: t2b = time.time()

        #-----------------------------------------------------------------------
        # (2) Get all the maps of valid aim blocks
        if cascade_id==0:
            base_attrs = self.root_h5fattrs
        else:
            base_attrs = self.get_new_attrs( cascade_id-1 )
        sg_bidxmap_fixed = np.ones(shape=(aim_nsubblock,aim_npoint_subblock)).astype(np.int32) * (-11)
        # record the block center of each grouped aim block
        aimb_bottom_center_xyz = np.zeros( shape=(aim_nsubblock,6) ).astype(np.float32)

        if cascade_id>0: base_nsubblock = self.nsubblock_candis[cascade_id-1]
        else: base_nsubblock = self.global_num_point
        if IsGenFlatbxmap:
            flatten_bidxmap = np.ones(shape=(base_nsubblock, np.max(self.flatbxmap_max_nearest_num),3)).astype(np.float64)*(-11)
            flatten_bidxmap_num = np.zeros( shape=(base_nsubblock) ).astype(np.int8)
        aimbids_cleaned = []
        for aim_b_index in range(aim_nsubblock):
            #--------------- (2.1) get sg_bidxmap -------------------------------
            aim_bid = sorted_aimbids_fixed[aim_b_index]
            base_bid_valid_indexs = bidxmap_dic_fixed[aim_bid]
            aim_npoint_subblock_err = aim_npoint_subblock - base_bid_valid_indexs.size
            if NETCONFIG['redundant_points_in_block']!='replicate' and  aim_npoint_subblock_err > 0:
                #assert NETCONFIG['redundant_points_in_block'] == -17
                tile_idx = np.array([NETCONFIG['redundant_points_in_block']]*aim_npoint_subblock_err)
                sg_bidxmap_fixed[aim_b_index,:] = np.concatenate( [base_bid_valid_indexs,tile_idx], 0 )
            else:
                sg_bidxmap_fixed[aim_b_index,:] = random_choice( base_bid_valid_indexs, aim_npoint_subblock )
            aimbcenter, aimbmin, aimbmax = Sorted_H5f.block_index_to_xyz_( aim_bid, aim_attrs )
            aimb_bottom_center_xyz[aim_b_index,0:3] = aimbmin
            aimb_bottom_center_xyz[aim_b_index,3:6] = aimbcenter

            if not IsGenFlatbxmap:
                continue
            if aimbids_tile.size>0 and isin_sorted( aimbids_tile, aim_bid ) and aim_bid not in aimbids_cleaned:
                aimbids_cleaned.append( aim_bid )
                num_cropped = base_bid_valid_indexs.size-aim_npoint_subblock
                if num_cropped > 0:
                    # aim_bid will be used twice, in the second time, select different basebids
                    cropped_aimbids = sg_bidxmap_fixed[aim_b_index,0:min(aim_npoint_subblock, base_bid_valid_indexs.size-aim_npoint_subblock )]
                    mask = np.in1d( base_bid_valid_indexs, cropped_aimbids, invert=True )
                    bidxmap_dic_fixed[aim_bid] = base_bid_valid_indexs[mask]

            #--------------- (2.2) get flatten_bidxmap -------------------------
            # save the valid base bidxs
            base_bnum_fixvalid = min( base_bid_valid_indexs.size, aim_npoint_subblock  )
            base_bid_valid_indexs_fixvalid = sg_bidxmap_fixed[aim_b_index, 0:base_bnum_fixvalid]

            if self.IsCheck_gsbb['Aim_b_index']:
                last_rootb_index = -1
                base_bid_valid_indexs_fixvalid = np.sort(base_bid_valid_indexs_fixvalid)
            for pointindex_within_subblock, baseb_index in enumerate( base_bid_valid_indexs_fixvalid ):
                if flatten_bidxmap_num[baseb_index] < cur_flatbxmap_max_nearest_num:
                    # exactly containing relationship, so the dis=0
                    flatten_bidxmap[baseb_index,flatten_bidxmap_num[baseb_index],:] = [aim_b_index,pointindex_within_subblock,0]
                    flatten_bidxmap_num[baseb_index] += 1

                # ------------ CHECK --------------
                if self.IsCheck_gsbb['Aim_b_index']:
                    if cascade_id==0:
                        base_bid, rootb_index, point_idx_inroot = GlobalSubBaseBLOCK.point_index_to_rootbid( rootb_split_idxmap, baseb_index, last_rootb_index )
                        last_rootb_index = rootb_index
                    else:
                        base_bid = valid_sorted_basebids[ baseb_index ]
                    aim_bids, aim_ixyzs = Sorted_H5f.get_blockids_of_dif_stride_step( base_bid, base_attrs, aim_attrs, padding=self.padding )
                    if aim_bid not in aim_bids:
                        # may because of block merge
                        aim_ixyz = Sorted_H5f.block_index_to_ixyz_( aim_bid, aim_attrs )
                        index_dis = np.min( [ np.linalg.norm(aim_ixyz - aim_ixyz_) for aim_ixyz_ in aim_ixyzs] )
                        if index_dis >= 3:
                            print('aim_bid and base_bid not match')
                            import pdb; pdb.set_trace()  # XXX BREAKPOINT
                            pass

            # save the missed base bidxs
            base_bid_indexs_missed = [ i for i in base_bid_valid_indexs if i not in base_bid_valid_indexs_fixvalid ]
            for baseb_index in base_bid_indexs_missed:
                if flatten_bidxmap_num[baseb_index] == 0:
                    # exactly containing relationship, index_dis = 0.01 to flag missing block
                    pointindex_within_subblock = 0 # actually not exist in the aim block, replace by the 0th base block
                    flatten_bidxmap[baseb_index,flatten_bidxmap_num[baseb_index],:] = [aim_b_index, pointindex_within_subblock,0.01]
                    # Do not increase flatten_bidxmap_num[baseb_index], so that next time is this base block get exactly contained, overwrite this.
                    ##flatten_bidxmap_num[baseb_index] += #1

        if  IsGenFlatbxmap:
            if self.IsCheck_gsbb['Aim_b_index']:
                print('Aim_b_index check OK')

            # When cascade_id>0, thers may be some replicated basebids. Their flatten_bidxmap can also be replicated.
            for baseb_index in range( flatten_bidxmap_num.shape[0] ):
                if cascade_id>0 and flatten_bidxmap_num[baseb_index] == 0:
                    if baseb_index>0 and valid_sorted_basebids[baseb_index] == valid_sorted_basebids[baseb_index-1] and flatten_bidxmap_num[baseb_index-1]!=0:
                        flatten_bidxmap[baseb_index] = flatten_bidxmap[baseb_index-1]
                        flatten_bidxmap_num[baseb_index] = flatten_bidxmap_num[baseb_index-1]

            baseb_exact_flat_num = np.histogram( flatten_bidxmap_num, bins=range( np.max(self.flatbxmap_max_nearest_num)+2) )[0]
            # set all the flatten_bidxmap_num==0 to 1, because these base blocks are already assigned with reponding 0th bid
            flatten_bidxmap_num[ flatten_bidxmap_num==0 ] = 1
            if IsRecordTime: t3 = time.time()
            #-----------------------------------------------------------------------
            # This step costs the most time!!!
            # (3) For the base block that are contained by flatten_bidxmap_num aim
            # blocks which flatten_bidxmap_num is < cur_flatbxmap_max_nearest_num.
            # Find from nearly aim blocks to compensate if possible.
            # The reason why flatten_bidxmap_num < cur_flatbxmap_max_nearest_num is
            # insufficient aim_nsubblock or small aim_sub_block_size.
            # If still cannot find enough from nealy space, these base blocks are
            # missed, and will be set random aim bid and large distance (zero weight)
            around_aimb_dis = []
            if cur_flatbxmap_max_nearest_num > 1:
                sr_counts = np.zeros( shape=(flatten_bidxmap.shape[0]) ).astype(np.uint8)
                allaimbids_in_base_dic = self.get_all_aim_bids_in_base_dic( cascade_id )
                if IsRecordTime: tts = []
                last_rootb_index = -1
                for baseb_index in range(flatten_bidxmap.shape[0]):
                    if flatten_bidxmap_num[baseb_index] < cur_flatbxmap_max_nearest_num:
                        if IsRecordTime: tt0 = time.time()
                        if cascade_id==0:
                            base_bid, rootb_index, point_idx_inroot = GlobalSubBaseBLOCK.point_index_to_rootbid( rootb_split_idxmap, baseb_index, last_rootb_index )
                            last_rootb_index = rootb_index
                        else:
                            base_bid = valid_sorted_basebids[baseb_index]
                        if IsRecordTime: tt1 = time.time()
                        #aim_bids, aim_ixyzs = Sorted_H5f.get_blockids_of_dif_stride_step( base_bid, base_attrs, aim_attrs, padding=self.padding )
                        if base_bid not in allaimbids_in_base_dic:
                            import pdb; pdb.set_trace()  # XXX BREAKPOINT
                            pass
                        aim_bids = allaimbids_in_base_dic[base_bid]
                        if IsRecordTime: tt2 = time.time()
                        # search around aim_ixyz to find the nearest cur_flatbxmap_max_nearest_num valid aim_bids
                        around_aimbidx_dis, sr_count = GlobalSubBaseBLOCK.get_around_bid_1base( aim_bids, aim_attrs, sorted_aimbids_fixed, cur_flatbxmap_max_nearest_num-flatten_bidxmap_num[baseb_index] )
                        sr_counts[baseb_index] += sr_count
                        for k in range( around_aimbidx_dis.shape[0] ):
                            flatten_bidxmap[baseb_index, flatten_bidxmap_num[baseb_index],:] = [ around_aimbidx_dis[k,0],0,around_aimbidx_dis[k,1] ]
                            flatten_bidxmap_num[baseb_index] += 1
                            around_aimb_dis.append( around_aimbidx_dis[k,1] )
                        if IsRecordTime: tt3 = time.time()
                        if IsRecordTime:
                                tts.append( np.expand_dims( np.array( [ (tt1-tt0)*1000, (tt2-tt1)*1000, (tt3-tt2)*1000 ] ),0) )
                        #    print('sr_count:%d  1:%f  2:%f  3:%f'%( sr_count, (tt1-tt0)*1000, (tt2-tt1)*1000, (tt3-tt2)*1000 ))
                if IsRecordTime:
                    tts = np.concatenate( tts )
                    print( 'tts: ', tts.mean(axis=0) )
            if IsRecordTime: t4 = time.time()

            #-----------------------------------------------------------------------
            #(4) If there are still any missed blocks, till random aim bids and set the dis to be very large =>>> the weight will be 0
            after_fix_missed_baseb_num = np.sum( flatten_bidxmap_num == 0 )
            for baseb_index in range(flatten_bidxmap.shape[0]):
                if flatten_bidxmap_num[baseb_index] == 0:
                    for i in range(flatten_bidxmap.shape[1]):
                        flatten_bidxmap[baseb_index,i,:] = [0,-2,1e7]
                        flatten_bidxmap_num[baseb_index] += 1
            if IsRecordTime: t5 = time.time()
            #-----------------------------------------------------------------------
            #(5) If there are still some bidxmaps not fill, till any valid one but weight be 0
            for baseb_index in range(flatten_bidxmap.shape[0]):
                if flatten_bidxmap_num[baseb_index]!=cur_flatbxmap_max_nearest_num:
                    for i in range(flatten_bidxmap_num[baseb_index],cur_flatbxmap_max_nearest_num):
                        flatten_bidxmap[baseb_index,i,:] = flatten_bidxmap[baseb_index,0,:]
                        flatten_bidxmap[baseb_index,i,2] = 1000
                        flatten_bidxmap_num[baseb_index] += 1
            flatten_bidxmap_fixed = flatten_bidxmap
            #-----------------------------------------------------------------------

            nonfull_baseb_num = np.sum( flatten_bidxmap_num != cur_flatbxmap_max_nearest_num )
            assert nonfull_baseb_num==0

            if IsRecordTime: t6 = time.time()
            if IsRecordTime:
                print('\n\ncascadeid:%d'%(cascade_id))
                print('(0) get aim_nsubblock t=%f'%(t1-t0))
                print('(1a) fix_bmap t=%f'%(t2a-t1))
                print('(1b) get valid_sorted_aimbids t=%f'%(t2b-t2a))
                print('(2) get flatten_bidxmap_num t=%f'%(t3-t2b))
                print('(3) fix flatten_bidxmap_num t=%f,  cur_flatbxmap_max_nearest_num=%d'%(t4-t3,cur_flatbxmap_max_nearest_num))
                print('(4) t=%f'%(t5-t4))
                print('(5) t=%f'%(t6-t5))
                print('(total) t=%f'%(t6-t0))

            ##-----------------------------------------------------------------------
            ##(6) tile the last one to fix flatten_bidxmap shape_0: when nsubblock is larger than valid
            #if cascade_id>0: base_nsubblock = self.nsubblock_candis[cascade_id-1]
            #else: base_nsubblock = self.global_num_point
            #if flatten_bidxmap.shape[0] > base_nsubblock:
            #    print('e')
            #    import pdb; pdb.set_trace()  # XXX BREAKPOINT
            #flatten_bidxmap_tile = np.tile( np.expand_dims(flatten_bidxmap[-1,:,:],0), (base_nsubblock-flatten_bidxmap.shape[0],1,1) )
            #flatten_bidxmap_fixed = np.concatenate( [flatten_bidxmap, flatten_bidxmap_tile],0 )
            #if flatten_bidxmap.shape[0] < base_nsubblock:
            #    import pdb; pdb.set_trace()  # XXX BREAKPOINT
            #    pass
        #-----------------------------------------------------------------------
        # check by visulization
        if self.IsCheck_gsbb['gen_ply_gsbb']:
            # rawdata ~ sg_basexyz_0 ->(gp) -> sg_aimbxyz_0 ! sg_basebxyz_1 ->(gp) sg_aimbxyz_1 ~ sg_basebxyz_2 ->(gp) sg_aimbxyz_2
            # sg_basebxyz_2 -flat> flat_xyz_2,  sg_basebxyz_1 -flat> flat_xyz_1, sg_basebxyz_0 -flat> flat_xyz_0
            sg_aimb_xyzs = np.zeros( shape=(aim_nsubblock, 3) )
            for aim_b_index in range( aim_nsubblock ):
                aim_bid = sorted_aimbids_fixed[aim_b_index]
                sg_aimb_xyzs[aim_b_index,:],_,_ = Sorted_H5f.block_index_to_xyz_( aim_bid, aim_attrs )
            ply_util.create_ply_matterport( sg_aimb_xyzs,'/tmp/sg_aimbxyz_%d.ply'%(cascade_id) )

            sg_baseb_xyzs = np.zeros( shape=(aim_nsubblock, aim_npoint_subblock, 3) )
            for aim_b_index in range( aim_nsubblock ):
                for pi in range( aim_npoint_subblock ):
                    point_id = sg_bidxmap_fixed[aim_b_index,pi]
                    if cascade_id == 0:
                        if point_id<0: continue # invalid points
                        point_index = valid_sorted_pointids[ point_id ]
                        root_bid, rootb_index, point_idx_inroot = GlobalSubBaseBLOCK.point_index_to_rootbid( rootb_split_idxmap, point_index, 0 )
                        sg_baseb_xyzs[aim_b_index,pi,:] = self.root_s_h5f[str(root_bid)][point_idx_inroot,0:3]
                    else:
                        if point_id<0: continue # invalid points
                        base_bid = valid_sorted_basebids[ point_id ]
                        sg_baseb_xyzs[aim_b_index,pi,:],_,_ = Sorted_H5f.block_index_to_xyz_( base_bid, base_attrs )
            ply_util.create_ply_matterport( sg_baseb_xyzs,'/tmp/sg_basebxyz_%d.ply'%(cascade_id) )

            if  IsGenFlatbxmap:
                flat_xyzs = np.zeros( shape=(flatten_bidxmap_fixed.shape[0],3) )
                for i in range( flat_xyzs.shape[0] ):
                    flat_idx = flatten_bidxmap_fixed[i,0,0:2].astype(np.int32)
                    flat_xyzs[ i,: ] = sg_baseb_xyzs[ flat_idx[0], flat_idx[1],: ]
                ply_util.create_ply_matterport( flat_xyzs,'/tmp/flat_xyz_%d.ply'%(cascade_id) )
            if cascade_id == self.cascade_num-1 : import pdb; pdb.set_trace()  # XXX BREAKPOINT

        bxmap_meta = {}
        bxmap_meta['count'] = np.array([1])

        aimbnum_missed_add = aim_nsubblock - valid_sorted_aimbids.size
        bxmap_meta['aimbnum_missed_add'] = np.array([[ valid_sorted_aimbids.size, min(0,aimbnum_missed_add), max(0,aimbnum_missed_add) ]])
        npointsubblock_mean = np.mean(baseb_num_inaim_ls)

        dif = aim_npoint_subblock - np.array(baseb_num_inaim_ls)
        miss = np.sum( dif * (dif<0) ) * 1.0 / aim_nsubblock
        add = np.sum( dif * (dif>0) ) * 1.0 / aim_nsubblock
        bxmap_meta['npointsubblock_missed_add'] = np.array( [[ npointsubblock_mean, miss, add ]] )
        bxmap_meta['npoint_subblock_std'] = np.array( [np.std(baseb_num_inaim_ls)] )

        if IsGenFlatbxmap:
            bxmap_meta['after_fix_missed_baseb_num'] = np.array([after_fix_missed_baseb_num])
            bxmap_meta['baseb_exact_flat_num'] = np.expand_dims( baseb_exact_flat_num,0 )

            if len(around_aimb_dis) > 0:
                bxmap_meta['around_aimb_dis_mean'] = np.array( [np.mean(around_aimb_dis)] )
                bxmap_meta['around_aimb_dis_std'] = np.array( [np.std(around_aimb_dis)] )
                bxmap_meta['sr_count'] = np.array( [sr_counts.mean()] )
            else:
                bxmap_meta['around_aimb_dis_mean'] = np.array( [0] )
                bxmap_meta['around_aimb_dis_std'] = np.array( [0] )
                bxmap_meta['sr_count'] = np.array( [0] )
        else:
            flatten_bidxmap_fixed = np.ones(shape=(base_nsubblock, 0,3)).astype(np.float64)*(-11)


        # convert to mm and int32
        aimb_bottom_center_mm = np.rint(aimb_bottom_center_xyz * 1000).astype(np.int32)
        sg_bidxmap_fixed = np.concatenate( [sg_bidxmap_fixed, aimb_bottom_center_mm],-1 )

        CheckScopeWithInGlobal = False
        if CheckScopeWithInGlobal:
            # check scope
            aimb_bottom_min = aimb_bottom_center_xyz[:,0:3].min(0)
            aimb_top = aimb_bottom_center_xyz[:,3:6] * 2 - aimb_bottom_center_xyz[:,0:3]
            aimb_top_max = aimb_top.max(0)
            gb_bottom = debug_meta['gb_bottom']
            gb_top = debug_meta['gb_center']*2 - debug_meta['gb_bottom']

            bottom_err = (aimb_bottom_min - gb_bottom) / aim_attrs['block_stride']
            top_err = (gb_top - aimb_top_max) / aim_attrs['block_stride']

            bottom_check = (aimb_bottom_min - gb_bottom > -1e-5).all()
            top_check = (gb_top - aimb_top_max > -1e-5 ).all()
            if not (bottom_check and top_check):
                print('\ncascade_id:',cascade_id)
                print(' bottom_check:%s\n top_check:%s'%(bottom_check, top_check))
                print('gb_bottom:', gb_bottom)
                print('aimb_bottom_min:', aimb_bottom_min)
                print('gb_top:', gb_top)
                print('aimb_top_max:', aimb_top_max)
                print('bottom_err:',bottom_err)
                print('top_err:',top_err)
        return sg_bidxmap_fixed, sorted_aimbids_fixed, num_valid_aimbids,  flatten_bidxmap_fixed, bxmap_meta

    @staticmethod
    def convert_dis_to_weight( flatten_bidxmap ):
        # convert dis to weight
        assert flatten_bidxmap.dtype == np.float32
        theta = 1.5
        for baseb_index in range(flatten_bidxmap.shape[0]):
            dis = self.flatbxmap_max_dis -  flatten_bidxmap[baseb_index,:,2]
            weight = np.exp(dis*theta)
            weight = weight / (weight.sum()+1e-8)
            flatten_bidxmap[baseb_index,:,2] = weight

    @staticmethod
    def get_around_bid_multibase( bids_ls, attrs, valid_bids, max_need_num, max_search_dis=1 ):
        '''
        Inputs:
            bids_ls: len=b, [i]:shape=(n,)  n is flexible
        Outputs:
            ar_bidx_dis_ls: len=b, [i]:shape=(p,2) [:,[b_index, index_distance]]
                        p=0~max_need_num
            sr_count_ls: len=b, [i]:scale, search time
        '''
        ar_bidx_dis_ls = []
        sr_count_ls = []
        for bids in bids_ls:
            ar_bidx_dis, sr_count = GlobalSubBaseBLOCK.get_around_bid_1base( bids, attrs, valid_bids, max_need_num, max_search_dis )
            ar_bidx_dis_ls.append( ar_bidx_dis )
            sr_count_ls.append( sr_count )
        return ar_bidx_dis_ls, sr_count_ls

    @staticmethod
    def get_around_bid_1base( bids, attrs, valid_bids, max_need_num, max_search_dis=1 ):
        '''
        NOTE!!!: the return distance is index distance, not meters
        Search the max_need_num neighbor blocks of the n input bids.
        max_need_num is the total required neighbor blocks of all the n input bids, because they are belong to aimbids of the same basebid.
        Inputs:
            bids: shape=(n,)
            valid_bids: shape=(m,)
            max_need_num: scalar
        Outputs:
            ar_bidx_dis: shape=(p,2) [:,[b_index, index_distance]]
                        p=0~max_need_num
            sr_count: scale, search time
        '''

        #ixyzs = []
        #for bid in bids:
        #    ixyzs.append( np.expand_dims( Sorted_H5f.block_index_to_ixyz_( bid, attrs ), 0 ) )
        #ixyzs = np.concatenate( ixyzs,0 )
        ixyzs = Sorted_H5f.block_index_to_ixyz_( np.array(bids), attrs )
        if ixyzs.ndim==1:
            ixyzs = np.expand_dims( ixyzs,0 )

        IsRecordTime = False
        if IsRecordTime: t0 = time.time()
        ar_bidx_dis = []
        ar_bids = []
        ar_ixyzs = []
        block_dims_N = attrs['block_dims_N']
        offset_ls = []
        offset_ls.append( np.array( [ [1,0,0],[0,1,0],[0,0,1] ] ) )
        offset_ls.append( -offset_ls[0] )
        offset_ls.append( np.array( [ [1,1,0],[1,-1,0],[0,1,1],[0,1,-1],[1,0,1],[1,0,-1] ] ) )
        offset_ls.append( -offset_ls[2] )
        offset_ls.append( np.array( [ [1,1,1],[-1,-1,-1] ] ) )
        sr_offsets0 = np.concatenate( offset_ls, 0 )
        sr_count = 0
        for step in range( 1,max_search_dis+1 ):
            sr_offsets = sr_offsets0 * step
            for k in range( ixyzs.shape[0] ):
                ixyz = ixyzs[k,:]
                if len(ar_bidx_dis) >= max_need_num: break
                for i in range(sr_offsets.shape[0]):
                    ixyz_sr = np.copy(ixyz)
                    ixyz_sr += sr_offsets[i,:]
                    sr_count += 1
                    if (ixyz_sr >= 0).all() and (ixyz_sr < block_dims_N).all():
                        bid = Sorted_H5f.ixyz_to_block_index_( ixyz_sr,attrs )
                        if (not np.isin(bid,bids)) and isin_sorted( valid_bids, bid ):
                            b_index = index_in_sorted( valid_bids, np.array([bid]) )
                            if (bid not in ar_bids):
                                dis = np.linalg.norm( ixyz - ixyz_sr )
                                ar_bidx_dis.append( [b_index[0], dis] )
                                ar_ixyzs.append( ixyz_sr )
                                ar_bids.append( bid )

                                #print( 'sr_count=%d, find new bid=%d, bindex=%d, dis=%s'%(sr_count, bid, b_index, dis) )
                                if len(ar_bidx_dis) >= max_need_num: break
        ar_bidx_dis = np.array( ar_bidx_dis )
        if IsRecordTime:
            t = time.time() - t0
            print('sr_count = %d   t = %f'%(sr_count,t*1000))
            if sr_count > 20:
                import pdb; pdb.set_trace()  # XXX BREAKPOINT
                pass
        return ar_bidx_dis, sr_count


    @staticmethod
    def get_sg_bidxmap_sample_num_elename():
        return [ 'missed_baseb_num','all_baseb_num',  'nsubblock','valid_subblock_num', 'unit_block_num',  'npoint_subblock', 'valid_npoint_subblock','subblock_num' ]
    @staticmethod
    def get_flatten_bidxmaps_sample_num_elename():
        return [ 'flatten_fixed_num', 'flatten_valid_num', 'block_num' ]

    def get_all_bidxmaps(self, rootb_split_idxmap, debug_meta ):
        '''
        fuse bidxmap from cascade_id 0 to end
        (1)sg_bidxmaps_ls: list, len= self.cascade_num-1, start from cascade_id=1
            sg_bidxmaps_ls[cascade_id-1]: (nsubblock,npoint_subblock)
            eg. cascade_id=1; bidxmap1 = sg_bidxmaps_ls[0] are all the cas0_b_indexs in cas1_b.
            bidxmap1[cas1_b_index,cas1_subb_index] = cas0_b_index
            This is used to get grouped points by:  grouped_points0 = tf.gather(points0,bidxmap1)

        (2)flatten_bidxmaps_ls, len = self.cascade_num, start from cascade_id=0
            flatten_bidxmaps_ls[cascade_id]:(last_nsubblock,2)
            eg. cascade_id=0, flatten_bidxmap0 = flatten_bidxmaps_ls[0] are all the cas1_b_index and cas1_subb_index.
            flatten_bidxmap0[cas0_b_index,:] = [cas1_b_index, cas1_subb_index]
            This is used to get raw unsampled points from grouped points by: point0 = tf.gather(grouped_points0,flatten_bidxmap0)
        (3) sg_bidxmap_sample_num:
            [ missed_baseb_num, all_baseb_num,  nsubblock,valid_subblock_num, unit_block_num,  npoint_subblock, valid_npoint_subblock, subblock_num ]

            In an unit block, there a <subblock_num> subbloks. In a subblock, there are <npoint_subblock> points.
            missed_baseb_num  is affected by both sub_block_size & nsubblock.
            Eg. based on sg_bidxmap_sample_num of cascade 0:
                missed_baseb_num_ca0 > 0 => (1) sub_block_size too small or (2) nsubblock_cas0 too small
                nsubblock_cas0 < valid_subblock_num_cas0 => nsubblock_cas0 too small
                npoint_subblock_cas0 < valid_npoint_subblock => npoint_subblock_cas0 too small

            Note: missed_baseb_num != valid_npoint_subblock - npoint_subblock
                  Because some base blocks may not in aim_block.
        (4) flatten_bmap_sample_num:
            [ flatten fixed num, flatten valid num, block_num ]
        '''
        IsCheck_bidxmap_extract = self.IsCheck_gsbb['bidxmap_extract']

        sg_bidxmaps_ls = []
        sg_bidxmaps_fixed_ls =  []
        sg_bidxmaps_fixed_shape1 = self.get_sg_bidxmaps_fixed_shape()[1]
        #sg_bidxmap_sample_num = np.zeros(shape=(self.cascade_num,8)).astype(np.uint64)
        flatten_bidxmaps_ls = []
        #flatten_bmap_sample_num =  np.zeros(shape=(self.cascade_num,3)).astype(np.uint64)

        #sg_bidxmap_sample_num[0,:] = sg_sample_num_cas0
        def fix_var_shape(org_var,aim_shape,axis):
            tile_num = aim_shape - org_var.shape[axis]
            if tile_num == 0:
                new_var = org_var
            elif tile_num < 0:
                assert "tile_num shoue >= 0, but =%d"%(tile_num)
                fix_indices = random_choice( np.arange(0,org_var.shape[axis]), aim_shape )
                new_var = np.take( org_var,fix_indices,axis=axis )
            elif tile_num >0:
                tile_method = 'invalid'
                if tile_method == 'valid':
                    if axis==0:
                        tiled = np.tile(org_var[0:1,:],[tile_num,1])
                    elif axis==1:
                        tiled = np.tile(org_var[:,0:1],[1,tile_num])
                    new_var = np.concatenate([org_var, tiled],axis=axis)
                if tile_method == 'invalid':
                    if axis==0:
                        new_var = np.ones(shape=(aim_shape,org_var.shape[1])).astype(org_var.dtype) * (-1)
                        new_var[0:org_var.shape[0],:] = org_var
                    elif axis==1:
                        new_var = np.ones(shape=(org_var.shape[0],aim_shape)).astype(org_var.dtype) * (-1)
                        new_var[:,0:org_var.shape[1]] = org_var
            sample_num = np.array([aim_shape,org_var.shape[axis],1]).reshape(1,3)
            return new_var, sample_num


        global_num = self.global_num_point
        #flatten_bidxmap0,inv_sample_num0 = fix_var_shape(flatten_bidxmap0,global_num,0)
        #flatten_bidxmaps_ls.append(flatten_bidxmap0)
        #flatten_bmap_sample_num[0,:] = inv_sample_num0
        bxmap_metas = {}

        valid_sorted_basebids_fixed = rootb_split_idxmap
        num_valid_basebids = None
        for cascade_id in range(0,self.cascade_num):
            sg_bidxmap, valid_sorted_basebids_fixed, num_valid_basebids, flatten_bidxmap, bxmap_meta = self.get_bidxmap(cascade_id, valid_sorted_basebids_fixed, num_valid_basebids, debug_meta )
            if IsCheck_bidxmap_extract:  sg_bidxmaps_ls.append( sg_bidxmap )
            sg_bidxmap_fixed = np.ones( shape=(sg_bidxmap.shape[0],sg_bidxmaps_fixed_shape1) ).astype(np.int32) * (-1)
            sg_bidxmap_fixed[:,0:sg_bidxmap.shape[1]] = sg_bidxmap
            sg_bidxmaps_fixed_ls.append( sg_bidxmap_fixed )
            flatten_bidxmaps_ls.append(flatten_bidxmap)

            for key in bxmap_meta:
                if cascade_id==0: bxmap_metas[key] = bxmap_meta[key]
                else:
                    bxmap_metas[key] = np.concatenate( [bxmap_metas[key], bxmap_meta[key]],0 )

            #sg_bidxmap_sample_num[cascade_id,:] = sg_sample_num
            #flatten_bidxmap,sample_num_flatten = fix_var_shape(flatten_bidxmap,self.nsubblock_candis[cascade_id-1],0)
            #flatten_bmap_sample_num[cascade_id,:] = sample_num_flatten
        sg_bidxmaps = np.concatenate(sg_bidxmaps_fixed_ls,axis=0)
        flatten_bidxmaps = np.concatenate(flatten_bidxmaps_ls,axis=0)

        if IsCheck_bidxmap_extract:
            assert sg_bidxmaps.shape == self.get_sg_bidxmaps_fixed_shape()
            assert flatten_bidxmaps.shape ==  self.get_flatten_bidxmaps_shape()
            for cascade_id in range(0,self.cascade_num):
                sg_bidxmap0_extracted = self.extract_sg_bidxmaps(sg_bidxmaps,cascade_id)
                assert np.sum(sg_bidxmaps_ls[cascade_id] != self.extract_sg_bidxmaps(sg_bidxmaps,cascade_id))==0
                assert np.sum(flatten_bidxmaps_ls[cascade_id] != self.extract_flatten_bidxmaps(flatten_bidxmaps,cascade_id))==0
            print( 'bidxmap_extract check OK' )

        return sg_bidxmaps, flatten_bidxmaps, bxmap_metas

    def check_block_scope( self ):
        # Check block position
        for cascade_id in range(0, self.cascade_num):
            aim_attrs = self.get_new_attrs(cascade_id)
            aimbcenter, aimbmin, aimbmax = Sorted_H5f.ixyz_to_xyz( np.array([0,0,0]), aim_attrs )
            print( 'cascade_id',cascade_id )
            print( 'step',aim_attrs['block_step'] )
            print( 'aimbmin',aimbmin )
            import pdb; pdb.set_trace()  # XXX BREAKPOINT
            pass
        import pdb; pdb.set_trace()  # XXX BREAKPOINT
        pass


    def extract_sg_bidxmaps(self, sg_bidxmaps, cascade_id, flag="both"):
        return GlobalSubBaseBLOCK.extract_sg_bidxmaps_( sg_bidxmaps, self.sg_bidxmaps_extract_idx, cascade_id, flag )

    @staticmethod
    def extract_sg_bidxmaps_(sg_bidxmaps, sg_bidxmaps_extract_idx, cascade_id, flag="both"):
        # all cascade sg_bidxmap are concated together into sg_bidxmaps
        # Here to extract one sg_bidxmap of cascade_id
        # include_bcxyz: the last 3 channels of dim 0 is grouped block center xyz
        start = sg_bidxmaps_extract_idx[cascade_id,:]
        end   = sg_bidxmaps_extract_idx[cascade_id+1,:]
        if flag=="both" or "only_aimb_center_xyz_mm":
            include_bcxyz = True
        else:
            include_bcxyz = False
            assert flag == 'only_sg_bidxmap'
        if flag == "only_aimb_center_xyz_mm":
            start_1 = end[1]
        else:
            start_1 = 0

        if sg_bidxmaps.ndim == 2:
            return sg_bidxmaps[ start[0]:end[0], start_1:end[1]+6*include_bcxyz ]
        else:# with batch dimension
            return sg_bidxmaps[ :,start[0]:end[0], start_1:end[1]+6*include_bcxyz ]

    def extract_flatten_bidxmaps(self,flatten_bidxmaps,cascade_id):
        start = self.flatten_bidxmaps_extract_idx[cascade_id,:]
        end = self.flatten_bidxmaps_extract_idx[cascade_id+1,:]
        if flatten_bidxmaps.ndim == 3:
            return flatten_bidxmaps[start[0]:end[0],:,:]
        else:# with batch dimension
            return flatten_bidxmaps[:,start[0]:end[0],:,:]

    def get_sg_bidxmaps_fixed_shape(self):
        # tile all the sg_bidxmaps to same(max) shape[0], so that they can be
        # concatenated in one array
        shape0 = np.sum(self.nsubblock_candis[0:self.cascade_num])
        # add 3 for block center xyz
        shape1 = max(self.npoint_subblock_candis[0:self.cascade_num]) + 6
        return (shape0,shape1)
    def load_one_bidxmap(self,cascade_id,out=['block_num','all_sorted_aimbids','basebids_ina_aim','allbasebids_in_aim_dic'],new_bid=None):
        # load one block id map
        # return block id map from cascade_id-1 to cascade_id
        return self.load_one_bidxmap_(cascade_id,out,new_bid)
    def flatten_bmap_shape0(self,cascade_id):
        if cascade_id==0:
            return self.global_num_point
        else:
            return self.nsubblock_candis[cascade_id-1]
    def get_flatten_bidxmaps_shape(self):
        shape0 = np.sum([self.flatten_bmap_shape0(cid) for cid in range(self.cascade_num)])
        return (shape0,np.max(self.flatbxmap_max_nearest_num),3)

    def load_one_bidxmap_(self,cascade_id,out=['block_num','all_sorted_aimbids','basebids_ina_aim','allbasebids_in_aim_dic','allaimbids_in_base_dic'],aim_bid=None):
        assert os.path.exists(self.bmh5_fn),"file not exist: %s"%(self.bmh5_fn)
        with h5py.File(self.bmh5_fn, 'r') as h5f:
            larger_stride, larger_step = self.get_stride_step_(cascade_id)
            cascade_grp_name = self.get_cascade_grp_name(cascade_id)
            cascade_grp = h5f[ cascade_grp_name ]
            base_in_aim_grp = h5f[ cascade_grp_name + '/aim' ]
            aim_in_base_grp = h5f[ cascade_grp_name + '/base' ]
            bm_output = {}
            if 'block_num' in out or 'all_sorted_aimbids' in out:
                all_sorted_aimbids = cascade_grp['all_sorted_aimbids'][...]
                if 'block_num' in out:
                    bm_output['block_num'] = all_sorted_aimbids.shape[0]
                if 'all_sorted_aimbids' in out:
                    bm_output['all_sorted_aimbids'] = all_sorted_aimbids
            if 'basebids_ina_aim' in out:
                bm_output['basebids_ina_aim'] = base_in_aim_grp[str(aim_bid)][...]
            if 'allbasebids_in_aim_dic' in out:
                allbasebids_in_aim_dic = {}
                for aim_bid_str in base_in_aim_grp:
                    if aim_bid_str != 'all_sorted_aimbids':
                        allbasebids_in_aim_dic[int(aim_bid_str)] = base_in_aim_grp[aim_bid_str][...]
                bm_output['allbasebids_in_aim_dic'] = allbasebids_in_aim_dic

            if 'aimbids_ina_base' in out:
                bm_output['aimbids_ina_base'] = aim_in_base_grp[str(base_bid)][...]
            if 'allaimbids_in_base_dic' in out:
                allaimbids_in_base_dic = {}
                for base_bid_str in aim_in_base_grp:
                    if base_bid_str != 'all_sorted_basebids':
                        allaimbids_in_base_dic[int(base_bid_str)] = aim_in_base_grp[base_bid_str][...]
                bm_output['allaimbids_in_base_dic'] = allaimbids_in_base_dic
            return bm_output

    def show_all_groupnames(self):
        with h5py.File(self.bmh5_fn,'r') as h5f:
            print('\nall group names')
            for group_name in h5f:
                print(group_name)

    def show_all(self):
        self.show_all_groupnames( )
        t0 = time.time()
        new_bid = 0
        for out in ['block_num','all_sorted_aimbids']:
            cascade_id_ls = self.cascade_id_ls
            for cascade_id in cascade_id_ls:
                if cascade_id=='root' and out=='basebids_ina_aim':
                    continue
                output = self.load_one_bidxmap_(cascade_id,[out],new_bid)

                if out=='all_sorted_aimbids':
                    n = output[out].shape[0]
                    print('all_sorted_aimbids: shape=',output[out].shape)
                    print('\t',output[out][0:min(20,n)])
                if out=='block_num':
                    print('block_num:',output[out])
                #print('\nt = %f ms, cascade_id = %s   %s'%(1000*(time.time()-t0),str(cascade_id),out))
        IsIntact,ck_str = GlobalSubBaseBLOCK.check_bmh5_intact( self.bmh5_fn )
        print('IsIntact:',IsIntact)


    def save_bmap_between_dif_stride_step(self):
        '''
        bmh5: structure:
            for each cascde_id, create a group. eg cascade_id='root' grp_name='root-stride_0d1_step_0d1'
            In each grp,
                1) create a dataset: "all_sorted_aimbids", shape=all_sorted_larger_aimbids.shape
                2) for each new_bid, create a dataset: str(new_bid), shape=(len(base_ids),) blockid_map_dset[...] = base_ids

        bmh5 example:
            elements in h5f:
                [u'BASE_stride_0d1_step_0d1-AIM_stride-1d6-step-2', u'BASE_stride_0d1_step_0d1-AIM_stride_0d2_step_0d2',
                 u'BASE_stride_0d2_step_0d2-AIM_stride_0d6_step_0d6', u'BASE_stride_0d6_step_0d6-AIM_stride_1d1_step_1d1', u'root-stride_0d1_step_0d1']
            elements in h5f.attrs:
                [u'is_intact_bmh5', u'max_global_num_point', u'global_num_point', u'global_stride', u'global_step', u'sub_block_stride_candis', u'sub_block_step_candis',
                u'nsubblock_candis', u'npoint_subblock_candis', u'gsbb_config', u'flatbxmap_max_nearest_num', u'flatbxmap_max_dis', u'padding', u'root_block_stride', u'root_block_step']
            elements in  h5f['root-stride_0d1_step_0d1']:
                [u'all_sorted_aimbids']
                * h5f['root-stride_0d1_step_0d1']['all_sorted_aimbids'].shape: (10929,)
            elements in h5f['BASE_stride_0d1_step_0d1-AIM_stride_0d2_step_0d2']:
                [u'aim', u'all_sorted_aimbids', u'base']
                * group 'aim' contains 3017 dataset with names as str(aim_bid), each dataset is a one dimension array, storing all the basebids inside this aim block.
                * group 'base' contains 10929 dataset with names as str(base_bid), each dataset is a one dimension array, storing all the aimbids containing this base block.
        '''
        assert self.mode == 'write'
        folder = os.path.dirname(self.bmh5_fn)
        if not os.path.exists( folder ):
            os.makedirs( folder )
        print('start writing %s'%(self.bmh5_fn))
        # the elements required to draw block
        wanted_attr_eles = ['block_dims_N','block_step','block_stride','xyz_min_aligned']
        bmh5_meta_fn = os.path.splitext(self.bmh5_fn)[0]+'.txt'
        t0 = time.time()
        with h5py.File(self.bmh5_fn,'w') as h5f:
            h5f.attrs['is_intact_bmh5'] = 0
            self.write_gsbbattrs_to_bmh5_bxmh5( h5f.attrs, file_format='bmh5' )

            all_sorted_blockids_dic={}
            all_sorted_blockids_dic['root'] = np.sort([int(k) for k in self.root_s_h5f])

            cascade_id_ls = self.cascade_id_ls
            cascade_attrs = {}
            cascade_attrs['root'] = self.root_s_h5f.attrs
            bmh5_metas = []
            for cascade_id in cascade_id_ls:
                if cascade_id == 'root':
                    all_sorted_larger_aimbids = all_sorted_blockids_dic[cascade_id]
                else:
                    base_cascadeid = self.base_cascade_ids[cascade_id]
                    all_sorted_base_blockids = all_sorted_blockids_dic[base_cascadeid]
                    base_attrs = cascade_attrs[base_cascadeid]
                    larger_stride, larger_step = self.get_stride_step_(cascade_id)
                    new_attrs, basebids_in_largeraimbid_dic, all_sorted_larger_aimbids, aimbids_in_smallerbasebid_dic, bmh5_meta = \
                            GlobalSubBaseBLOCK.get_basebids_in_all_largerbid(base_attrs,all_sorted_base_blockids,larger_stride,larger_step, cascade_id, self.padding)
                    all_sorted_blockids_dic[cascade_id] = all_sorted_larger_aimbids
                    cascade_attrs[cascade_id] = new_attrs

                    bmh5_metas.append( bmh5_meta )
                    #for key in bmh5_meta_i:
                    #    if key in bmh5_meta:
                    #        bmh5_meta[key] += bmh5_meta_i[key]
                    #    else:
                    #        bmh5_meta[key] = bmh5_meta_i[key]
                group_name = self.get_cascade_grp_name(cascade_id)

                grp = h5f.create_group(group_name)

                print('\ncascade_id: %s'%(cascade_id))
                for ele in wanted_attr_eles:
                    grp.attrs[ele] = cascade_attrs[cascade_id][ele]
                    print( ele,':\t', grp.attrs[ele] )

                all_sorted_aimbids_dset = grp.create_dataset( 'all_sorted_aimbids',shape=all_sorted_larger_aimbids.shape,dtype=np.int32  )
                all_sorted_aimbids_dset[...] = all_sorted_larger_aimbids
                if not cascade_id == 'root':
                    for aim_bid, base_bids in basebids_in_largeraimbid_dic.items():
                        base_in_aim_map_dset = grp.create_dataset( 'aim/'+str(aim_bid),shape=(len(base_bids),),dtype=np.int32  )
                        base_in_aim_map_dset[...] = base_bids
                    for base_bid, aim_bids in aimbids_in_smallerbasebid_dic.items():
                        aim_in_base_map_dset = grp.create_dataset( 'base/'+str(base_bid),shape=(len(aim_bids),),dtype=np.int32  )
                        aim_in_base_map_dset[...] = aim_bids
            t_bmh5 = time.time() -t0
            h5f.attrs['t'] = t_bmh5
            h5f.attrs['is_intact_bmh5'] = 1
            h5f.flush()

            with open(bmh5_meta_fn,'w') as bmh5_meta_f:
                bmh5_meta_f.write('Key notes:\n\tReduce lost: increse block_stride of next cascade, increase padding.\n\n')
                for ele in h5f.attrs:
                    bmh5_meta_f.write( '%s: %s\n'%(ele, h5f.attrs[ele]) )
                bmh5_meta_f.write('\n\n')
                for cas, bmh5_meta in enumerate(bmh5_metas):
                    if cas == len(bmh5_metas)-1:
                        bmh5_meta_f.write('cascade global:\n')
                    else:
                        bmh5_meta_f.write('cascade %d:\n'%(cas))
                    for key, value in bmh5_meta.items():
                        bmh5_meta_f.write( '\t%s: %s \n'%(key, value) )
                bmh5_meta_f.write( '\ngen t: %0.2f sec'%(t_bmh5) )
            print('write finish: %s'%(self.bmh5_fn))


    @staticmethod
    def check_bmh5_intact(file_name):
        #file_name = self.get_bmapfn(root_s_h5f_fn)
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.bmh5'
        if not os.path.exists(file_name):
            return False,"%s not exist"%(file_name)
        #if os.path.getsize( file_name ) / 1000.0 < 10:
        #    return False,"file too small < 20 K"
        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        #print('checking bmh5 file:',file_name)
        with h5py.File(file_name,'r') as h5f:
            if 'is_intact_bmh5' not in h5f.attrs:
                return False,""
            IsIntact = h5f.attrs['is_intact_bmh5'] == 1
            if IsIntact:
               if not SHOW_ONLY_ERR:  print('bmh5 file intact:',file_name)
            return IsIntact,""

    @staticmethod
    def get_scope_of_bids(bids,attrs):
        xyz_min = np.array([1.0,1.0,1.0])*(10000)
        xyz_max = np.array([1.0,1.0,1.0])*(-10000)
        for i in range(bids.size):
            block_min, block_max, i_xyz = Sorted_H5f.get_block_scope_from_k_(bids[i],attrs)
            for j in range(3):
                if block_min[j] < xyz_min[j]:
                    xyz_min[j] = block_min[j]
                if block_max[j] > xyz_max[j]:
                    xyz_max[j] = block_max[j]
        assert (xyz_max > xyz_min).all()
        xyz_scope = xyz_max - xyz_min
        return xyz_scope
    @staticmethod
    def get_basebids_in_all_largerbid(base_attrs, all_base_bids, larger_stride, larger_step, cascade_id, padding):
        '''
        find all the valid block ids with larger_stride and larger_step,
        and all the base block ids in each larger_stride and larger_step.
        root_s_h5f: sorted h5f object
        '''
        new_sorted_h5f_attrs = Sorted_H5f.get_attrs_of_new_stride_step_(base_attrs,larger_stride,larger_step)
        new_block_dims_N = new_sorted_h5f_attrs['block_dims_N']
        new_total_block_N = 0

        IsSortRes = True
        GroupingMethod = 'search_by_voxel'    # 26.4 s
        GroupingMethod = 'search_by_point'   # 8.7 s
        IsCheckTwoMethodsSame = False

        if GroupingMethod == 'search_by_voxel' or IsCheckTwoMethodsSame:
            basebids_in_largeraimbid_dic_1 = {}
            aimbids_in_smallerbasebid_dic_1 = {}
            max_new_block_id = Sorted_H5f.ixyz_to_block_index_(new_block_dims_N-1,new_sorted_h5f_attrs)
            print('max_new_block_id = ',max_new_block_id)
            for new_block_id in range(max_new_block_id+1):
                base_bid_ls,_ = Sorted_H5f.get_blockids_of_dif_stride_step(
                                        new_block_id, new_sorted_h5f_attrs, base_attrs, padding=padding, cascade_id=cascade_id )
                base_bids = np.array(base_bid_ls).astype(np.uint32)
                mask = np.in1d( base_bids,all_base_bids )
                valid_base_bids = base_bids[mask]

                # check the scope of valid_cur_bids
                if valid_base_bids.shape[0] > 0:
                    valid_scope = GlobalSubBaseBLOCK.get_scope_of_bids( valid_base_bids, base_attrs )
                    scope_rate = valid_scope / new_sorted_h5f_attrs['block_step']
                    valid_scope_rate = np.min(scope_rate)
                    if valid_scope_rate > 0.0:
                        new_total_block_N += 1
                        basebids_in_largeraimbid_dic_1[new_block_id] = valid_base_bids
                    else:
                        import pdb; pdb.set_trace()  # XXX BREAKPOINT
                        pass

                if new_block_id >0 and new_block_id % 5000==0:
                    rate = 1.0*(new_block_id+1)/(max_new_block_id+1)*100
                    print('%f%%  new id: %d  new stride step: %s      base stride step: %s'%(rate,new_block_id,
                        get_stride_step_name(larger_stride,larger_step),get_stride_step_name(base_attrs['block_stride'],base_attrs['block_step'])))
            basebids_in_largeraimbid_dic = basebids_in_largeraimbid_dic_1
            # get aimbids_in_smallerbasebid_dic_1
            for aim_bid, basebids in basebids_in_largeraimbid_dic_1.items():
                for base_bid in basebids:
                    if base_bid not in aimbids_in_smallerbasebid_dic_1:
                        aimbids_in_smallerbasebid_dic_1[base_bid] = np.array( [aim_bid] )
                    else:
                        aimbids_in_smallerbasebid_dic_1[base_bid] = np.concatenate( [aimbids_in_smallerbasebid_dic_1[base_bid],np.array( [aim_bid] )]  )
            for base_bid in all_base_bids:
                if base_bid not in basebids_in_largeraimbid_dic_1:
                    basebids_in_largeraimbid_dic_1[base_bid] = []
            if IsSortRes:
                for key in aimbids_in_smallerbasebid_dic_1:
                    aimbids_in_smallerbasebid_dic_1[key].sort()
            num_lost_baseb = all_base_bids.size - len(aimbids_in_smallerbasebid_dic_1)
            aimbids_in_smallerbasebid_dic = aimbids_in_smallerbasebid_dic_1
        if GroupingMethod == 'search_by_point' or IsCheckTwoMethodsSame:
            #print('base step: %s'%(base_attrs['block_step']))
            #print('base stride: %s'%(base_attrs['block_stride']))
            #print('aim step: %s'%(new_sorted_h5f_attrs['block_step']))
            #print('aim stride: %s'%(new_sorted_h5f_attrs['block_stride']))
            basebids_in_largeraimbid_dic_2 = {}
            aimbids_in_smallerbasebid_dic_2 = {}
            num_lost_baseb = 0  # Reduce lost: increse aim_stride, increase padding
            for j, base_bid in  enumerate(all_base_bids):
                new_bids_ls,_ = Sorted_H5f.get_blockids_of_dif_stride_step(
                                        base_bid, base_attrs, new_sorted_h5f_attrs, padding=padding, cascade_id=cascade_id )
                aimbids_in_smallerbasebid_dic_2[base_bid] = np.array( new_bids_ls )
                if len(new_bids_ls)==0:
                    num_lost_baseb += 1
                    continue
                for new_bid in new_bids_ls:
                    if new_bid not in basebids_in_largeraimbid_dic_2:
                        basebids_in_largeraimbid_dic_2[new_bid] = np.array([],dtype=np.uint32)
                    basebids_in_largeraimbid_dic_2[new_bid] = np.append( basebids_in_largeraimbid_dic_2[new_bid], base_bid )

                if j>0 and j % int(all_base_bids.size/5) == 0:
                    rate = 100.0*j/all_base_bids.size
                    print('%f%%  new stride step: %s      base stride step: %s'%(rate,
                        get_stride_step_name(larger_stride,larger_step),get_stride_step_name(base_attrs['block_stride'],base_attrs['block_step'])))

            # sort basebids_in_largeraimbid_dic_2
            if IsSortRes:
                for key in basebids_in_largeraimbid_dic_2:
                    basebids_in_largeraimbid_dic_2[key].sort()

            if IsCheckTwoMethodsSame:
                assert len(aimbids_in_smallerbasebid_dic_1) == len(aimbids_in_smallerbasebid_dic_2)
                for key, value in aimbids_in_smallerbasebid_dic_1.items():
                    value_2 = aimbids_in_smallerbasebid_dic_2[key]
                    if  value.size != value_2.size or  np.not_equal(value, value_2).sum() != 0:
                        import pdb; pdb.set_trace()  # XXX BREAKPOINT
                        pass
                assert len( basebids_in_largeraimbid_dic_1 ) == len( basebids_in_largeraimbid_dic_2 )
                for key, value in basebids_in_largeraimbid_dic_1.items():
                    value_2 = basebids_in_largeraimbid_dic_2[key]
                    if  value.size != value_2.size or  np.not_equal(value, value_2).sum() != 0:
                        import pdb; pdb.set_trace()  # XXX BREAKPOINT
                        pass
                print('Two grouping methods check to be the same')
                import pdb; pdb.set_trace()  # XXX BREAKPOINT
            basebids_in_largeraimbid_dic = basebids_in_largeraimbid_dic_2
            aimbids_in_smallerbasebid_dic = aimbids_in_smallerbasebid_dic_2


        larger_blockids = np.array(list(basebids_in_largeraimbid_dic.keys())).astype(np.uint32)
        all_sorted_larger_aimbids = np.sort(larger_blockids)
        bmh5_meta = {}
        bmh5_meta['num_lost_baseb'] = num_lost_baseb
        bmh5_meta['base_block_num'] = all_base_bids.size
        bmh5_meta['aim_block_num'] = all_sorted_larger_aimbids.size
        bmh5_meta['GroupingMethod'] = GroupingMethod
        assert 1.0 * num_lost_baseb / all_base_bids.size  < 0.6, "lost too many base b %d / %d "%( num_lost_baseb, all_base_bids.size )

        #if len(aimbids_in_smallerbasebid_dic) != all_base_bids.size:
        #    import pdb; pdb.set_trace()  # XXX BREAKPOINT
        #    assert False, "all_base_bids.size=%d  len(aimbids_in_smallerbasebid_dic)=%d"%( all_base_bids.size, len(aimbids_in_smallerbasebid_dic) )
        #    pass
        # check: basebids_in_largeraimbid_dic shoule contain all the all_base_bids. If larger_stride==larger_step, each base bid should occur one time.
        if GlobalSubBaseBLOCK.IsCheck_gsbb['all_base_included']:
            all_base_bids_indic = np.concatenate( basebids_in_largeraimbid_dic.values()).astype(np.int32)
            if not (larger_stride == larger_step).all():
                all_base_bids_indic = np.setxor1d(all_base_bids_indic,np.array([])).astype(np.int32)
            all_base_bids_indic = np.sort(all_base_bids_indic)
            if not all_base_bids_indic.shape[0] == all_base_bids.shape[0]:
                assert False, "Not  all the base blocks are exactly included"
            if not (all_base_bids_indic == all_base_bids).all():
                assert False, "Not  all the base blocks are exactly included"

            #print('\nbasebids in each largerbid dic check ok\n  new stride step: %s      base stride step: %s'%(
            #          get_stride_step_name(larger_stride,larger_step),get_stride_step_name(base_attrs['block_stride'],base_attrs['block_step'])))

        return new_sorted_h5f_attrs, basebids_in_largeraimbid_dic, all_sorted_larger_aimbids, aimbids_in_smallerbasebid_dic, bmh5_meta

    @staticmethod
    def fix_bmap( cascade_id, all_sorted_aimbids, all_base_bids_in_aim_dic, nsubblock, npoint_subblock, aim_attrs, debug_meta, IsCheckMissingAimb=False):
        '''
        When aim block num is larger than nsubblock, select the aim blocks with more points
        '''
        merge_blocks_while_fix_bmap = NETCONFIG['merge_blocks_while_fix_bmap']

        fix_bmap_method = GlobalSubBaseBLOCK.settings['fix_bmap_method']
        debug_meta['fix_bmap_method'] = fix_bmap_method

        if IsCheckMissingAimb:
            org_all_sorted_aimbids = np.copy( all_sorted_aimbids )
            org_baseb_num = 0
            for baseb_ls in all_base_bids_in_aim_dic.values():
                org_baseb_num += len(baseb_ls)

        all_base_bids_in_aim_dic_fixed = all_base_bids_in_aim_dic
        if fix_bmap_method == 'random':
            choice = random_choice( np.arange(all_sorted_aimbids.shape[0]), nsubblock, keeporder=True )
            kept_aim_bids = all_sorted_aimbids[choice]
            cropped_aim_bids = [ bid for bid in all_sorted_aimbids if bid not in kept_aim_bids ]
            # merge base_bids of each cropped aim b to closest aim b
            ar_bidx_dis, sr_count = GlobalSubBaseBLOCK.get_around_bid_multibase( cropped_aim_bids, aim_attrs, kept_aim_bids, max_need_num=3, max_search_dis=1 )
            for i in range( len(cropped_aim_bids) ):
                cropped_aim_bid = cropped_aim_bids[i]
                if ar_bidx_dis[i].shape[0]==0:
                    # cannot find a neighbor
                    print('no neighbor for %d'%(cropped_aim_bid))
                    continue
                ar_aimb_indexes = ar_bidx_dis[i][:,0].astype(np.int32)
                baseb_nums = [len(all_base_bids_in_aim_dic_fixed[kept_aim_bids[aimb_index]]) for aimb_index in ar_aimb_indexes ]
                j = np.argmin( np.array(baseb_nums) )
                ar_aimb_index = int(ar_bidx_dis[i][j,0])
                ar_aim_bid = kept_aim_bids[ ar_aimb_index ]


                #all_base_bids_in_aim_dic_fixed[ar_aim_bid] = np.concatenate( [all_base_bids_in_aim_dic_fixed[ar_aim_bid], all_base_bids_in_aim_dic_fixed[cropped_aim_bid]] )

                #print( 'crop\n',all_base_bids_in_aim_dic_fixed[cropped_aim_bid] )
                #print( 'ar\n',all_base_bids_in_aim_dic_fixed[ar_aim_bid] )
                # save the larger one
                if len( all_base_bids_in_aim_dic_fixed[cropped_aim_bid] ) >= len( all_base_bids_in_aim_dic_fixed[ar_aim_bid] ):
                    #print('crop:',cropped_aim_bids[i])
                    #print('kept:',kept_aim_bids[ ar_aimb_index ])
                    cropped_aim_bids[i] = ar_aim_bid
                    kept_aim_bids[ ar_aimb_index ] = cropped_aim_bid
                    all_base_bids_in_aim_dic_fixed[cropped_aim_bid] = np.concatenate( [all_base_bids_in_aim_dic_fixed[cropped_aim_bid], all_base_bids_in_aim_dic_fixed[ar_aim_bid ]] )
                    #print('crop:',cropped_aim_bids[i])
                    #print('kept:',kept_aim_bids[ ar_aimb_index ])
                else:
                    all_base_bids_in_aim_dic_fixed[ar_aim_bid] = np.concatenate( [all_base_bids_in_aim_dic_fixed[ar_aim_bid], all_base_bids_in_aim_dic_fixed[cropped_aim_bid]] )
                #print( 'crop\n',all_base_bids_in_aim_dic_fixed[cropped_aim_bid] )
                #print( 'ar\n',all_base_bids_in_aim_dic_fixed[ar_aim_bid] )
                #print('ar_aim_bid:', ar_aim_bid)

            for cut_aim_bid in cropped_aim_bids:
                del all_base_bids_in_aim_dic_fixed[cut_aim_bid]

        elif fix_bmap_method == 'num_order':
            # merge sub_blocks in order from the ones with less base_blocks
            # any one only merge once
            def get_baseb_num_ls( all_sorted_aimbids ):
                org_aim_b_num = all_sorted_aimbids.shape[0]
                assert org_aim_b_num >= nsubblock
                base_b_num = np.zeros(shape=(org_aim_b_num)).astype(np.uint32)
                for i in range(org_aim_b_num):
                    base_b_num[i] = len(all_base_bids_in_aim_dic_fixed[all_sorted_aimbids[i]])
                return base_b_num
            loop_n = 0
            max_loop_n = 3
            while ( all_sorted_aimbids.shape[0] != nsubblock ):
                if loop_n >= max_loop_n or merge_blocks_while_fix_bmap==False:
                    # randomly select and abandon some
                    choice = random_choice( np.arange(all_sorted_aimbids.shape[0]), nsubblock, keeporder=True )
                    all_sorted_aimbids = all_sorted_aimbids[choice]
                    break

                loop_n += 1
                base_b_num = get_baseb_num_ls( all_sorted_aimbids )
                sort_aim_indices = np.argsort(base_b_num)

                n_delb = 0
                cropped_aim_bids = [ all_sorted_aimbids[j] for j in sort_aim_indices ]
                kept_aim_bids = []
                for cropped_aim_bid in cropped_aim_bids:
                    if cropped_aim_bid not in all_sorted_aimbids:
                        # deleted because this block already merge another, do not
                        # merge again
                        continue
                    ar_bidx_dis, sr_count = GlobalSubBaseBLOCK.get_around_bid_1base( cropped_aim_bid, aim_attrs, all_sorted_aimbids, max_need_num=1, max_search_dis=1 )
                    if ar_bidx_dis.shape[0]==0:
                        # cannot find a neighbor
                        #print('no neighbor for %d'%(cropped_aim_bid))
                        continue
                    ar_aimb_indexes = ar_bidx_dis[:,0].astype(np.int32)
                    baseb_nums = [len(all_base_bids_in_aim_dic_fixed[all_sorted_aimbids[aimb_index]]) for aimb_index in ar_aimb_indexes ]
                    m = np.argmin( np.array(baseb_nums) )
                    ar_aimb_index = int(ar_bidx_dis[m,0])
                    ar_aim_bid = all_sorted_aimbids[ ar_aimb_index ]
                    all_base_bids_in_aim_dic_fixed[ar_aim_bid] = np.unique( np.concatenate( [all_base_bids_in_aim_dic_fixed[ar_aim_bid], all_base_bids_in_aim_dic_fixed[cropped_aim_bid]] ) )

                    del all_base_bids_in_aim_dic_fixed[cropped_aim_bid]
                    j = index_in_sorted( all_sorted_aimbids, cropped_aim_bid )[0]
                    assert all_sorted_aimbids[j] == cropped_aim_bid
                    all_sorted_aimbids = np.delete( all_sorted_aimbids, j )
                    j = index_in_sorted( all_sorted_aimbids, ar_aim_bid )[0]
                    assert all_sorted_aimbids[j] == ar_aim_bid
                    all_sorted_aimbids = np.delete( all_sorted_aimbids, j )
                    kept_aim_bids.append( ar_aim_bid )
                    if all_sorted_aimbids.shape[0] + len(kept_aim_bids) <= nsubblock:
                        break
                all_sorted_aimbids = np.sort( np.concatenate( [np.array(kept_aim_bids), all_sorted_aimbids] ) )
            kept_aim_bids = all_sorted_aimbids
            #print( 'loop_n:', loop_n )

            #base_b_num_1 = get_baseb_num_ls( all_sorted_aimbids )

        else:
            assert False

        # check lost base b
        if IsCheckMissingAimb:
            fixed_baseb_num = 0
            for baseb_ls in all_base_bids_in_aim_dic_fixed.values():
                fixed_baseb_num += len(baseb_ls)
            lost_b_num = org_baseb_num -  fixed_baseb_num
            print('lost_b_num:',lost_b_num)
            if debug_meta['global_bidx'] < 5:
                debug_meta['cascade_id'] = cascade_id
                GlobalSubBaseBLOCK.draw_fix_progress( org_all_sorted_aimbids, kept_aim_bids, aim_attrs, debug_meta )

        return kept_aim_bids, all_base_bids_in_aim_dic_fixed

    @staticmethod
    def draw_fix_progress( org_aimbids, kept_aim_bids, aim_attrs, debug_meta ):
        org_xyz_center, _, _ = Sorted_H5f.block_index_to_xyz_( org_aimbids, aim_attrs )
        kept_xyz_center, _, _ = Sorted_H5f.block_index_to_xyz_( kept_aim_bids, aim_attrs )
        region_name = os.path.splitext( os.path.basename(debug_meta['bxmh5_fn']) )[0]
        path = os.path.dirname( debug_meta['bxmh5_fn'] ) + '/fixbmap_ply_'+debug_meta['fix_bmap_method']
        if not os.path.exists( path ):
            os.makedirs(path)
        org_ply_fn = '%s/%s_gb%d_cas%d_org.ply'%( path,region_name,debug_meta['global_bidx'], debug_meta['cascade_id'] )
        kept_ply_fn = '%s/%s_gb%d_cas%d_kept.ply'%( path,region_name,debug_meta['global_bidx'], debug_meta['cascade_id'] )
        ply_util.create_ply( org_xyz_center, org_ply_fn, force_color=[0,255,0] )
        ply_util.create_ply( kept_xyz_center, kept_ply_fn, force_color=[255,0,0] )
        print('gen %s\ngen %s\n'%( org_ply_fn, kept_ply_fn ))

    def gen_bmap_ply( self, sph5_fn ):
        assert self.mode=='load'
        IsIntact_sph5, ck_str = Normed_H5f.check_sph5_intact( sph5_fn )
        IsIntact_bmh5, ck_str = GlobalSubBaseBLOCK.check_bmh5_intact( self.bmh5_fn )
        if not (IsIntact_sph5 and IsIntact_bmh5):
            return
        print('start gen ply for %s'%(self.bmh5_fn))
        region_name = os.path.splitext( os.path.basename(self.bmh5_fn) )[0]
        house_path = os.path.dirname(self.bmh5_fn)
        ply_path = '%s/ply'%(house_path)
        if not os.path.exists(ply_path):
            os.makedirs(ply_path)

        with h5py.File( self.bmh5_fn, 'r' ) as bmh5f:
          with h5py.File( sph5_fn, 'r' ) as sph5f:
            pl_xyz = sph5f['data'][...,0:3]
            for cascade_id in self.cascade_id_ls:
                group_name = self.get_cascade_grp_name(cascade_id)
                grp = bmh5f[group_name]
                all_sorted_aimbids = grp['all_sorted_aimbids'][:]
                attrs = grp.attrs
                bxyz_center, bxyz_min, bxyz_max = Sorted_H5f.block_index_to_xyz_( all_sorted_aimbids, attrs )

                ply_fn = '%s/bmap_%s_cascade_%s.ply'%(ply_path, region_name, cascade_id)
                ply_util.gen_box_norotation( ply_fn, bxyz_min, bxyz_max )

    def gen_bxmap_ply( self, sph5_fn, bxmh5_fn ):
        IsIntact_sph5_bxmap,ck_str = Normed_H5f.check_sph5_intact( bxmh5_fn )
        IsIntact_sph5, ck_str = Normed_H5f.check_sph5_intact( sph5_fn )
        IsIntact_bmh5, ck_str = GlobalSubBaseBLOCK.check_bmh5_intact( self.bmh5_fn )
        if not (IsIntact_sph5_bxmap and IsIntact_sph5 and IsIntact_bmh5):
            return
        print('start gen ply for %s'%(bxmh5_fn))
        with h5py.File( bxmh5_fn, 'r' ) as bxmh5f:
         with h5py.File( self.bmh5_fn, 'r' ) as bmh5f:
          with h5py.File( sph5_fn, 'r' ) as sph5f:
            pl_xyz = sph5f['data'][...,0:3]
            sg_all_bidxmaps = bxmh5f['bidxmaps_sample_group']
            flatten_bidxmaps = bxmh5f['bidxmaps_flat']

            region_name = os.path.splitext( os.path.basename( bxmh5_fn ) )[0]
            house_path = os.path.dirname( bxmh5_fn )
            ply_path = '%s/ply'%(house_path)
            if not os.path.exists(ply_path):
                os.makedirs(ply_path)

            pl_xyz_grps = []
            pl_xyz_grps.append( pl_xyz )
            sg_pl_ls = []

            for cascade_id in range(self.cascade_num):
                # gen sg ply
                sg_bidxmap_i = self.extract_sg_bidxmaps( sg_all_bidxmaps, cascade_id )

                batch_size, nsubblock, npoint_subblock = sg_bidxmap_i.shape
                sg_pl = np.zeros([batch_size,nsubblock,npoint_subblock,3])
                for b in range( batch_size ):
                    pl_batch = np.zeros( [nsubblock,npoint_subblock,3] )
                    for i in range(nsubblock):
                        pl_subb = np.zeros( [npoint_subblock,3] )
                        for j in range( npoint_subblock ):
                            k = sg_bidxmap_i[b,i,j]
                            pl_subb[j,:] = pl_xyz_grps[cascade_id][b, k, :]
                        pl_batch[i,...] = pl_subb
                    sg_pl[b,...] = pl_batch
                sg_pl_ls.append( sg_pl )
                pl_xyz_grps.append( sg_pl.mean(2) )

                ply_fn = '%s/sg_%s_cascade_%d.ply'%(ply_path, region_name, cascade_id)
                ply_util.create_ply_matterport( sg_pl, ply_fn )
                ply_fn = '%s/input_of_sg_%s_cascade_%d.ply'%(ply_path, region_name, cascade_id)
                ply_util.create_ply_matterport( pl_xyz_grps[cascade_id], ply_fn )

                # gen flatten ply
                flatten_bidxmap_i = self.extract_flatten_bidxmaps( flatten_bidxmaps, cascade_id )
                pl_flatten = np.zeros( flatten_bidxmap_i.shape[0:-2]+(3,) )
                n_point = flatten_bidxmap_i.shape[1]
                for b in range( batch_size ):
                    for i in range( n_point ):
                        # are exactly the base points
                        aim_b_index, point_index_in_aimb = flatten_bidxmap_i[b,i,0,:]
                        pl_flatten[b,i,:] = sg_pl[b, aim_b_index, point_index_in_aimb,:]

                ply_fn = '%s/flat_%s_cascade_%d.ply'%(ply_path, region_name, cascade_id)
                ply_util.create_ply_matterport( pl_flatten, ply_fn )


gsbb_empty =  GlobalSubBaseBLOCK()

class Raw_H5f():
    '''
    * raw data:unsorted points,all the time in one dataset
    * Each data type as a hdf5 dataset: xyz, intensity, label, color
    * class "Sorted_H5f" will sort data to blocks based on this class
    '''
    file_flag = 'RAW_H5F'
    h5_num_row_1M = 50*1000
    dtypes = { 'xyz':np.float32, 'nxnynz':np.float32, 'intensity':np.int32, 'color':np.uint8,'label_category':np.uint32,'label_instance':np.int32,'label_material':np.int32, 'label_mesh':np.int32 }
    num_channels = {'xyz':3,'nxnynz':3,'intensity':1,'color':3,'label_category':1,'label_instance':1,'label_material':1,'label_mesh':1}
    def __init__(self,raw_h5_f,file_name,datasource_name=None):
        self.h5f = raw_h5_f
        if datasource_name == None:
            assert 'datasource_name' in self.h5f.attrs
        else:
            self.h5f.attrs['datasource_name'] = datasource_name
        assert self.h5f.attrs['datasource_name'] in DATA_SOURCE_NAME_LIST
        self.get_summary_info()
        self.file_name = file_name
        self.num_default_row = 0

    def show_h5f_summary_info(self):
        print('\n\nsummary of file: ',self.file_name)
        return show_h5f_summary_info(self.h5f)

    def set_num_default_row(self,N):
        self.num_default_row = N

    def get_dataset(self,data_name):
        if data_name in self.h5f:
            return self.h5f[data_name]
        assert(data_name in self.dtypes)
        nc = self.num_channels[data_name]
        dset = self.h5f.create_dataset(data_name,shape=(self.num_default_row,nc),\
                                    maxshape=(None,nc),dtype=self.dtypes[data_name],\
                                    chunks = (self.h5_num_row_1M,nc),\
                                    compression = "gzip")
        dset.attrs['valid_num'] = 0
        setattr(self,data_name+'_dset',dset)
        if 'element_names' not in self.h5f.attrs:
            self.h5f.attrs['element_names'] = [data_name]
        else:
            self.h5f.attrs['element_names'] = [data_name]+[e for e in self.h5f.attrs['element_names']]
        return dset
    def get_total_num_channels_name_list(self):
        total_num_channels = 0
        data_name_list = [str(dn) for dn in self.h5f]
        for dn in data_name_list:
            total_num_channels += self.num_channels[dn]

        return total_num_channels,data_name_list

    def append_to_dset(self,dset_name,new_data):
       self.add_to_dset(dset_name,new_data,None,None)

    def get_all_dsets(self,start_idx,end_idx):
        out_dset_order = ['xyz','color','label','intensity']
        data_list = []
        for dset_name in out_dset_order:
            if dset_name in self.h5f:
                data_k = self.h5f[dset_name][start_idx:end_idx,:]
                data_list.append(data_k)
        data = np.concatenate(data_list,1)
        return data

    def add_to_dset(self,dset_name,new_data,start,end):
        dset = self.get_dataset(dset_name)
        valid_n  = dset.attrs['valid_num']
        if start == None:
            start = valid_n
            end = start + new_data.shape[0]
        if dset.shape[0] < end:
            dset.resize((end,)+dset.shape[1:])
        if valid_n < end:
            dset.attrs['valid_num'] = end
        if new_data.ndim==1 and dset.ndim==2 and dset.shape[1]==1:
            new_data = np.expand_dims(new_data,1)
        dset[start:end,:] = new_data

    def rm_invalid(self):
        for dset_name in self.h5f:
            dset = self.h5f[dset_name]
            if 'valid_num' in dset.attrs:
                valid_num = dset.attrs['valid_num']
                if valid_num < dset.shape[0]:
                    dset.resize( (valid_num,dset.shape[1:]) )

    def get_summary_info(self):
        for dset_name in self.h5f:
            setattr(self,dset_name+'_dset',self.h5f[dset_name])
        if 'xyz' in self.h5f:
            self.total_row_N = self.xyz_dset.shape[0]
            self.xyz_max = self.xyz_dset.attrs['max']
            self.xyz_min = self.xyz_dset.attrs['min']
            self.xyz_scope = self.xyz_max - self.xyz_min


    def generate_objfile(self,obj_file_name=None,IsLabelColor=False,xyz_cut_rate=None):
        if obj_file_name==None:
            base_fn = os.path.basename(self.file_name)
            base_fn = os.path.splitext(base_fn)[0]
            folder_path = os.path.dirname(self.file_name)
            obj_folder = os.path.join(folder_path,'obj/'+base_fn)
            print('obj_folder:',obj_folder)
            obj_file_name = os.path.join(obj_folder,base_fn+'.obj')
            obj_file_name_nocolor = os.path.join(obj_folder,base_fn+'_xyz.obj')
            if not os.path.exists(obj_folder):
                os.makedirs(obj_folder)
            print('automatic obj file name: %s'%(obj_file_name))


        with open(obj_file_name,'w') as out_obj_file:
          with open(obj_file_name_nocolor,'w') as xyz_obj_file:
            xyz_dset = self.xyz_dset
            if 'color' in self.h5f:
                color_dset = self.color_dset
            else:
                IsLabelColor = True
            label_category_dset = self.label_category_dset

            if xyz_cut_rate != None:
                # when rate < 0.5: cut small
                # when rate >0.5: cut big
                xyz_max = np.array([ np.max(xyz_dset[:,i]) for i in range(3) ])
                xyz_min = np.array([ np.min(xyz_dset[:,i]) for i in range(3) ])
                xyz_scope = xyz_max - xyz_min
                xyz_thres = xyz_scope * xyz_cut_rate + xyz_min
                print('xyz_thres = ',str(xyz_thres))
            cut_num = 0

            row_step = self.h5_num_row_1M * 10
            row_N = xyz_dset.shape[0]
            for k in range(0,row_N,row_step):
                end = min(k+row_step,row_N)
                xyz_buf_k = xyz_dset[k:end,:]

                if 'color' in self.h5f:
                    color_buf_k = color_dset[k:end,:]
                    buf_k = np.hstack((xyz_buf_k,color_buf_k))
                else:
                    buf_k = xyz_buf_k
                label_k = label_category_dset[k:end,0]
                for j in range(0,buf_k.shape[0]):
                    is_cut_this_point = False
                    if xyz_cut_rate!=None:
                        # cut by position
                        for xyz_j in range(3):
                            if (xyz_cut_rate[xyz_j] >0.5 and buf_k[j,xyz_j] > xyz_thres[xyz_j]) or \
                                (xyz_cut_rate[xyz_j]<=0.5 and buf_k[j,xyz_j] < xyz_thres[xyz_j]):
                                is_cut_this_point =  True
                    if is_cut_this_point:
                        cut_num += 1
                        continue

                    if not IsLabelColor:
                        str_j = 'v   ' + '\t'.join( ['%0.5f'%(d) for d in  buf_k[j,0:3]]) + '  \t'\
                        + '\t'.join( ['%d'%(d) for d in  buf_k[j,3:6]]) + '\n'
                    else:
                        label = label_k[j]
                        label_color = Normed_H5f.g_label2color_dic[self.h5f.attrs['datasource_name']][label]
                        str_j = 'v   ' + '\t'.join( ['%0.5f'%(d) for d in  buf_k[j,0:3]]) + '  \t'\
                        + '\t'.join( ['%d'%(d) for d in  label_color ]) + '\n'
                    nocolor_str_j = 'v   ' + '\t'.join( ['%0.5f'%(d) for d in  buf_k[j,0:3]]) + '  \n'
                    out_obj_file.write(str_j)
                    xyz_obj_file.write(nocolor_str_j)

            print('gen raw obj: %s'%(obj_file_name,))

    def rh5_create_done(self):
        self.rm_invalid()
        self.add_geometric_scope()

        self.write_raw_summary()
        #self.show_h5f_summary_info()

    def write_raw_summary(self):
        summary_fn = os.path.splitext( self.file_name )[0]+'.txt'
        with open(summary_fn,'w') as summary_f:
            summary_f.write( self.show_h5f_summary_info() )

    def add_geometric_scope(self,line_num_limit=None):
        ''' calculate the geometric scope of raw h5 data, and add the result to attrs of dset'''
        #begin = time.time()
        max_xyz = -np.ones((3))*1e10
        min_xyz = np.ones((3))*1e10

        xyz_dset = self.xyz_dset
        row_step = self.h5_num_row_1M
        print('File: %s   %d lines'\
              %(os.path.basename(self.file_name),xyz_dset.shape[0]) )
        #print('read row step = %d'%(row_step))

        for k in range(0,xyz_dset.shape[0],row_step):
            end = min(k+row_step,xyz_dset.shape[0])
            xyz_buf = xyz_dset[k:end,:]
            xyz_buf_max = xyz_buf.max(axis=0)
            xyz_buf_min = xyz_buf.min(axis=0)
            max_xyz = np.maximum(max_xyz,xyz_buf_max)
            min_xyz = np.minimum(min_xyz,xyz_buf_min)

            if line_num_limit!=None and k > line_num_limit:
                print('break at k = ',line_num_limit)
                break
        xyz_dset.attrs['max'] = max_xyz
        xyz_dset.attrs['min'] = min_xyz
        self.h5f.attrs['xyz_max'] = max_xyz
        self.h5f.attrs['xyz_min'] = min_xyz
        max_str = '  '.join([ str(e) for e in max_xyz ])
        min_str = '  '.join([ str(e) for e in min_xyz ])
        print('max_str=%s\tmin_str=%s'%(max_str,min_str) )
        #print('T=',time.time()-begin)

    @staticmethod
    def check_rh5_intact( file_name ):
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.rh5'
        if not os.path.exists(file_name):
            return False, "%s not exist"%(file_name)
        #if os.path.getsize( file_name ) / 1000.0 < 100:
        #    return False,"file too small < 20 K"
        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        with h5py.File(file_name,'r') as h5f:
            attrs_to_check = ['xyz_max','xyz_min']
            for attrs in attrs_to_check:
                if attrs not in h5f.attrs:
                    return False, "%s not in %s"%(attrs,file_name)
        return True,""


class Sorted_H5f():
    '''
    (1) sorted: sort Raw_H5f by position to blocks, each block in one dataset.
        The dataset name is the voxel index.
    (2) store all types of data (xyz,color,intensity,label..) together (float32) in one dataset
    (3) All the information are stored in self.h5f.attrs,like:
The root_attr:  [u'datasource_name', u'xyz_max', u'xyz_min', u'element_names', u'stride_to_align', u'block_step', u'block_stride', u'block_dims_N', u'xyz_min_aligned', u'xyz_max_aligned', u'xyz_scope_aligned', u'total_row_N', u'total_block_N']
datasource_name: MATTERPORT
element_names: ['label_material' 'label_instance' 'label_category' 'color' 'nxnynz' 'xyz']
total_row_N: 65536
total_block_N: 8
block_step: [4 4 2]
block_stride: [2 2 2]
block_dims_N: [2 2 2]
xyz_min: [-0.95031667 -2.15375018 -0.01794876]
xyz_max: [ 2.48232388  0.5618372   2.2046504 ]
xyz_min_aligned: [-1.  -2.2 -0.2]
xyz_max_aligned: [ 2.5  0.6  2.3]
xyz_scope_aligned: [ 3.5  2.8  2.5]
    (4) The label_category in Sorted_H5f is raw_category_idx, the label_category in Normed_H5f is mpcat40 index
    (5) The xyz_1norm_file in h5f is normed with file space scale. Can be used directly in training.
        The xyz_midnorm_block may be not useful if larger scale used in training.
        While using larger block scale in training, xyz_1norm_block and xyz_midnorm_block has to be generated online.
    '''
    IsCheck_sh5f = {}
    IsCheck_sh5f['bid_mapping'] = False and ENABLECHECK
    IsCheck_sh5f['bid_scope'] = False and ENABLECHECK
    IsCheck_sh5f['index_to_ixyz'] = False and ENABLECHECK

    file_flag = 'SORTED_H5F'
    labels_order = ['label_category','label_instance','label_mesh','label_material']
    #label_candi_eles_len = {'label_category':1,'label_instance':1,'label_material':1}
    data_label_ele_candidates_order = ['xyz','nxnynz','color','label','intensity'] + labels_order
    data_label_ele_candidates_order += ['org_row_index']
    data_label_channels = {'xyz':3,'nxnynz':3,'color':3,'label':1,'label_category':1,'label_instance':1,'label_mesh':1,
                           'label_material':1,'intensity':1,'org_row_index':1,'xyz_1norm_file':3,'xyz_midnorm_block':3,
                           'color_1norm':3}
    IS_CHECK = False # when true, store org_row_index
    data_idxs = {}
    total_num_channels = 0

    actions = ''
    h5_num_row_1M = g_h5_num_row_1M

    def __init__(self,h5f,file_name=None):
        self.h5f = h5f
        if file_name != None:
            self.file_name = file_name
        else:
            self.file_name = None
        self.reduced_num = 0
        self.update_data_index_by_elementnames()

    def show_summary_info(self):
        print('\n\nsummary of file: ',self.file_name)
        return show_h5f_summary_info(self.h5f)

    def update_data_index_by_elementnames(self):
        # update by self.h5f.attrs['element_names']
        data_index = {}
        last_index = 0
        if 'element_names' in self.h5f.attrs:
            element_names = self.h5f.attrs['element_names']
        else:
            element_names = []
        if self.IS_CHECK and 'org_row_index' not in element_names:
            element_names += ['org_row_index']

        element_names = set(element_names)
        for dn in self.data_label_ele_candidates_order:
            if dn in element_names:
                data_index[dn] = range(last_index,last_index+self.data_label_channels[dn])
                last_index += self.data_label_channels[dn]
        self.data_idxs = data_index
        self.total_num_channels = last_index

        if 'element_names' in self.h5f.attrs:
            label_set_elements = []
            for e in self.labels_order:
                if e in self.h5f.attrs['element_names']:
                    label_set_elements += [e]
            self.label_set_elements = label_set_elements
            self.label_ele_idxs = self.get_label_ele_ids(label_set_elements)

        if 'datasource_name' in self.h5f.attrs:
            self.DatasetMeta = DatasetMeta(self.h5f.attrs['datasource_name'])
            self.num_classes = self.DatasetMeta.num_classes

    def get_label_ele_ids(self,label_eles):
        label_ele_idxs = {}
        k = 0
        for e in label_eles:
            assert e in self.labels_order
            label_ele_idxs[e] = range(k,k+self.data_label_channels[e])
            k += self.data_label_channels[e]
        return label_ele_idxs
    def get_data_ele_ids(self,data_eles):
        data_ele_idxs = {}
        k = 0
        for e in data_eles:
            assert e in self.data_label_channels, "%s not in self.data_label_channels"%(e)
            data_ele_idxs[e] = range(k,k+self.data_label_channels[e])
            k += self.data_label_channels[e]
        return data_ele_idxs

    def set_step_stride(self,block_step,block_stride,stride_to_align=0.1):
        self.h5f.attrs['block_step'] = block_step
        self.h5f.attrs['block_stride'] = block_stride
        self.h5f.attrs['stride_to_align'] = stride_to_align
        Sorted_H5f.update_align_scope_by_stridetoalign_(self.h5f.attrs)

    @staticmethod
    def set_whole_scene_stride_step(h5fattrs):
        '''
        global_step: global_stride: When gsbb_config_dic['global_step'] <0, the global step and stride is set as whole scene. But the limit is -global_step.
        '''
        if h5fattrs['block_step'][0]  < 0:
            assert h5fattrs['block_step'][1]  < 0 and h5fattrs['block_step'][2]  < 0
            real_xy_area = h5fattrs['xyz_scope_aligned'][0]  *  h5fattrs['xyz_scope_aligned'][1]
            config_xy_area = h5fattrs['block_step'][0] * h5fattrs['block_step'][1]
            if real_xy_area < config_xy_area:
                for i in range(3):
                    h5fattrs['block_step'][i] = h5fattrs['xyz_scope_aligned'][i]
                    h5fattrs['block_stride'][i] = h5fattrs['xyz_scope_aligned'][i]
            else:
                # Try to use two blocks for whole scene. This can avoid small
                # global blocks. But the draback is leading to unalignment
                # between global cascade and the others.
                IsUseTwoGlobalBlocks = True
                gsbb_config_dic = get_gsbb_config()
                last_gsbb_stride = gsbb_config_dic['sub_block_stride_candis'][-1]
                for i in range(3):
                    h5fattrs['block_step'][i] = -h5fattrs['block_step'][i]
                    tmp = h5fattrs['xyz_scope_aligned'][i] - h5fattrs['block_step'][i]
                    tmp_fixed = math.ceil(tmp/last_gsbb_stride)*last_gsbb_stride
                    if IsUseTwoGlobalBlocks and tmp_fixed <=  h5fattrs['block_step'][i] and tmp_fixed > 0:
                        # (1) use two blocks can totally include whole scene
                        # (2) block_stride has to be integral multiple of gsbb_config['sub_block_stride_candis'][-1]
                        h5fattrs['block_stride'][i] =  max( tmp_fixed,0 )
                    else:
                        # when two blocks is not enough, use the fixed stride value
                        h5fattrs['block_stride'][i] = -h5fattrs['block_stride'][i]

    @staticmethod
    def update_align_scope_by_stridetoalign_(h5fattrs):
        if 'xyz_min' not in h5fattrs:
            return
        xyz_min = h5fattrs['xyz_min']
        xyz_max = h5fattrs['xyz_max']
        xyz_min_aligned = xyz_min - xyz_min % h5fattrs['stride_to_align'] - [0,0,0.1]
        xyz_max_aligned = xyz_max - xyz_max % 0.1 + 0.1
        xyz_scope_aligned =  xyz_max_aligned - xyz_min_aligned

        if 'block_step' in h5fattrs:
            block_step = h5fattrs['block_step']
            block_stride = h5fattrs['block_stride']
            h5fattrs['block_step']  = block_step
            h5fattrs['block_stride'] = block_stride
            Sorted_H5f.set_whole_scene_stride_step(h5fattrs)
            h5fattrs['block_dims_N'] = np.ceil( (xyz_scope_aligned - h5fattrs['block_step']) / h5fattrs['block_stride'] + 1 ).astype(np.int64)
            h5fattrs['block_dims_N'] = np.maximum( h5fattrs['block_dims_N'], np.array([1,1,1]) )

        h5fattrs['xyz_min_aligned'] = xyz_min_aligned
        h5fattrs['xyz_max_aligned'] = xyz_max_aligned
        h5fattrs['xyz_scope_aligned'] = xyz_scope_aligned

        for i in range(3):
            if block_step[i] > xyz_scope_aligned[i]:
                block_step[i] = xyz_scope_aligned[i]
            if block_stride[i] > xyz_scope_aligned[i]:
                block_stride[i] = xyz_scope_aligned[i]
        h5fattrs['block_step'] = block_step
        h5fattrs['block_stride'] = block_stride

    def add_total_row_block_N(self,raw_h5f_total_row_N=None):
        total_row_N = 0
        n = -1
        for n,dn in enumerate( self.h5f ):
            total_row_N += self.h5f[dn].shape[0]

        if raw_h5f_total_row_N != None:
            assert total_row_N == raw_h5f_total_row_N, 'ERROR: blocked total_row_N= %d, raw = %d'%(total_row_N,raw_h5f_total_row_N)
        self.h5f.attrs['total_row_N']=total_row_N
        self.h5f.attrs['total_block_N']=n+1
        print('add_total_row_block_N:  file: %s \n   total_row_N = %d,  total_block_N = %d'%(
            os.path.basename(self.file_name),total_row_N,n+1))
        return total_row_N, n+1

    @staticmethod
    def check_sh5_intact( file_name ):
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.sh5' or f_format == '.rsh5'
        if not os.path.exists(file_name):
            return False,"%s not exist"%(file_name)
        #if os.path.getsize( file_name ) / 1000.0 < 100:
        #    return False,"file too small < 20 K"
        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        with h5py.File(file_name,'r') as h5f:
            if 'is_intact' in h5f.attrs:
                IsIntact = h5f.attrs['is_intact'] == 1
                return IsIntact,""
            else:
                attrs_to_check = ['total_row_N','total_block_N','label_category_hist','label_category_hist1norm']
                for attrs in attrs_to_check:
                    if attrs not in h5f.attrs:
                        return False, "%s not in %s"%(attrs,f_format)
                return True,""

    def add_label_histagram(self):
        label_name = 'label_category'
        if label_name in self.label_ele_idxs:
            label_category_id = self.data_idxs['label_category'][0]
            label_hist  = np.zeros(shape=[self.num_classes]).astype(np.int64)
            for k in self.h5f:
                label_hist_k,_ = np.histogram(self.h5f[k][:,label_category_id],range(self.num_classes+1))
                label_hist += label_hist_k.astype(np.int64)
            label_hist_1norm = label_hist / np.sum(label_hist).astype(np.float)
            self.h5f.attrs[label_name+'_hist'] = label_hist
            self.h5f.attrs[label_name+'_hist1norm'] = label_hist_1norm
            print('adding label hist ok:',label_hist)

    def copy_root_summaryinfo_from_another(self,h5f0,copy_flag):
        attrs = ['datasource_name','xyz_max','xyz_min','element_names','stride_to_align']   # 'new_stride'
        if copy_flag == 'sub' or copy_flag == 'sample':
            attrs += ['block_step','block_stride','block_dims_N','total_block_N']
        if copy_flag == 'all':
            attrs = [a for a in h5f0.attrs]

        for attr in attrs:
            if attr in h5f0.attrs:
                self.h5f.attrs[attr] = h5f0.attrs[attr]
        self.h5f.attrs['is_intact'] = 0
        Sorted_H5f.update_align_scope_by_stridetoalign_(self.h5f.attrs)
        self.update_data_index_by_elementnames()

    def copy_root_attrs_from_raw(self,h5f_raw):
        attrs=['datasource_name','element_names','xyz_max','xyz_min']
        for attr in attrs:
            if attr in h5f_raw.attrs:
                self.h5f.attrs[attr] = h5f_raw.attrs[attr]
        self.h5f.attrs['is_intact'] = 0
        self.update_data_index_by_elementnames()


    @staticmethod
    def block_index_to_xyz_(block_k,attrs):
        ixyz = Sorted_H5f.block_index_to_ixyz_( block_k,attrs )
        xyz_center, xyz_bottom, xyz_top = Sorted_H5f.ixyz_to_xyz( ixyz,attrs )
        return xyz_center, xyz_bottom, xyz_top
    def block_index_to_ixyz(self,block_k):
        return Sorted_H5f.block_index_to_ixyz_(block_k,self.h5f.attrs)
    @staticmethod
    def block_index_to_ixyz_(block_k, attrs):
        '''
        block_k: shape=(n,)
        i_xyz: shape=(n,3)
        '''
        block_k = np.array(block_k)
        if block_k.ndim == 0:
            block_k = np.array([block_k])
            input_scalar = True
        else:
            input_scalar = False
        i_xyz = np.zeros( block_k.shape+(3,),np.int64)
        assert 'block_dims_N' in attrs
        block_dims_N = attrs['block_dims_N']
        if Sorted_H5f.IsCheck_sh5f['index_to_ixyz']:
            if block_k.max() >= block_dims_N[0]*block_dims_N[1]*block_dims_N[2]:
                assert False, "input bid %d exceed limit"%(block_k)
        i_xyz[:,2] = block_k % block_dims_N[2]
        k = block_k / block_dims_N[2]
        i_xyz[:,1] = k % block_dims_N[1]
        k =  k / block_dims_N[1]
        i_xyz[:,0] = k % block_dims_N[0]

        if Sorted_H5f.IsCheck_sh5f['index_to_ixyz']:
            bid_tocheck = Sorted_H5f.ixyz_to_block_index_( i_xyz, attrs )
            assert np.sum(bid_tocheck - block_k)==0
        if input_scalar:
            i_xyz = np.squeeze( i_xyz,0 )
        return i_xyz
    @staticmethod
    def old__block_index_to_ixyz_(block_k, attrs):
        assert block_k.size == 1
        i_xyz = np.zeros(3,np.int64)
        assert 'block_dims_N' in attrs
        block_dims_N = attrs['block_dims_N']
        if Sorted_H5f.IsCheck_sh5f['index_to_ixyz']:
            if block_k >= block_dims_N[0]*block_dims_N[1]*block_dims_N[2]:
                assert False, "input bid %d exceed limit"%(block_k)
        i_xyz[2] = block_k % block_dims_N[2]
        k = int( block_k / block_dims_N[2] )
        i_xyz[1] = k % block_dims_N[1]
        k = int( k / block_dims_N[1] )
        i_xyz[0] = k % block_dims_N[0]

        if Sorted_H5f.IsCheck_sh5f['index_to_ixyz']:
            bid_tocheck = Sorted_H5f.ixyz_to_block_index_( i_xyz, attrs )
            assert bid_tocheck == block_k
        return i_xyz
    @staticmethod
    def get_block_dis_(bid0,bid1,attrs):
        ixyz0 = Sorted_H5f.block_index_to_ixyz_(bid0,attrs)
        ixyz1 = Sorted_H5f.block_index_to_ixyz_(bid1,attrs)
        xyz0,_,_ = Sorted_H5f.ixyz_to_xyz( ixyz0,attrs )
        xyz1,_,_ = Sorted_H5f.ixyz_to_xyz( ixyz1,attrs )
        dis = np.linalg.norm(xyz1-xyz0)
        return dis

    @staticmethod
    def ixyz_to_xyz( ixyz, attrs ):
        xyz_bottom =  ixyz * attrs['block_stride'] + attrs['xyz_min_aligned']
        xyz_center = xyz_bottom + attrs['block_step']*0.5
        xyz_top = xyz_bottom + attrs['block_step']
        return xyz_center, xyz_bottom, xyz_top
    @staticmethod
    def xyz_to_ixyz( xyz, attrs ):
        ixyz =  (xyz - attrs['xyz_min_aligned'] - attrs['block_step']*0.5) / attrs['block_stride']
        return ixyz.astype(np.int64)

    def ixyz_to_block_index(self,i_xyz):
        i_xyz = i_xyz.astype(np.uint64)
        block_dims_N = self.h5f.attrs['block_dims_N']
        block_k = int( i_xyz[0]*block_dims_N[1]*block_dims_N[2] + i_xyz[1]*block_dims_N[2] + i_xyz[2] )
        return block_k

    def xyz_to_block_index(self,xyz_k):
        return Sorted_H5f.xyz_to_block_index_( xyz_k, self.h5f.attrs )

    @staticmethod
    def ixyz_to_block_index_(i_xyz,attrs):
        block_dims_N = attrs['block_dims_N']
        assert (i_xyz >= 0).all(),"i_xyz < 0 : %s"%(i_xyz)
        assert (i_xyz < block_dims_N).all(),"i_xyz > block_dims_N %s > %s"%(i_xyz,block_dims_N)
        i_xyz = i_xyz.astype(np.uint64)
        block_k = int( i_xyz[0]*block_dims_N[1]*block_dims_N[2] + i_xyz[1]*block_dims_N[2] + i_xyz[2] )
        return block_k

    @staticmethod
    def xyz_to_block_index_(xyz_k,attrs):
        assert((attrs['block_step'] == attrs['block_stride']).all()),"step != stride,the out k is not unique"
        i_xyz = Sorted_H5f.xyz_to_ixyz( xyz_k, attrs )
        block_k = Sorted_H5f.ixyz_to_block_index_(i_xyz,attrs)
        return block_k, i_xyz

    def get_block_scope_from_k(self,block_k):
        return Sorted_H5f.get_block_scope_from_k_(block_k,self.h5f.attrs)
    @staticmethod
    def get_block_scope_from_k_(block_k,h5fattrs, IsCropByFile=False):
        i_xyz = Sorted_H5f.block_index_to_ixyz_(block_k,h5fattrs)
        return Sorted_H5f.get_block_scope_from_ixyz_(i_xyz,h5fattrs,IsCropByFile)
    @staticmethod
    def get_block_scope_from_ixyz_(i_xyz,h5fattrs, IsCropByFile=False):
        block_dims_N = h5fattrs['block_dims_N']
        block_k = int( i_xyz[0]*block_dims_N[1]*block_dims_N[2] + i_xyz[1]*block_dims_N[2] + i_xyz[2] )
        block_min = i_xyz * h5fattrs['block_stride'] + h5fattrs['xyz_min_aligned']
        block_max = block_min + h5fattrs['block_step']
        if IsCropByFile:
            block_max = np.minimum( block_max, h5fattrs['xyz_max_aligned'] )
        return block_min,block_max,i_xyz

    def get_attrs_of_new_stride_step(self,new_stride,new_step):
        return Sorted_H5f.get_attrs_of_new_stride_step_(self.h5f.attrs,new_stride,new_step)
    @staticmethod
    def get_attrs_of_new_stride_step_(base_h5fattrs,new_stride,new_step):
        new_sorted_h5f_attrs = copy_h5f_attrs( base_h5fattrs )
        new_sorted_h5f_attrs['block_step'] = np.array(new_step).astype(np.float64)
        new_sorted_h5f_attrs['block_stride'] = np.array(new_stride).astype(np.float64)
        if 'total_block_N' in new_sorted_h5f_attrs:
            del new_sorted_h5f_attrs['total_block_N']
        if 'total_row_N' in new_sorted_h5f_attrs:
            del new_sorted_h5f_attrs['total_row_N']
        Sorted_H5f.update_align_scope_by_stridetoalign_(new_sorted_h5f_attrs)
       # print('new_attrs')
        #get_attrs_str(new_sorted_h5f_attrs)
       # print('\n\norg_attrs')
       # get_attrs_str(self.h5f.attrs)
        return new_sorted_h5f_attrs


    @staticmethod
    def get_blockids_of_dif_stride_step(base_bid, base_attrs, aim_attrs, padding, IsCheck_mapping=None, cascade_id=None):
        '''
            base_bid: int, The known base lock id.
            base_attrs: dictionary
            aim_attrs: dictionary

            (1) large_step_flag = 'base'
                Bid with larger step is known (input:base_bid), ask for the smaller block ids in this larger block (return: aim_bid_ls).
        '''
        assert (base_attrs['xyz_min_aligned'] == aim_attrs['xyz_min_aligned']).all()
        assert (base_attrs['xyz_max_aligned'] == aim_attrs['xyz_max_aligned']).all()
        IsMoveForwardLastLargeBaseBlock = True
        # the max padding space if smallb_ixyz_padding_max * small_block_step
        # When smallb_ixyz_padding_max is 0: (1) aim block totally inside the base block (2) aim block totally contain base block
        smallb_ixyz_padding_max = np.array([1.0, 1.0, 1.0]) * padding

        base_bixyz = Sorted_H5f.block_index_to_ixyz_(base_bid, base_attrs)

        # Disable this, so the two grouping methods get the same results.
        #for i in range(3):
        #    if base_bixyz[i] == base_attrs['block_dims_N'][i]-1:
        #        # at the edge block, use 1.0 padding to avoid lost some aim_bids
        #        smallb_ixyz_padding_max[i]  =1

        large_step_flag = ''
        if (base_attrs['block_step'] == aim_attrs['block_step']).all():
            aim_bid_ls = [base_bid]
            aim_bixyz_ls = [base_bixyz]
            return aim_bid_ls, aim_bixyz_ls
        elif (base_attrs['block_step'] > aim_attrs['block_step']).any():
            assert base_attrs['block_step'][0] >= aim_attrs['block_step'][0]
            assert base_attrs['block_step'][1] >= aim_attrs['block_step'][1]
            if not base_attrs['block_step'][2] >= aim_attrs['block_step'][2]:
                import pdb; pdb.set_trace()  # XXX BREAKPOINT
                pass

            large_step_flag = 'base'
            aim_bixyz_threshold_min = ( base_bixyz * base_attrs['block_stride'] ) / aim_attrs['block_stride']
            aim_bixyz_threshold_max = ( base_bixyz * base_attrs['block_stride'] + base_attrs['block_step'] - aim_attrs['block_step']) / aim_attrs['block_stride']
            aim_bixyz_min = my_ceil( aim_bixyz_threshold_min - smallb_ixyz_padding_max )
            aim_bixyz_max = my_fix( aim_bixyz_threshold_max + smallb_ixyz_padding_max )
            if IsMoveForwardLastLargeBaseBlock:
                # In order not to let the last base block be too small. Move forward it to make it full of valid base small blocks.
                for i in range(3):
                    if base_bixyz[i] == base_attrs['block_dims_N'][i]-1:
                        vacant_aim_b_num = int( base_attrs['block_step'][i] / aim_attrs['block_step'][i] ) - ( aim_bixyz_max[i] - aim_bixyz_min[i] +1 )
                        if vacant_aim_b_num > 0:
                            import pdb; pdb.set_trace()  # XXX BREAKPOINT
                            aim_bixyz_min[i] = max(0, aim_bixyz_min[i] - vacant_aim_b_num)
        else:
            assert base_attrs['block_step'][0] <= aim_attrs['block_step'][0]
            assert base_attrs['block_step'][1] <= aim_attrs['block_step'][1]
            assert base_attrs['block_step'][2] <= aim_attrs['block_step'][2]

            large_step_flag = 'aim'
            aim_bixyz_threshold_max = ( (base_bixyz + smallb_ixyz_padding_max) * base_attrs['block_stride'] ) / aim_attrs['block_stride']
            aim_bixyz_threshold_min = ( (base_bixyz - smallb_ixyz_padding_max) * base_attrs['block_stride'] + base_attrs['block_step'] - aim_attrs['block_step']) / aim_attrs['block_stride']
            aim_bixyz_max = my_fix( aim_bixyz_threshold_max )
            aim_bixyz_min = my_ceil( aim_bixyz_threshold_min )
        for i in range(3):
            aim_bixyz_min[i] = max( aim_bixyz_min[i], 0 )
            aim_bixyz_max[i] = min( aim_bixyz_max[i], aim_attrs['block_dims_N'][i]-1 )

        # forcely reduce base block lost
        max_force_rate = 0.3
        IsNoAim =  np.sum( aim_bixyz_max-aim_bixyz_min ) < 0
        IsForceMoved = False
        if large_step_flag=='aim' and IsNoAim:
            for i in range(3):
                if  aim_bixyz_max[i]-aim_bixyz_min [i] < 0:
                    if aim_bixyz_min[i] > 0 and aim_bixyz_threshold_min[i] + 1 - aim_bixyz_min[i] < max_force_rate:
                        aim_bixyz_min[i] -= 1
                        IsForceMoved = True
                    elif aim_bixyz_max[i] < aim_attrs['block_dims_N'][i]-1 and aim_bixyz_threshold_max[i] - aim_bixyz_max[i] < max_force_rate:
                        aim_bixyz_max[i] += 1
                        IsForceMoved = True

        IsCheck_Scope = Sorted_H5f.IsCheck_sh5f['bid_scope']
        if IsCheck_Scope:
            base_xyz_min,base_xyz_max,_ = Sorted_H5f.get_block_scope_from_k_(base_bid, base_attrs, IsCropByFile=True)

        def show_info():
            print('\n%s step larger'%(large_step_flag))
            print('base_bid: ',base_bid, 'ixyz:', base_bixyz)
            print('aim_bid_ls:',aim_bid_ls[0:3], 'aim_bixyz_ls:',aim_bixyz_ls[0:3])
            print('base: ',base_attrs['block_stride'], base_attrs['block_step'])
            print('aim: ',aim_attrs['block_stride'], aim_attrs['block_step'])
            print('lost space: %s'%(lost_space_gap))

        aim_bixyz_ls = []
        aim_bid_ls = []
        for ix in range( aim_bixyz_min[0], aim_bixyz_max[0]+1 ):
            for iy in range( aim_bixyz_min[1], aim_bixyz_max[1]+1 ):
                for iz in range( aim_bixyz_min[2], aim_bixyz_max[2]+1 ):
                    aim_bixyz = np.array([ix,iy,iz]).astype(np.int32)
                    aim_bid = Sorted_H5f.ixyz_to_block_index_( aim_bixyz, aim_attrs )
                    aim_bixyz_ls.append( aim_bixyz )
                    aim_bid_ls.append( aim_bid )

        #check scope
        if IsCheck_Scope and len(aim_bid_ls)>0 and not IsForceMoved:
            ixyz_check = True
            aim_xyz_max = np.array([-1.0e10,-1.0e10,-1.0e10])
            aim_xyz_min = np.array([1.0e10,1.0e10,1.0e10])
            for i in range(len(aim_bid_ls)):
                aim_bixyz_tocheck = Sorted_H5f.block_index_to_ixyz_(aim_bid_ls[i], aim_attrs)
                ixyz_check_i = (aim_bixyz_ls[i] == aim_bixyz_tocheck).all()
                ixyz_check = ixyz_check and ixyz_check_i
                aim_xyz_min_i, aim_xyz_max_i, _ = Sorted_H5f.get_block_scope_from_k_(aim_bid_ls[i], aim_attrs, IsCropByFile=True)
                aim_xyz_min = np.minimum( aim_xyz_min_i,aim_xyz_min )
                aim_xyz_max = np.maximum( aim_xyz_max_i,aim_xyz_max )
            if large_step_flag == 'base':
                min_gap = -(base_xyz_min - aim_xyz_min)
                max_gap = -(aim_xyz_max - base_xyz_max)
                max_step_gap = base_attrs['block_step'] - aim_attrs['block_step'] + np.array([1.0e-10,1.0e-10,1.0e-10])
                small_step = aim_attrs['block_step'] + np.array([1.0e-10,1.0e-10,1.0e-10])
            elif large_step_flag == 'aim':
                min_gap = base_xyz_min - aim_xyz_min
                max_gap = aim_xyz_max - base_xyz_max
                max_step_gap = aim_attrs['block_step'] - base_attrs['block_step'] + np.array([1.0e-10,1.0e-10,1.0e-10])
                small_step = base_attrs['block_step'] + np.array([1.0e-10,1.0e-10,1.0e-10])
            max_padding = smallb_ixyz_padding_max * small_step + 1e-10
            # both gap > 0 => gap > -padding
            # both gap < max_step_gap
            # gap < -padding or  gap > max_step_gap => partial containing or not containing
            lost_space_gap = (min_gap + max_gap)
            min_check = ( min_gap > -max_padding ).all() and ( min_gap < max_step_gap + max_padding ).all()
            max_check = ( max_gap > -max_padding ).all() and ( max_gap < max_step_gap + max_padding ).all()
            if not (min_check and max_check and ixyz_check):
                show_info()
                if not min_check:
                    print('\nmin check failed in get_sub_blcok_ks, min_gap=%s'%(min_gap))
                    import pdb; pdb.set_trace()  # XXX BREAKPOINT
                if not max_check:
                    print('\nmax check failed in get_blockids_of_dif_stride_step, max_gap=%s'%(max_gap))
                    import pdb; pdb.set_trace()  # XXX BREAKPOINT
                if not ixyz_check:
                    print('ixyz check failed, i_xyz_new:',i_xyz_new,'\t i_xyz_new_tocheck:',i_xyz_new_tocheck)
                    import pdb; pdb.set_trace()  # XXX BREAKPOINT
            assert ixyz_check and min_check and max_check
            print( 'base_xyz_min', base_xyz_min )
            print( 'base_xyz_max', base_xyz_max )
            print( 'aim_xyz_min', aim_xyz_min )
            print( 'aim_xyz_max', aim_xyz_max )

        if IsCheck_mapping==None:
            IsCheck_mapping = Sorted_H5f.IsCheck_sh5f['bid_mapping']
        if IsCheck_mapping:
            #print( 'block_k_new_list:',block_k_new_list,'\n' )
            for k, bid_new in enumerate( aim_bid_ls ):
                base_bids_check, base_ixyzs_check = Sorted_H5f.get_blockids_of_dif_stride_step(bid_new, aim_attrs, base_attrs, padding=padding, IsCheck_mapping = False)
                if base_bid not in base_bids_check:
                    show_info()
                    print('mapping err: base_bid %s not in base_bid_check %s  for aim_bid %s'%(base_bid, base_bids_check, bid_new))
                    print('base_ixyz: %s  aim_ixyz: %s  base_ixyzs_check:%s'%( base_bixyz, aim_bid_ls[k], base_ixyzs_check))
                    import pdb; pdb.set_trace()  # XXX BREAKPOINT
                else:
                    #print('mapping OK: base_bid %s in base_bid_check %s  for aim_bid %s'%(base_bid, base_bids_check, bid_new))
                    #print('base_ixyz: %s  aim_ixyz: %s  base_ixyzs_check:%s'%(base_bixyz, aim_bixyz_ls[k], base_ixyzs_check))
                    #print('\n')
                    pass
            #print('all mapping OK: base_bid %s aim_bid %s'%(base_bid, aim_bid_ls))
        return aim_bid_ls, aim_bixyz_ls

    def get_blocked_dset(self,block_k,new_set_default_rows=None,column_N = 9):
        if not type(block_k) is int:
            block_k = int(block_k)

        dset_name = str(block_k)
        if dset_name in self.h5f:
            return self.h5f[dset_name]
        if new_set_default_rows==None:
            new_set_default_rows = self.h5_num_row_1M
        #dset = self.h5f_blocked.create_dataset( dset_name,shape=(new_set_default_rows,n),\
                #maxshape=(None,n),dtype=np.float32,chunks=(self.h5_num_row_1M/5,n) )
        dset = self.h5f.create_dataset( dset_name,shape=(new_set_default_rows,column_N),\
                maxshape=(None,column_N),dtype=np.float32,compression="gzip"  )
        dset.attrs['valid_num']=0
        block_min, block_max,i_xyz = self.get_block_scope_from_k(block_k)
        dset.attrs['i_xyz'] = i_xyz
        dset.attrs['xyz_min'] = block_min
        dset.attrs['xyz_max'] = block_max
        #print('block %s min = %s  max = %s '%(dset_name,block_min,block_max))
        return dset
    def rm_invalid_data(self):
        for dset_name_i in self.h5f:
            dset_i = self.h5f[dset_name_i]
            valid_n = dset_i.attrs['valid_num']
            if dset_i.shape[0] > valid_n:
                #print('resizing block %s from %d to %d'%(dset_name_i,dset_i.shape[0],valid_n))
                dset_i.resize( (valid_n,dset_i.shape[1]) )


    def check_xyz_scope_k(self,block_k):
        '''
        (1) anno-scope == scope_from_k
        (2) xyz data is in scope
        '''
        dset = self.h5f[str(block_k)]
        min_anno = dset.attrs['xyz_min']
        max_anno = dset.attrs['xyz_max']
        min_k,max_k,i_xyz = self.get_block_scope_from_k(block_k)

        e_min = min_anno-min_k
        e_max = max_anno-max_k
        e = np.linalg.norm(e_min) + np.linalg.norm(e_max)
        if e > 1e-5:
            print('block %d scope anno error! '%(block_k),'\nscope_k=\n',[min_k,max_k],'scope_anno=\n',[min_anno,max_anno])
            return False

        xyz = dset[:,0:3]
        xyz_max = xyz.max(axis=0)
        xyz_min = xyz.min(axis=0)
        if (max_k >= xyz_max).all() and (min_k <= xyz_min).all():
            #print('scope checked OK')
            return True
        else:
            if not (min_k <= xyz_min).all():
                print('\nmin check failed')
            if not (max_k >= xyz_max).all():
                print('\nmax check failed')
            print('scope_min=\n',min_k,'\nreal_min=\n',xyz_min)
            print('scope_max=\n',max_k,'\nreal_max=\n',xyz_max)
            print('stride=\n',self.block_stride,'\nstep=\n',self.block_step)
            return False
    def check_xyz_scope(self):
        step = int(self.total_block_N/20)+1
        Flag = True
        n=0
        for i,dset_n in enumerate(self.h5f):
            block_k = int(dset_n)
            if i%step == 0:
                flag = self.check_xyz_scope_k(block_k)
                if not flag:
                    Flag = False
                    print('dset: %s xyz scope check                   failed'%(dset_n))
                else:
                    n += 1
                    pass
                    #print('dset: %s xyz scope check passed'%(dset_n))
        if Flag:
            print('\nall %d dsets  xyz scope check passed\n'%(n))
        return Flag

    def check_equal_to_raw(self,raw_h5f):
        check_flag = True
        for k,block_k in enumerate(self.h5f):
            #print('checing block %s'%(block_k))
            dset_k = self.h5f[block_k]
            step = max(int(dset_k.shape[0]/30),1)
            for i in range(0,dset_k.shape[0],step):
                sorted_d_i = dset_k[i,0:-1]
                raw_k = int(dset_k[i,-1])
                if raw_k < 0 or raw_k > 16777215: # for float32, it is not accurate again
                    continue
                #raw_d_i = np.concatenate(  [raw_xyz_set[raw_k,:],raw_color_set[raw_k,:],raw_label_set[raw_k,:],raw_intensity_set[raw_k,:]] )
                raw_d_i = raw_h5f.get_all_dsets(raw_k,raw_k+1)
                error = raw_d_i - sorted_d_i
                err = np.linalg.norm( error )
                if err != 0:
                    check_flag = False
                    print('\nsorted error:raw_k=%d  block_k=%s,i=%d'%(raw_k,block_k,i))
                    print('raw_data = \n',raw_d_i,'\nsorted_data = \n',sorted_d_i)
                    break
                else:
                    pass
                    #print('equal check passed: block_k=%s,i=%d'%(block_k,i))
#            if flag_k:
#                    print('equal check passed: block_k=%s '%(block_k))
#            else:
#                    print('equal check failed: block_k=%s '%(block_k))
        return check_flag

    def append_to_dset(self,aim_block_k,source_dset,vacant_size=0,IsSample=False,sample_num=None):
        '''
        if append frequently to one dataset, vacant_size > 0 to avoid frequent resize
        '''

        source_N = source_dset.shape[0]
        if IsSample:
            sample_choice,reduced_num = get_sample_choice(source_N,sample_num)
            self.reduced_num += reduced_num
            #sample_choice = np.sort(sample_choice)
            new_row_N = sample_choice.size
        else:
            new_row_N = source_N

        aim_dset = self.get_blocked_dset(aim_block_k,vacant_size,self.total_num_channels)
        assert aim_dset.shape[-1] == source_dset.shape[-1], "The num_channels may be wrong (in append_to_dset)"
        row_step = self.h5_num_row_1M * 10
        org_row_N = aim_dset.attrs['valid_num']
        aim_dset.resize((org_row_N+new_row_N+vacant_size,aim_dset.shape[1]))
        for k in range(0,new_row_N,row_step):
            end = min(k+row_step,new_row_N)
            if IsSample == False:
                dset_buf = source_dset[k:end,:]
                self.raw_category_idx_2_mpcat40(dset_buf)
                aim_dset[org_row_N+k:org_row_N+end,:] =  dset_buf
            else:
                choice_k = sample_choice[k:end]
                dset_buf = source_dset[choice_k.min():choice_k.max()+1,:]
                self.raw_category_idx_2_mpcat40(dset_buf)
                aim_dset[org_row_N+k:org_row_N+end,:] = dset_buf[choice_k-choice_k.min(),:]
            aim_dset.attrs['valid_num'] = end + org_row_N

    def raw_category_idx_2_mpcat40(self,data_labels_with_rawcategory):
        if self.h5f.attrs['datasource_name']=='MATTERPORT' and 'label_category' in self.data_idxs:
            assert data_labels_with_rawcategory.ndim == 2
            raw_category_idx = self.data_idxs['label_category'][0]
            data_labels_with_rawcategory[:,raw_category_idx] = get_cat40_from_rawcat(data_labels_with_rawcategory[:,raw_category_idx])

    def generate_one_block_to_object(self,block_k,out_obj_file,IsLabelColor=False):
        row_step = self.h5_num_row_1M * 10
        dset_k = self.get_blocked_dset(block_k)
        row_N = dset_k.shape[0]
        for k in range(0,row_N,row_step):
            end = min(k+row_step,row_N)
            buf_k = dset_k[k:end,:]
            #buf_k[:,0:3] -= middle
            for j in range(0,buf_k.shape[0]):
                if not IsLabelColor:
                    str_j = 'v ' + ' '.join( ['%0.3f'%(d) for d in  buf_k[j,0:3]]) + ' \t'\
                    + ' '.join( ['%d'%(d) for d in  buf_k[j,3:6]]) + '\n'
                else:
                    label = buf_k[j,self.data_idxs['label'][0]]
                  #  if label == 0:
                  #      continue
                    label_color = Normed_H5f.g_label2color_dic[self.h5f.attrs['datasource_name']][label]
                    str_j = 'v ' + ' '.join( ['%0.3f'%(d) for d in  buf_k[j,0:3]]) + ' \t'\
                    + ' '.join( ['%d'%(d) for d in  label_color ]) + '\n'

                out_obj_file.write(str_j)

    def gen_file_obj(self,IsLabelColor=False):
        if self.file_name == None:
            print('set file_name (gen_file_obj)')
            return
        base_fn = os.path.basename(self.file_name)
        base_fn = os.path.splitext(base_fn)[0]
        folder_path = os.path.dirname(self.file_name)
        obj_folder = os.path.join(folder_path,base_fn)
        print('obj path:',obj_folder)
        if not os.path.exists(obj_folder):
            os.makedirs(obj_folder)

        aim_scope = np.array([[-30,-30,-20],[20,20,50]])
        aim_scope = None
        n = 0
        last_rate = -20
        out_info_fn = os.path.join(obj_folder,'info.txt')
        with open(out_info_fn,'w') as info_f:
            for dset_name in self.h5f:
                row_N = self.h5f[dset_name].shape[0]

                min_i = self.h5f[dset_name].attrs['xyz_min']
                max_i = self.h5f[dset_name].attrs['xyz_max']
                if aim_scope == None:
                    IsInScope = True
                else:
                    IsInScope = (min_i > aim_scope[0,:]).all() and ( max_i < aim_scope[1,:]).all()
                if not IsInScope:
                    continue
                if IsLabelColor:
                    name_meta = 'labeled_'
                else:
                    name_meta = ''
                out_fn = os.path.join(obj_folder,name_meta+dset_name+'_'+str(row_N)+'.obj')
                with open(out_fn,'w') as out_f:
                    self.generate_one_block_to_object(dset_name,out_f,IsLabelColor)
                n += row_N
                rate = 100.0 * n / self.h5f.attrs['total_row_N']
                if int(rate) % 2 == 0 and rate - last_rate > 3:
                    last_rate = rate
                    print('%0.2f%% generating file: %s'%(rate,os.path.basename(out_fn)) )

                info_str = 'dset: %s \tN= %d   \tmin=%s   \tmax=%s \n'%(dset_name,self.h5f[dset_name].shape[0], np.array_str(min_i), np.array_str(max_i)  )
                info_f.write(info_str)
                #print(info_str)
                #if rate > 30:
                    #break
    def extract_sub_area(self,sub_xyz_scope,sub_file_name):
        with h5py.File(sub_file_name,'w') as sub_h5f:
            sub_f = Sorted_H5f(sub_h5f,sub_file_name)

            sub_f.copy_root_summaryinfo_from_another(self.h5f,'sub')
            sub_f.set_step_stride(self.block_step,self.block_stride)
            for dset_name_i in self.h5f:
                xyz_min_i = self.h5f[dset_name_i].attrs['xyz_min']
                xyz_max_i = self.h5f[dset_name_i].attrs['xyz_max']
                if (xyz_min_i > sub_xyz_scope[0,:]).all() and (xyz_max_i < sub_xyz_scope[1,:]).all():
                    sub_f.get_blocked_dset(dset_name_i,0)
                    sub_f.append_to_dset(dset_name_i,self.h5f[dset_name_i])
            sub_f.add_total_row_block_N()


    def file_random_sampling(self,sample_num,gen_norm=False,gen_obj=False,min_keep_rate=0.01):
        '''
        automatically create a folder in uper directory to store sampled files
        '''
        # randomly select n points
        out_folder = os.path.dirname(self.file_name)+'_'+str(sample_num)
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)
        file_name_base = os.path.splitext(os.path.basename(self.file_name))[0]
        sampled_filename = os.path.join(out_folder,file_name_base+'.rsh5')

        print('start genrating sampled file: ',sampled_filename)
        ave_dset_num = self.h5f.attrs['total_row_N'] /  self.h5f.attrs['total_block_N']
        print('ave_org_num = ',ave_dset_num)
        print('sample_num = %d   %d%%'%(sample_num,100.0*sample_num/ave_dset_num) )
        with h5py.File(sampled_filename,'w') as sampled_h5f:
            sampled_sh5f = Sorted_H5f(sampled_h5f,sampled_filename)
            sampled_sh5f.copy_root_summaryinfo_from_another(self.h5f,'sample')
            #sampled_sh5f.set_root_attr('sample_num',sample_num)
            for i, k_str in enumerate( self.h5f ):
                dset_k = self.h5f[k_str]
                if dset_k.shape[0] < sample_num*min_keep_rate:
                    continue
                sampled_sh5f.append_to_dset(int(k_str),dset_k,vacant_size=0,\
                                            IsSample=True,sample_num=sample_num)
            sampled_sh5f.add_total_row_block_N()
            print('reduced_num = %d  %d%%'%(sampled_sh5f.reduced_num,100.0*sampled_sh5f.reduced_num/self.h5f.attrs['total_row_N'] ))
            reduced_block_N = self.h5f.attrs['total_block_N'] - sampled_sh5f.h5f.attrs['total_block_N']
            print('reduced block num = %d  %d%%'%(reduced_block_N,100*reduced_block_N/self.h5f.attrs['total_block_N']))

            if gen_obj:
               sampled_sh5f.gen_file_obj()
            if gen_norm:
                sampled_sh5f.file_normalize_to_NormedH5F()


    def get_sample_shape(self):
            for i,k_str in  enumerate(self.h5f):
                dset = self.h5f[k_str]
                return dset.shape

    @staticmethod
    def norm_xyz(raw_xyz,h5fattrs,block_id,norm_list,out_norm_data_dic):
        block_min,block_max,_ = Sorted_H5f.get_block_scope_from_k_(block_id,h5fattrs)
        if norm_list==None or 'xyz_1norm_file' in norm_list:
            # used by QI
            # 1norm within the whole scene
            # use by QI in indoor. Since room scale is not large, this is fine.
            # For outdoor,a scene could be too large, maybe not a good choice
            IsUseAligned = True
            if IsUseAligned:
                file_scene_zero = h5fattrs['xyz_min_aligned']
                file_scene_scope = h5fattrs['xyz_max_aligned'] - h5fattrs['xyz_min_aligned']
            else:
                file_scene_zero = h5fattrs['xyz_min']
                file_scene_scope = h5fattrs['xyz_max'] - h5fattrs['xyz_min']
            xyz_1norm_file = (raw_xyz - file_scene_zero) / file_scene_scope
            out_norm_data_dic['xyz_1norm_file'] = xyz_1norm_file
        if norm_list==None or 'xyz_1norm_block' in norm_list:
            # 1norm within the block
            block_scope = block_max - block_min
            xyz_1norm_block = (raw_xyz-block_min) / block_scope
            out_norm_data_dic['xyz_1norm_block'] = xyz_1norm_block

        # xyz_midnorm
        if norm_list==None or 'xyz_midnorm_block' in norm_list:
            xyz_midnorm_block = np.copy( raw_xyz ) # as a new variable, not a reference
            # only norm x,y. Keep z be the raw value
            #xyz_min_real = np.min(raw_xyz,axis=0)
            #xyz_midnorm_block[:,0:2] -= (xyz_min_real[0:2] + self.block_step[0:2]/2)  # used by QI
            block_mid = (block_min + block_max ) / 2
            xyz_midnorm_block[:,0:2] -= block_mid[0:2]  # I think is better
            # for z, just be positive
            xyz_midnorm_block[:,2] -= h5fattrs['xyz_min'][2]
            out_norm_data_dic['xyz_midnorm_block'] = xyz_midnorm_block

    def normalize_dset(self,block_k_str,xyz_1norm_scale='file'):
        '''
        (1) xyz/max
        (2) xy-min-block_size/2  (only xy)
        (3) color / 255
        '''
        raw_dset_k = self.h5f[block_k_str]

        norm_data_dic = {}
        raw_xyz = raw_dset_k[:,self.data_idxs['xyz']]
        norm_data_dic['xyz'] = raw_xyz
        norm_data_dic['nxnynz'] = raw_dset_k[:,self.data_idxs['nxnynz']]
      #  #  xyz_1norm
      #  if xyz_1norm_scale == 'file': # used by QI
      #      # 1norm within the whole scene
      #      # use by QI in indoor. Since room scale is not large, this is fine.
      #      # For outdoor,a scene could be too large, maybe not a good choice
      #      IsUseAligned = True
      #      if IsUseAligned:
      #          file_scene_zero = self.h5f.attrs['xyz_min_aligned']
      #          file_scene_scope = self.h5f.attrs['xyz_max_aligned'] - self.h5f.attrs['xyz_min_aligned']
      #      else:
      #          file_scene_zero = self.h5f.attrs['xyz_min']
      #          file_scene_scope = self.h5f.attrs['xyz_max'] - self.h5f.attrs['xyz_min']
      #      xyz_1norm = (raw_xyz - file_scene_zero) / file_scene_scope
      #  elif 'block' in xyz_1norm_scale:
      #      # 1norm within the block
      #      block_scope = raw_dset_k.attrs['xyz_max'] - raw_dset_k.attrs['xyz_min']
      #      xyz_1norm = (raw_xyz-raw_dset_k.attrs['xyz_min']) / block_scope

      #  # xyz_midnorm
      #  xyz_midnorm = raw_xyz+0 # as a new variable, not a reference
      #  # only norm x,y. Keep z be the raw value
      #  #xyz_min_real = np.min(raw_xyz,axis=0)
      #  #xyz_midnorm[:,0:2] -= (xyz_min_real[0:2] + self.block_step[0:2]/2)  # used by QI
      #  block_mid = (raw_dset_k.attrs['xyz_min'] + raw_dset_k.attrs['xyz_max'] ) / 2
      #  xyz_midnorm[:,0:2] -= block_mid[0:2]  # I think is better
      #  # for z, just be positive
      #  xyz_midnorm[:,2] -= self.h5f.attrs['xyz_min'][2]

        xyz_norm_list = ['xyz_1norm_file','xyz_midnorm_block']
        Sorted_H5f.norm_xyz(raw_xyz,self.h5f.attrs,block_k_str,xyz_norm_list,norm_data_dic)
      #  norm_data_dic['xyz_midnorm_block'] = xyz_midnorm
      #  norm_data_dic['xyz_1norm_'+xyz_1norm_scale] = xyz_1norm

        # color_1norm
        if 'color' in self.data_idxs:
            color_1norm = raw_dset_k[:,self.data_idxs['color']] / 255.0
            norm_data_dic['color_1norm']=color_1norm


        # intensity_1norm
        if 'intensity' in self.data_idxs:
            # ETH senmantic3D intensity range from -2047 to 2048
            intensity = raw_dset_k[:,self.data_idxs['intensity']]
            intensity_1norm = (intensity+2047)/(2048+2047)
            norm_data_dic['intensity_1norm']=intensity_1norm

        return norm_data_dic

    def add_normalize_to_self(self,IsShowSummaryFinished=False):
        xyz_1norm_scale = 'file'
        IsAddColor1Norm = ('color' in self.h5f) and ('color_1norm' not in self.h5f)
        IsAddXyzNorm = ('xyz' in self.h5f) and ('xyz_1norm_file' not in self.h5f)
        if IsAddColor1Norm:
            self.h5f.attrs['element_names'] += ['color1norm']
        for i,k_str in  enumerate(self.h5f):
            dset_i = self.h5f[k_str]
            if IsAddColor1Norm:
                color = dset_i[...,self.data_idxs['color']]
                color1norm = color / 255.0



            norm_data_dic = self.normalize_dset(k_str,xyz_1norm_scale)

            norm_data_list = []
            for data_name in Normed_H5f.normed_ele_idx_order:
                if data_name in norm_data_dic:
                    norm_data_list.append(norm_data_dic[data_name])
            data_norm = np.concatenate( norm_data_list,1 )

            label_eles = [lb for lb in Normed_H5f.labels_order if lb in self.h5f.attrs['element_names']]
            labels = []
            for label_e_name in label_eles:
                label_e_d = raw_dset_k[:,self.data_idxs[label_e_name][0]]
                labels.append(np.expand_dims(label_e_d,axis=-1))
            labels = np.concatenate(labels,axis=-1)


            normed_h5f.append_to_dset('data',normed_data_i)
            normed_h5f.append_to_dset('labels',normed_labels_i)
            #normed_h5f.append_to_dset('blockid',int(k_str))
            normed_h5f.sph5_create_done()
            if IsShowSummaryFinished:
                normed_h5f.show_summary_info()
            print('normalization finished: data shape: %s'%(str(normed_h5f.data_set.shape)) )

    def file_normalize_to_NormedH5F(self,IsShowSummaryFinished=False):
        '''
        automatically create a folder in uper directory to store sampled files
        '''
        xyz_1norm_scale = 'file'

        out_folder = os.path.dirname(self.file_name)+'_normed'
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)
        file_name_base = os.path.splitext(os.path.basename(self.file_name))[0]
        normalized_filename = os.path.join(out_folder,file_name_base+'.sph5')

        print('start gen normalized file: ',normalized_filename)
        with h5py.File(normalized_filename,'w') as h5f:
            normed_h5f = Normed_H5f(h5f,normalized_filename,self.h5f.attrs['datasource_name'])
            for i,k_str in  enumerate(self.h5f):
                dset = self.h5f[k_str]
                h5f_sorted_shape = dset.shape
                break
            sample_num =  (h5f_sorted_shape[0],)
            normed_h5f.copy_root_attrs_from_sorted(self.h5f.attrs,sample_num,self.IS_CHECK)

            for i,k_str in  enumerate(self.h5f):
                norm_data_dic = self.normalize_dset(k_str,xyz_1norm_scale)

                norm_data_list = []
                for data_name in Normed_H5f.normed_ele_idx_order:
                    if data_name in norm_data_dic:
                        norm_data_list.append(norm_data_dic[data_name])
                normed_data_i = np.concatenate( norm_data_list,1 )

                label_eles = [lb for lb in Normed_H5f.labels_order if lb in self.h5f.attrs['element_names']]
                labels = []
                for label_e_name in label_eles:
                    label_e_d = raw_dset_k[:,self.data_idxs[label_e_name][0]]
                    labels.append(np.expand_dims(label_e_d,axis=-1))
                labels_i = np.concatenate(labels,axis=-1)


                normed_h5f.append_to_dset('data',normed_data_i)
                normed_h5f.append_to_dset('labels',labels_i,IsLabelWithRawCategory=False)
                #normed_h5f.append_to_dset('blockid',int(k_str))
            normed_h5f.sph5_create_done()
            if IsShowSummaryFinished:
                normed_h5f.show_summary_info()
            print('normalization finished: data shape: %s'%(str(normed_h5f.data_set.shape)) )

    def merge_to_new_step(self,larger_stride,larger_step,out_folder,more_actions_config=None):
        '''
        merge blocks of sorted raw h5f to get new larger step / stride
        '''
        if not os.path.exists(out_folder):
            os.makedirs(out_folder)
        new_name = os.path.join(out_folder,os.path.basename(self.file_name))
        print('new file: ',new_name)
        #if os.path.exists(new_name):
        #    print('already exists, skip')
        #    return
        with  h5py.File(self.file_name,'r') as base_h5f:
            with h5py.File(new_name,'w') as new_h5f:
                new_sh5f = Sorted_H5f(new_h5f,new_name)
                new_sh5f.copy_root_summaryinfo_from_another(base_h5f,'new_stride')
                new_sh5f.set_step_stride(larger_step,larger_stride)

                read_row_N = 0
                rate_last = -10
                print('%d rows and %d blocks to merge'%(self.h5f.attrs['total_row_N'],self.h5f.attrs['total_block_N']))
                for dset_name in  base_h5f:
                    block_i_base = int(dset_name)
                    base_dset_i = base_h5f[dset_name]
                    block_k_new_ls,i_xyz_new_ls = Sorted_H5f.get_blockids_of_dif_stride_step(block_i_base,self.h5f.attrs,new_sh5f.h5f.attrs)

                    read_row_N += base_dset_i.shape[0]
                    rate = 100.0 * read_row_N / self.h5f.attrs['total_row_N']
                    if int(rate)%10 < 1 and rate-rate_last>5:
                        rate_last = rate
                        print(str(rate),'%   ','  dset_name = ',dset_name, '  new_k= ',block_k_new_ls,'   id= ',os.getpid())
                        new_sh5f.h5f.flush()

                    for block_k_new in block_k_new_ls:
                        new_sh5f.append_to_dset(block_k_new,base_dset_i)
                    #if rate > 5:
                        #break
                if read_row_N != self.h5f.attrs['total_row_N']:
                    print('ERROR!!!  total_row_N = %d, but only read %d'%( self.h5f.attrs['total_row_N'],read_row_N))

                total_block_N = 0
                total_row_N = 0
                for total_block_N,dn in enumerate(new_sh5f.h5f):
                    total_row_N += new_sh5f.h5f[dn].shape[0]
                total_block_N += 1
                new_sh5f.h5f.attrs['total_row_N']=total_row_N
                new_sh5f.h5f.attrs['total_block_N']=total_block_N
                new_sh5f.h5f.attrs['is_intact'] = 1
                print('total_row_N = ',total_row_N)
                print('total_block_N = ',total_block_N)
                new_sh5f.h5f.flush()

                #new_sh5f.check_xyz_scope()

                if more_actions_config != None:
                    actions = more_actions_config['actions']
                    if 'obj_merged' in actions:
                        new_sh5f.gen_file_obj(True)
                        new_sh5f.gen_file_obj(False)
                    if 'sample_merged' in actions:
                        Is_gen_obj = 'obj_sampled_merged' in actions
                        Is_gen_norm = 'norm_sampled_merged' in actions
                        new_sh5f.file_random_sampling(more_actions_config['sample_num'],\
                                            gen_norm=Is_gen_norm,gen_obj = Is_gen_obj)


    #***************************************************************************
    #Net feed utils: extract data from unsampled sorted dataset
    #***************************************************************************
    def get_block_data_of_new_stride_step_byxyz1norm( self,xyz1norm_k, new_stride,new_step,
                                          feed_data_elements=['xyz_midnorm'],feed_label_elements=['label_category'], sample_num=None ):
        xyz_k = np.array(xyz1norm_k) * self.h5f.attrs['xyz_scope_aligned'] + self.h5f.attrs['xyz_min_aligned']
        return self.get_block_data_of_new_stride_step_byxyz(xyz_k,new_stride,new_step,feed_data_elements,feed_label_elements,sample_num)
    def get_block_data_of_new_stride_step_byxyz( self,xyz_k, new_stride,new_step,
                                          feed_data_elements=['xyz_midnorm'],feed_label_elements=['label_category'] ):
        new_sorted_h5f_attrs = self.get_attrs_of_new_stride_step(new_stride,new_step)
        new_block_id,new_ixyz = Sorted_H5f.xyz_to_block_index_(xyz_k,new_sorted_h5f_attrs)
        return self.get_block_data_of_new_stride_step_byid(new_block_id,new_sorted_h5f_attrs,feed_data_elements,feed_label_elements)

    def get_numpoint_of_new_stride_step_byid( self,new_block_id, new_sorted_h5f_attrs,gsbb):
        root_bids_in_cas0 = gsbb.get_basebids_ina_aim(0,new_block_id)
        num_point_all = 0
        for cur_block_id in root_bids_in_cas0:
            num_point_all += self.h5f[str(cur_block_id)].shape[0]
        return num_point_all


    def remove_void(self,feed_data,feed_label):
        if self.h5f.attrs['datasource_name'] == 'MATTERPORT':
            NoneVoidIndices, = np.nonzero( feed_label[ :,self.label_ele_idxs['label_category'][0] ] )
            void_num = feed_data.shape[0] - NoneVoidIndices.size
            feed_data = feed_data[ NoneVoidIndices,: ]
            feed_label = feed_label[ NoneVoidIndices,: ]
            #if void_num>0:
            #    print('void num = %d'%void_num)
        return feed_data, feed_label

    def get_block_data_of_new_stride_step_byid( self,root_bids_in_cas0, feed_data_elements, feed_label_elements, new_block_id=None, new_sorted_h5f_attrss=None ):
        # feed data and label ele orders are stored according to feed_data_elements and feed_label_elements
        datas = []
        labels = []
        #root_bids_in_cas0 = gsbb.get_basebids_ina_aim(0,new_block_id)
        for cur_block_id in root_bids_in_cas0:
           # if str(cur_block_id) not in self.h5f:
           #     continue
            dset = self.h5f[str(cur_block_id)]
            feed_label_ids_inall = []
            for le in feed_label_elements:
                feed_label_ids_inall += self.data_idxs[le]
            dset_data = dset[...]
            raw_xyz = dset_data[...,self.data_idxs['xyz']]
            feed_label = dset_data[:,feed_label_ids_inall].astype(np.int32)

            norm_data_dic = {}
            if 'xyz' in feed_data_elements:
                norm_data_dic['xyz'] = raw_xyz
            if 'nxnynz' in feed_data_elements:
                norm_data_dic['nxnynz'] = dset_data[...,self.data_idxs['nxnynz']]
            # cal normalizaed data
            if 'xyz_1norm_file' in feed_data_elements or 'xyz_midnorm_block' in feed_data_elements:
                assert new_block_id!= None
                Sorted_H5f.norm_xyz(raw_xyz, new_sorted_h5f_attrs, new_block_id, feed_data_elements, norm_data_dic)
            if 'color_1norm' in feed_data_elements:
                color = dset[...,self.data_idxs['color']]
                color_1norm = color/255.0
                norm_data_dic['color_1norm'] = color_1norm

            feed_data = np.concatenate( [norm_data_dic[de] for de in feed_data_elements],axis=-1 )
            feed_data, feed_label = self.remove_void( feed_data, feed_label )

            datas.append(feed_data)
            labels.append(feed_label)
        datas = np.concatenate(datas,axis=0)
        labels = np.concatenate(labels,axis=0)

        return datas, labels

    def get_data_larger_block( self,global_block_id,gsbb,feed_data_elements,feed_label_elements, global_num_point, max_rootb_num ):
        '''
        1) global block is the learning block unit. Use current stride and step as base block units.
        2) ( corresponding to farest distance sampling ) Within each global block, select npoint sub-points. Each sub-point is the center of a sub-block. The sub-block stride and step is manually  set to ensure all valid space is used.
        Use 0.1 stride and 0.1 step block as base blocks. All the base block centers are candidate sub-points. Randomly select nsubblock points from all candidate sub-points.
        3) Get sub-group data for each sub-point.

        * Return:
            sampled_xyzs: [nsubblock,xyz] the sampled points in global block
            global_block_datas: [ nsubblock,npoint_subblock,data_nchannel ]
            global_block_labels: [ nsubblock,npoint_subblock,label_nchannel ]

        * Check + Problem:
            If nsubblock and sub_block_size are reasonable, to ensure all valid space is utilized, and no base block id is missed.
        '''
        h5f = self.h5f
        # (1) SAMPLE: Use all the center of base blocks as candidate sub-points
        base_cascadeid = gsbb.base_cascade_ids['global']
        assert base_cascadeid == 'root'
        root_bids_in_global = gsbb.get_basebids_ina_aim('global',global_block_id)

        # (2) GROUP: Collect all the base blocks for each sub-point.
        global_block_datas = []
        global_block_labels = []
        rootb_split_idxmap = []
        sum_global_point_num = 0
        for root_bid_index,root_bid in enumerate( root_bids_in_global ):
            datas_k, labels_k = self.get_block_data_of_new_stride_step_byid( [root_bid], feed_data_elements, feed_label_elements )
            num_point_k = datas_k.shape[0]
            if num_point_k !=0:
                global_block_datas.append( datas_k )
                global_block_labels.append( labels_k )
                sum_global_point_num += num_point_k
                rootb_split_idxmap.append( np.expand_dims( np.array([root_bid, sum_global_point_num]),0 ) )

        if len( global_block_datas )==0:
            # all void points
            return np.array([]),None,None,None,None

        global_block_datas = np.concatenate(global_block_datas,axis=0).astype( np.float32 )
        global_block_labels = np.concatenate(global_block_labels,axis=0).astype( np.int32 )
        rootb_split_idxmap = np.concatenate(rootb_split_idxmap,axis=0)

        global_sample_rate = 1.0 * global_num_point /  global_block_datas.shape[0]
        global_block_datas, global_block_labels, rootb_split_idxmap, global_sampling_meta = Sorted_H5f.down_sample_global_block( global_block_datas, global_block_labels, rootb_split_idxmap, global_num_point )
        # fix root b num
        rootb_split_idxmap_fixed = Sorted_H5f.fix_rootb_split_idxmap( rootb_split_idxmap )

        return global_block_datas, global_block_labels, rootb_split_idxmap_fixed, global_sampling_meta, global_sample_rate

    @staticmethod
    def fix_rootb_split_idxmap( rootb_split_idxmap ):
        assert  Normed_H5f.max_rootb_num  >= rootb_split_idxmap.shape[0]
        rootb_split_idxmap_fixed = (np.ones( shape=(Normed_H5f.max_rootb_num,2) ) * (-1)).astype(np.int32)
        rootb_split_idxmap_fixed[0:rootb_split_idxmap.shape[0],:] = rootb_split_idxmap
        return rootb_split_idxmap_fixed

    @staticmethod
    def down_sample_global_block( data, label,  bsplit_idxmap, sample_num ):
        org_num = bsplit_idxmap[-1,1]
        assert org_num == data.shape[0]
        sampling_meta = {}
        if org_num <= sample_num:
            data_tile = np.tile( data[org_num-1:org_num,:],[sample_num-org_num,1] )
            data = np.concatenate( [data,data_tile], 0 )
            label_tile = np.tile( label[org_num-1:org_num,:],[sample_num-org_num,1] )
            label = np.concatenate( [label,label_tile], 0 )
            #bsplit_idxmap[-1,1] = sample_num
            bsplit_idxmap_add = np.array([ [ bsplit_idxmap[-1,0], sample_num ]] )
            bsplit_idxmap = np.concatenate( [bsplit_idxmap, bsplit_idxmap_add], 0 )
            sampling_meta['missed_rootb_num'] = 0
        else:
            del_choice = np.sort( random_choice(  np.arange(org_num), org_num - sample_num, keeporder=True ) )
            data = np.delete( data, del_choice, axis=0 )
            label = np.delete( label, del_choice, axis=0 )

            bsplit_idxmap, missed_rootb_num = Sorted_H5f.down_sample_bsplit_idxmap( bsplit_idxmap, del_choice, sample_num )

            sampling_meta['missed_rootb_num'] = missed_rootb_num
        sampling_meta['missed_point_num'] = org_num - sample_num

        return data, label, bsplit_idxmap, sampling_meta

    @staticmethod
    def down_sample_bsplit_idxmap( raw_bsplit_idxmap, del_choice, sample_num ):
            bsplit_idxmap = np.copy(raw_bsplit_idxmap)
            if bsplit_idxmap[-1,0] == -1:
                for valid_n in range(bsplit_idxmap.shape[0]):
                    if bsplit_idxmap[valid_n,0] == -1:
                        bsplit_idxmap = bsplit_idxmap[0:valid_n,:]
                        break

            del_num_eachb = np.zeros( shape=(bsplit_idxmap.shape[0]) )
            del_bidx = 0
            for del_point_idx in del_choice:
                while del_point_idx >= bsplit_idxmap[del_bidx,1]:
                    del_bidx += 1
                del_num_eachb[del_bidx] += 1
            sum_del_point_num = 0
            for bidx in range( bsplit_idxmap.shape[0] ):
                sum_del_point_num += del_num_eachb[bidx]
                bsplit_idxmap[bidx,1] -= sum_del_point_num
            # check if points in some blocks are all delted
            empty_bidxs = []
            last_point_idx = 0
            for bidx in range( bsplit_idxmap.shape[0] ):
                if bsplit_idxmap[bidx][1] == last_point_idx:
                    empty_bidxs.append( bidx )
                last_point_idx = bsplit_idxmap[bidx][1]
            empty_bidxs = np.array( empty_bidxs )
            bsplit_idxmap = np.delete( bsplit_idxmap, empty_bidxs, 0 )
            if not bsplit_idxmap[-1,1] == sample_num:
                import pdb; pdb.set_trace()  # XXX BREAKPOINT
                assert False, '%d != %d'%(bsplit_idxmap[-1,1], sample_num)
            missed_rootb_num = empty_bidxs.size
            if raw_bsplit_idxmap.shape[0] == Normed_H5f.max_rootb_num:
                bsplit_idxmap = Sorted_H5f.fix_rootb_split_idxmap( bsplit_idxmap )
            return bsplit_idxmap, missed_rootb_num

    def get_batch_of_larger_block( self,global_blockid_start,global_blockid_end,feed_data_elements,feed_label_elements ):
        gsbb = GlobalSubBaseBLOCK(self.h5f,self.file_name)
        all_sorted_global_bids = gsbb.get_all_sorted_aimbids('global')
        assert 0<= global_blockid_start  <=all_sorted_global_bids.shape[0]
        assert 0<= global_blockid_end <= all_sorted_global_bids.shape[0]
        all_sorted_global_bids = all_sorted_global_bids[global_blockid_start:global_blockid_end]
        batch_datas = []
        batch_labels = []
        for global_block_id in all_sorted_global_bids:
            block_datas,block_labels,_,_ = self.get_data_larger_block( global_block_id,gsbb,feed_data_elements,feed_label_elements )
            batch_datas.append(np.expand_dims(block_datas,axis=0))
            batch_labels.append(np.expand_dims(block_labels,axis=0))
        batch_datas = np.concatenate(batch_datas,axis=0)
        batch_labels = np.concatenate(batch_labels,axis=0)

       # print(batch_datas.shape)
       # print(batch_labels.shape)

        return batch_datas, batch_labels

    @staticmethod
    def check_sgfh5_intact( file_name ):
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.sgfh5'
        if not os.path.exists(file_name):
            return False, "%s not exist"%(file_name)
        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        with h5py.File(file_name,'r') as h5f:
            if 'is_intact_sgfh5' not in h5f.attrs:
                return False,"no is_intact_sgfh5 attr"
            IsIntact = h5f.attrs['is_intact_sgfh5'] == 1
            return IsIntact,"is_intact_sgfh5=1"

    def file_saveas_pyramid_feed(self,IsShowSummaryFinished=False,Always_CreateNew_plh5=False,Always_CreateNew_bmh5=False,Always_CreateNew_bxmh5=False, IsGenPly=False):
        '''
        save by global block
        '''
        t0 = time.time()
        datasource_name = self.h5f.attrs['datasource_name']
        gsbb_write = GlobalSubBaseBLOCK( root_s_h5f = self.h5f, root_s_h5f_fn = self.file_name )

        if datasource_name == 'MATTERPORT':
            region_name = os.path.splitext( os.path.basename(self.file_name) )[0]
            house_dir_name = os.path.dirname(self.file_name)
            house_name = os.path.basename(house_dir_name)
            rootsort_dirname = os.path.dirname(house_dir_name)

            out_folder_sph5 = rootsort_dirname + '/ORG_sph5/' + gsbb_write.get_pyramid_flag( 'sph5' ) + '/' + house_name
            out_folder_bxmh5 = rootsort_dirname + '/ORG_bxmh5/' + gsbb_write.get_pyramid_flag( 'bxmh5' ) + '/' + house_name
            pl_sph5_filename = os.path.join(out_folder_sph5,region_name+'.sph5')
        elif datasource_name == 'SCANNET':
            scene_name  =  region_name = os.path.splitext( os.path.basename(self.file_name) )[0]
            scannet_h5f_dir = os.path.dirname( os.path.dirname( os.path.dirname(self.file_name) ))
            out_folder_sph5 =  scannet_h5f_dir + '/ORG_sph5/' + gsbb_write.get_pyramid_flag( 'sph5' )
            out_folder_bxmh5 =  scannet_h5f_dir + '/ORG_bxmh5/' + gsbb_write.get_pyramid_flag( 'bxmh5'  )

        elif datasource_name == 'KITTI':               ## benz_m
            scene_name  =  region_name = os.path.splitext( os.path.basename(self.file_name) )[0]
            scannet_h5f_dir = os.path.dirname(( os.path.dirname(self.file_name) ))
            out_folder_sph5 =  scannet_h5f_dir + '/ORG_sph5/' + gsbb_write.get_pyramid_flag( 'sph5' )
            out_folder_bxmh5 =  scannet_h5f_dir + '/ORG_bxmh5/' + gsbb_write.get_pyramid_flag( 'bxmh5'  )

        else:
            assert False, datasource_name

        if not os.path.exists(out_folder_sph5):
            os.makedirs(out_folder_sph5)
        if not os.path.exists(out_folder_bxmh5):
            os.makedirs(out_folder_bxmh5)

        IsIntact_sh5,ck_str = Sorted_H5f.check_sh5_intact( self.file_name )
        if not IsIntact_sh5:
            print( "\n\nsh5 not intact:  %s \nAbandon generating sph5"%(self.file_name) )
            return

        # check bmh5 intact primarily
        IsIntact_bmh5,ck_str = GlobalSubBaseBLOCK.check_bmh5_intact(gsbb_write.bmh5_fn)
        if Always_CreateNew_bmh5 or ( not IsIntact_bmh5 ):
            gsbb_write.save_bmap_between_dif_stride_step()
        t1 = time.time()
        #-----------------------------------------------------------------------
        pl_sph5_filename = os.path.join(out_folder_sph5,scene_name+'.sph5')
        IsIntact_pl_sph5,ck_str = Normed_H5f.check_sph5_intact( pl_sph5_filename )
        if (not Always_CreateNew_plh5) and IsIntact_pl_sph5:
            with h5py.File( pl_sph5_filename,'r' ) as plh5f:
                cur_global_num_point = plh5f.attrs['sample_num']
            if cur_global_num_point != gsbb_write.global_num_point:
                Sorted_H5f.add_new_sample_num_in_plsph5( pl_sph5_filename, gsbb_write )
            else:
                if not SHOW_ONLY_ERR: print('pyh5 intact: %s'%(pl_sph5_filename))
        else:
            self.save_pl_sph5( pl_sph5_filename, gsbb_write, self, IsShowSummaryFinished)

        IsIntact_pl_sph5,ck_str = Normed_H5f.check_sph5_intact( pl_sph5_filename )
        if ck_str == 'void file':
            if not SHOW_ONLY_ERR: print('void file, skip generating bxmh5 : %s'%(pl_sph5_filename))
            return
        #-----------------------------------------------------------------------
        t2 = time.time()
        # save bxmap file
        bxmh5_fn = os.path.join(out_folder_bxmh5,region_name+'.bxmh5')
        IsIntact_sph5_bmap,ck_str = Normed_H5f.check_sph5_intact( bxmh5_fn )
        if (not Always_CreateNew_bxmh5) and  IsIntact_sph5_bmap:
            if not SHOW_ONLY_ERR: print('bxmh5 intact: %s'%(bxmh5_fn))
        else:
            Sorted_H5f.save_bxmap_h5f( bxmh5_fn, gsbb_write, self, pl_sph5_filename )
        t3 = time.time()
        scope = self.h5f.attrs['xyz_max'] - self.h5f.attrs['xyz_min']
        area = scope[0] * scope[1]
        if t3 - t0 > 1:
            print('\tper square meters save bmh5 t:%f  save pl_sph5 t: %f, save bxmap_h5 t: %f  area: %s'%( (t1-t0), (t2-t1), (t3-t2), area ))

        # gen ply
        if IsGenPly:
            gsbb_write.gen_bxmap_ply( pl_sph5_filename, bxmh5_fn )

    def save_pl_sph5(self, pl_sph5_filename, gsbb_write, S_H5f, IsShowSummaryFinished):
        global_num_point = gsbb_write.global_num_point
        assert global_num_point >= gsbb_write.max_global_num_point, "max_global_num_point=%d pl_sph5 file not exist, cannot add global_num_point=%d"%(gsbb_write.max_global_num_point,global_num_point)
        print('start gen sph5 file: ',pl_sph5_filename)
        t0 = time.time()
        with h5py.File(pl_sph5_filename,'w') as h5f:
            global_attrs = gsbb_write.get_new_attrs('global')

            pl_sph5f = Normed_H5f(h5f,pl_sph5_filename,S_H5f.h5f.attrs['datasource_name'])
            pl_sph5f.copy_root_attrs_from_sorted(global_attrs,global_num_point,S_H5f.IS_CHECK)

            file_datas = []
            file_global_sample_rate = []
            file_labels = []
            file_rootb_split_idxmaps = []
            global_sampling_meta_sum = {}
            file_gbixyzs = []

            feed_norm_ele_info = Normed_H5f.get_norm_eles_by_attrs(S_H5f.h5f.attrs['element_names'])
            feed_data_elements = feed_norm_ele_info['norm_data_eles']
            feed_label_elements = feed_norm_ele_info['label_eles']

            all_sorted_global_bids = gsbb_write.get_all_sorted_aimbids('global')
            num_global_block_abandoned = 0
            num_point_abandoned = 0
            for global_block_id in all_sorted_global_bids:
                block_datas, block_labels, rootb_split_idxmap, global_sampling_meta, global_sample_rate = \
                    self.get_data_larger_block( global_block_id,gsbb_write,feed_data_elements,feed_label_elements, gsbb_write.global_num_point, Normed_H5f.max_rootb_num )
                global_bixyz = Sorted_H5f.block_index_to_ixyz_( global_block_id, global_attrs )
                if NETCONFIG['max_global_sample_rate']!=None and  global_sample_rate > NETCONFIG['max_global_sample_rate']:
                    num_global_block_abandoned += 1
                    num_point_abandoned += block_datas.shape[0]
                    continue    # too less points, abandon
                if block_datas.size == 0:
                    continue

                file_datas.append(np.expand_dims(block_datas,axis=0))
                file_global_sample_rate.append( global_sample_rate )
                file_labels.append(np.expand_dims(block_labels,axis=0))
                file_gbixyzs.append(np.expand_dims(global_bixyz,axis=0))
                file_rootb_split_idxmaps.append(np.expand_dims(rootb_split_idxmap,axis=0))
                if len( global_sampling_meta_sum ) == 0:
                    global_sampling_meta_sum = global_sampling_meta
                else:
                    for key in global_sampling_meta:
                        global_sampling_meta_sum[key] += global_sampling_meta[key]

            if len(file_datas) == 0:
                h5f.attrs['intact_void_file'] = 1
                print('all point in this file are void : %s\n'%(pl_sph5_filename))
            else:
                file_datas = np.concatenate(file_datas,axis=0)
                file_global_sample_rate = np.array( file_global_sample_rate )
                file_labels = np.concatenate(file_labels,axis=0)
                file_gbixyzs = np.concatenate(file_gbixyzs,axis=0)
                file_rootb_split_idxmaps = np.concatenate(file_rootb_split_idxmaps,axis=0)

                pl_sph5f.append_to_dset('data',file_datas)
                pl_sph5f.append_to_dset('block_sample_rate',file_global_sample_rate)
                if file_labels.size > 0:
                    pl_sph5f.append_to_dset('labels',file_labels,IsLabelWithRawCategory=False)
                pl_sph5f.append_to_dset('gbixyz',file_gbixyzs)
                pl_sph5f.append_to_dset('rootb_split_idxmap', file_rootb_split_idxmaps)
                for key in global_sampling_meta_sum:
                    h5f['rootb_split_idxmap'].attrs[key] = global_sampling_meta_sum[key]
                h5f['rootb_split_idxmap'].attrs['num_global_block_abandoned'] = num_global_block_abandoned
                h5f['rootb_split_idxmap'].attrs['num_point_abandoned'] = num_point_abandoned
                h5f['rootb_split_idxmap'].attrs['max_global_sample_rate'] = NETCONFIG['max_global_sample_rate']

                t_sph5 = time.time() - t0
                pl_sph5f.h5f.attrs['t'] = t_sph5
                pl_sph5f.sph5_create_done()
                if IsShowSummaryFinished:
                    pl_sph5f.show_summary_info()
                print('plsph5 file create finished: data shape: %s'%(str(pl_sph5f.data_set.shape)) )


    @staticmethod
    def add_new_sample_num_in_plsph5( pl_sph5_filename, gsbb_write ):
        global_num_point = gsbb_write.global_num_point
        global_num_point_str  = str(int(global_num_point))
        with h5py.File(pl_sph5_filename,'a') as pl_h5f:
            # check is intact already
            rsm_str = global_num_point_str+'-rootb_split_idxmap'
            if rsm_str in pl_h5f and 'is_intact_new_sample_num' in pl_h5f[ rsm_str ].attrs:
                if pl_h5f[rsm_str].attrs['is_intact_new_sample_num']==1:
                    print('small global_num %d is already intact in %s'%(global_num_point,pl_sph5_filename))
                    return
            print('start add global_num_point (%d) to plsph5 file: %s'%(global_num_point, pl_sph5_filename) )

            cur_global_num_point = pl_h5f.attrs['sample_num']
            assert global_num_point < cur_global_num_point
            assert cur_global_num_point >= gsbb_write.max_global_num_point
            rootb_split_idxmap = pl_h5f['rootb_split_idxmap']
            new_rootb_split_idxmap = []
            new_point_idxs = []
            missed_rootb_num = 0
            missed_point_num = 0
            for k in range(rootb_split_idxmap.shape[0]):
                new_point_idxs_k = random_choice( np.arange(cur_global_num_point), global_num_point )
                new_point_idxs.append( np.expand_dims(new_point_idxs_k,0) )
                del_point_idxs_k = [i for i in range(cur_global_num_point) if i not in new_point_idxs_k ]
                new_rootb_split_idxmap_k,missed_rootb_num_k = Sorted_H5f.down_sample_bsplit_idxmap( rootb_split_idxmap[k], del_point_idxs_k, global_num_point )
                new_rootb_split_idxmap.append( np.expand_dims( new_rootb_split_idxmap_k,0 ) )
                missed_rootb_num += missed_rootb_num_k
                missed_point_num += pl_h5f['rootb_split_idxmap'].attrs['missed_point_num'] + pl_h5f.attrs['sample_num'] - global_num_point
            new_rootb_split_idxmap = np.concatenate( new_rootb_split_idxmap,0 )
            new_point_idxs = np.concatenate( new_point_idxs,0 )

            pl_sph5f = Normed_H5f(pl_h5f, pl_sph5_filename)
            pl_sph5f.create_dsets_new_sample_num( global_num_point )
            pl_sph5f.append_to_dset( global_num_point_str + '-rootb_split_idxmap', new_rootb_split_idxmap )
            pl_sph5f.append_to_dset( global_num_point_str + '-point_indices', new_point_idxs )
            pl_h5f[global_num_point_str + '-rootb_split_idxmap'].attrs['missed_point_num'] = missed_point_num
            pl_h5f[global_num_point_str + '-rootb_split_idxmap'].attrs['missed_rootb_num'] = missed_rootb_num
            pl_sph5f.sph5_create_done()
        print('finish adding global_num_point (%d) to plsph5 file: %s'%(global_num_point, pl_sph5_filename) )

    @staticmethod
    def save_bxmap_h5f(bxmh5_fn, gsbb_write, S_H5f, pl_sph5_filename ):
        '''
        bxmh5:
        '''
        print( 'start writing bxmap h5f: %s'%(bxmh5_fn))
        t0 = time.time()
        with h5py.File(pl_sph5_filename,'r') as pl_sph5f, h5py.File(bxmh5_fn,'w') as bxmh5f:
            if gsbb_write.global_num_point == pl_sph5f.attrs['sample_num']:
                rootb_split_idxmap = pl_sph5f['rootb_split_idxmap']
            else:
                rootb_split_idxmap = pl_sph5f[str(gsbb_write.global_num_point)+'-rootb_split_idxmap']
            global_block_num = rootb_split_idxmap.shape[0]

            bxmh5f = Normed_H5f(bxmh5f,bxmh5_fn,S_H5f.h5f.attrs['datasource_name'])
            bxmh5f.create_bidxmap_dsets( gsbb_write )

            sg_all_bidxmaps = []
            all_flatten_bidxmaps = []
            sum_bxmap_metas = []
            globalb_bottom_center_xyz = np.zeros( shape=(global_block_num,2,3), dtype=np.float32 )

            debug_meta={}
            debug_meta['bxmh5_fn'] = bxmh5_fn
            print('global_block_num: %d'%(global_block_num))
            for global_bidx in range( global_block_num ):
                gb_center, gb_bottom, gb_top =  Sorted_H5f.ixyz_to_xyz( pl_sph5f['gbixyz'][global_bidx], pl_sph5f.attrs )
                globalb_bottom_center_xyz[global_bidx, 0,:] = gb_bottom
                globalb_bottom_center_xyz[global_bidx, 1,:] = gb_center
                if DEBUGTMP:
                    debug_meta['gb_bottom'] = gb_bottom
                    debug_meta['gb_center'] = gb_center

                debug_meta['global_bidx'] = global_bidx
                sg_bidxmaps, flatten_bidxmaps, bxmap_metas =\
                       gsbb_write.get_all_bidxmaps( rootb_split_idxmap[global_bidx], debug_meta )
                sg_all_bidxmaps.append(np.expand_dims(sg_bidxmaps,0))
                all_flatten_bidxmaps.append(np.expand_dims(flatten_bidxmaps,0))

                if len(sum_bxmap_metas)==0: sum_bxmap_metas = bxmap_metas
                else:
                    for key in sum_bxmap_metas:
                        sum_bxmap_metas[key] += bxmap_metas[key]

            sg_all_bidxmaps = np.concatenate(sg_all_bidxmaps,0)
            all_flatten_bidxmaps = np.concatenate(all_flatten_bidxmaps,0)
            bxmh5f.append_to_dset('bidxmaps_sample_group',sg_all_bidxmaps)
            bxmh5f.append_to_dset('globalb_info', globalb_bottom_center_xyz)
            if all_flatten_bidxmaps.shape[2]>0:
                bxmh5f.append_to_dset('bidxmaps_flat',all_flatten_bidxmaps[...,0:2].astype(np.int32))
                bxmh5f.append_to_dset('fmap_neighbor_idis',all_flatten_bidxmaps[...,2:3].astype(np.float32))

            for key in sum_bxmap_metas:
                setattr(gsbb_write, key, sum_bxmap_metas[key])
            gsbb_write.write_gsbbattrs_to_bmh5_bxmh5( bxmh5f.h5f.attrs, file_format='bxmh5' )
            #gsbb_write.load_para_from_bxmh5( bxmh5_fn )
            #gsbb_write.write_bxm_paras_in_txt( bxmh5_fn, pl_sph5_filename )

            t_bxmh5 = time.time() - t0
            bxmh5f.h5f.attrs['t'] = t_bxmh5
            bxmh5f.sph5_create_done( bmh5_fn = gsbb_write.bmh5_fn, pl_sph5_fn = pl_sph5_filename )
            print('write finish: %s'%(bxmh5_fn))

    def get_feed_ele_ids(self,feed_data_elements,feed_label_elements):
        feed_data_ele_ids = self.get_data_ele_ids(feed_data_elements)
        feed_label_ele_ids = self.get_label_ele_ids(feed_label_elements)
        return feed_data_ele_ids,feed_label_ele_ids


def sort_to_blocks_onef(Sort_RawH5f_Instance,file_name,block_step_xyz=[1,1,1]):
    '''
    split th ewhole scene to space sorted small blocks
    The whole scene is a group. Each block is one dataset in the group.
    The block attrs represents the field.
    '''
    print('start sorting file to blocks: %s'%file_name)
    block_step = np.array( block_step_xyz )
    Sort_RawH5f_Instance.row_num_limit = None

    if not os.path.exists(Sort_RawH5f_Instance.out_folder):
        os.makedirs(Sort_RawH5f_Instance.out_folder)
    basefn = os.path.splitext(os.path.basename(file_name))[0]
    blocked_file_name = os.path.join(Sort_RawH5f_Instance.out_folder,basefn)+'.sh5'
    with h5py.File(blocked_file_name,'w') as h5f_blocked:
        with h5py.File(file_name,'r') as h5_f:
            Sort_RawH5f_Instance.raw_h5f = Raw_H5f(h5_f,file_name)
            Sort_RawH5f_Instance.s_h5f = Sorted_H5f(h5f_blocked,blocked_file_name)

            Sort_RawH5f_Instance.s_h5f.copy_root_attrs_from_raw( Sort_RawH5f_Instance.raw_h5f.raw_h5f )
            Sort_RawH5f_Instance.s_h5f.set_step_stride(block_step,block_step)

            #Sort_RawH5f_Instance.row_num_limit = int(self.raw_h5f.total_row_N/1000)

            row_step = g_h5_num_row_1M*2
            sorted_buf_dic = {}
            raw_row_N = Sort_RawH5f_Instance.raw_h5f.xyz_dset.shape[0]

            for k in range(0,raw_row_N,row_step):
                end = min(k+row_step,raw_row_N)
                _,data_name_list = Sort_RawH5f_Instance.raw_h5f.get_total_num_channels_name_list()
                raw_buf = np.zeros((end-k,Sort_RawH5f_Instance.s_h5f.total_num_channels))
                for dn in data_name_list:
                    raw_buf[:,Sort_RawH5f_Instance.s_h5f.data_idxs[dn] ] = Sort_RawH5f_Instance.raw_h5f.raw_h5f[dn][k:end,:]
                if Sort_RawH5f_Instance.s_h5f.IS_CHECK:
                    if end < 16777215: # this is the largest int float32 can acurately present
                        org_row_index = np.arange(k,end)
                    else:
                        org_row_index = -1
                    raw_buf[:,Sort_RawH5f_Instance.s_h5f.data_idxs['org_row_index'][0]] = org_row_index

                sorted_buf_dic={}
                Sort_RawH5f_Instance.sort_buf(raw_buf,k,sorted_buf_dic)

                Sort_RawH5f_Instance.h5_write_buf(sorted_buf_dic)

                if int(k/row_step) % 1 == 0:
                    print('%%%.1f  line[ %d:%d ] block_N = %d'%(100.0*end/Sort_RawH5f_Instance.raw_h5f.total_row_N, k,end,len(sorted_buf_dic)))
                     #print('line: [%d,%d] blocked   block_T=%f s, read_T=%f ms, cal_t = %f ms, write_t= %f ms'%\
                           #(k,end,time.time()-t0_k,(t1_k-t0_k)*1000,(t2_1_k-t2_0_k)*1000, (t2_2_k-t2_1_k)*1000 ))
                if hasattr(Sort_RawH5f_Instance,'row_num_limit') and Sort_RawH5f_Instance.row_num_limit!=None and  end>=Sort_RawH5f_Instance.row_num_limit:
                #if k /row_step >3:
                    print('break read at k= ',end)
                    break

            total_row_N,total_block_N = Sort_RawH5f_Instance.s_h5f.add_total_row_block_N()

            if total_row_N != Sort_RawH5f_Instance.raw_h5f.total_row_N:
                print('ERROR: blocked total_row_N= %d, raw = %d'%(total_row_N,Sort_RawH5f_Instance.raw_h5f.total_row_N))
            print('total_block_N = ',total_block_N)

            if Sort_RawH5f_Instance.s_h5f.IS_CHECK:
                check = Sort_RawH5f_Instance.s_h5f.check_equal_to_raw(Sort_RawH5f_Instance.raw_h5f) & Sort_RawH5f_Instance.s_h5f.check_xyz_scope()
                print('overall check of equal and scope:')
                if check:
                    print('both passed')
                else:
                    print('somewhere check failed')
            #Sort_RawH5f_Instance.s_h5f.show_summary_info()
class Sort_RawH5f():
    '''
    (1) Do sort: from "Raw_H5f" to "Sorted_H5f"
    unsampled: .sh5
    sampled: .rsh5  (fix number in each block)
    block_step_xyz=[0.5,0.5,0.5]
    '''
    def __init__(self,raw_file_list,block_step_xyz,out_folder,IsShowInfoFinished=False):
        self.IsShowInfoFinished = IsShowInfoFinished
        self.out_folder = out_folder
        self.Do_sort_to_blocks(raw_file_list,block_step_xyz)

    def Do_sort_to_blocks(self,raw_file_list,block_step_xyz):
        IsMulti = False
        if not IsMulti:
            for fn in raw_file_list:
                self.sort_to_blocks(fn,block_step_xyz)
                #sort_to_blocks_onef(self,fn,block_step_xyz)
        else:
            #pool = mp.Pool( max(mp.cpu_count()/2,1) )
            print('cpu_count= ',mp.cpu_count())
            pool = mp.Pool(processes=4)
            for i,fn in enumerate(raw_file_list):
                pool.apply_async(sort_to_blocks_onef,(self,fn,block_step_xyz,))
                print('apply_async %d  fn=%s'%(i,fn))
            pool.close()
            pool.join()


    def sort_to_blocks(self,file_name,block_step_xyz):
        '''
        split th ewhole scene to space sorted small blocks
        The whole scene is a group. Each block is one dataset in the group.
        The block attrs represents the field.
        '''
        t0 = time.time()
        IsIntact,_ = Raw_H5f.check_rh5_intact(file_name)
        if not IsIntact:
            print('Abandon sorting, rh5 not intact:'%(file_name))

        block_step = np.array( block_step_xyz )
        self.row_num_limit = None

        if not os.path.exists(self.out_folder):
            os.makedirs(self.out_folder)
        basefn = os.path.splitext(os.path.basename(file_name))[0]
        blocked_file_name = os.path.join(self.out_folder,basefn)+'.sh5'

        IsIntact,_ = Sorted_H5f.check_sh5_intact(blocked_file_name)
        if IsIntact:
            if not SHOW_ONLY_ERR: print('sh5 file intact: %s'%(blocked_file_name))
            return

        print('start sorting file to blocks: %s'%file_name)

        with h5py.File(blocked_file_name,'w') as h5f_blocked:
            with h5py.File(file_name,'r') as h5_f:
                self.raw_h5f = Raw_H5f(h5_f,file_name)
                self.s_h5f = Sorted_H5f(h5f_blocked,blocked_file_name)

                self.s_h5f.copy_root_attrs_from_raw( self.raw_h5f.h5f )
                self.s_h5f.set_step_stride(block_step,block_step)

                #self.row_num_limit = int(self.raw_h5f.total_row_N/1000)

                row_step = g_h5_num_row_1M*3
                sorted_buf_dic = {}
                raw_row_N = self.raw_h5f.xyz_dset.shape[0]

                for k in range(0,raw_row_N,row_step):
                    end = min(k+row_step,raw_row_N)
                    _,data_name_list = self.raw_h5f.get_total_num_channels_name_list()
                    raw_buf = np.zeros((end-k,self.s_h5f.total_num_channels))
                    for dn in data_name_list:
                        raw_buf[:,self.s_h5f.data_idxs[dn] ] = self.raw_h5f.h5f[dn][k:end,:]
                    if self.s_h5f.IS_CHECK:
                        if end < 16777215: # this is the largest int float32 can acurately present
                            org_row_index = np.arange(k,end)
                        else:
                            org_row_index = -1
                        raw_buf[:,self.s_h5f.data_idxs['org_row_index'][0]] = org_row_index

                    sorted_buf_dic={}
                    self.sort_buf(raw_buf,k,sorted_buf_dic)

                    self.h5_write_buf(sorted_buf_dic)

                    if int(k/row_step) % 1 == 0:
                        print('%%%.1f  line[ %d:%d ] block_N = %d'%(100.0*end/self.raw_h5f.total_row_N, k,end,len(sorted_buf_dic)))
                         #print('line: [%d,%d] blocked   block_T=%f s, read_T=%f ms, cal_t = %f ms, write_t= %f ms'%\
                               #(k,end,time.time()-t0_k,(t1_k-t0_k)*1000,(t2_1_k-t2_0_k)*1000, (t2_2_k-t2_1_k)*1000 ))
                    if hasattr(self,'row_num_limit') and self.row_num_limit!=None and  end>=self.row_num_limit:
                    #if k /row_step >3:
                        print('break read at k= ',end)
                        break
                assert end == self.raw_h5f.total_row_N
                total_row_N,total_block_N = self.s_h5f.add_total_row_block_N(self.raw_h5f.total_row_N)
                print('total_block_N = ',total_block_N)
                self.s_h5f.add_label_histagram()

                if self.s_h5f.IS_CHECK:
                    check = self.s_h5f.check_equal_to_raw(self.raw_h5f) & self.s_h5f.check_xyz_scope()
                    print('overall check of equal and scope:')
                    if check:
                        print('both passed')
                    else:
                        print('somewhere check failed')
                self.s_h5f.h5f.attrs['is_intact'] = 1
                if self.IsShowInfoFinished:
                    summary_str = self.s_h5f.show_summary_info()
                    summary_fn = os.path.splitext( blocked_file_name )[0] + '.txt'
                    with open( summary_fn, 'w' ) as sf:
                        sf.write( summary_str )

                scope = self.raw_h5f.xyz_scope
                area = scope[0] * scope[1]
                t = (time.time() - t0) / area
                print('sorted OK: %s  per 1 square meters t: %f'%(blocked_file_name,t))

    def sort_buf(self,raw_buf,buf_start_k,sorted_buf_dic):
        #t0 = time.time()
        IsMulti = False
        if IsMulti:
            block_ks = self.get_block_index_multi(raw_buf)
        else:
            block_ks = np.zeros(raw_buf.shape[0],np.int64)
            for j in range(raw_buf.shape[0]):
                block_ks[j] = self.s_h5f.xyz_to_block_index(raw_buf[j,0:3])[0]

        #t1 = time.time()
        for i in range(raw_buf.shape[0]):
            block_k = block_ks[i]
            row = raw_buf[i,:].reshape(1,-1)
            if not block_k in sorted_buf_dic:
                sorted_buf_dic[block_k]=[]
            sorted_buf_dic[block_k].append(row)
        #t2 = time.time()
        #print('t1 = %d ms, t2 = %d ms'%( (t1-t0)*1000,(t2-t1)*1000 ))

    def h5_write_buf(self,sorted_buf_dic):
        for key in sorted_buf_dic:
            sorted_buf_dic[key] = np.concatenate(sorted_buf_dic[key],axis=0)
        for block_k in sorted_buf_dic:
            self.s_h5f.append_to_dset(block_k,sorted_buf_dic[block_k],vacant_size=g_h5_num_row_1M)
        self.s_h5f.rm_invalid_data()
        self.s_h5f.h5f.flush()

    def get_block_index_multi(self,raw_buf):
        block_ks = mp.Array('i',raw_buf.shape[0])
        num_workers = 2
        step = int(raw_buf.shape[0]/num_workers)
        pool = []
        for i in range(0,raw_buf.shape[0],step):
            end = min( (i+1)*step, raw_buf.shape[0])
            p = mp.Process(target=self.get_block_index_subbuf,args=(raw_buf[i:end,0:3],block_ks,i) )
            p.start()
            pool.append(p)
        for p in pool:
            p.join()
        return block_ks

    def get_block_index_subbuf(self,sub_buf_xyz,block_ks,i_start):
        for i in range(sub_buf_xyz.shape[0]):
            block_ks[i+i_start] = self.s_h5f.xyz_to_block_index(sub_buf_xyz[i,0:3])[0]


class DatasetMeta():
    g_label2class_dic = {}
    g_label2class_dic['MATTERPORT'] = MatterportMeta['label2class']
    g_label2class_dic['ETH'] = {0: 'unlabeled points', 1: 'man-made terrain', 2: 'natural terrain',\
                     3: 'high vegetation', 4: 'low vegetation', 5: 'buildings', \
                     6: 'hard scape', 7: 'scanning artefacts', 8: 'cars'}

    g_label2class_dic['STANFORD_INDOOR3D'] = \
                    {0:'ceiling', 1:'floor', 2:'wall', 3:'beam', 4:'column', 5:'window', 6:'door', 7:'table',
                     8:'chair', 9:'sofa', 10:'bookcase', 11:'board', 12:'clutter'}

    g_label2class_dic['SCANNET'] = {0:'unannotated', 1:'wall', 2:'floor', 3:'chair', 4:'table', 5:'desk',\
                                6:'bed', 7:'bookshelf', 8:'sofa', 9:'sink', 10:'bathtub', 11:'toilet',\
                                12:'curtain', 13:'counter', 14:'door', 15:'window', 16:'shower curtain',\
                                17:'refridgerator', 18:'picture', 19:'cabinet', 20:'otherfurniture'}
    g_label2color_dic = {}
    g_label2color_dic['MATTERPORT'] = MatterportMeta['label2color']
    g_label2color_dic['ETH'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],5:[10,200,100]}
    g_label2color_dic['STANFORD_INDOOR3D'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100]}
    g_label2color_dic['SCANNET'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100],
                    13: [100,200,200],14: [200,100,200],15: [100,200,100],16: [100,100,200],
                     17:[100,100,100],18:[200,200,200],19:[200,200,100],20:[200,200,100]}

    g_label2class_dic['KITTI'] = {0:'background', 1:'car', 2:'pedestrian', 3:'cyclist'}  ## benz_m
    g_label2color_dic['KITTI'] = { 0:[0,0,0], 1:[0,0,255], 2:[0,255,255], 3:[255,255,0]  }     ## benz_m

    def __init__(self,datasource_name):
        self.datasource_name = datasource_name
        self.g_label2class = self.g_label2class_dic[self.datasource_name]
        self.g_label2color = self.g_label2color_dic[self.datasource_name]
        self.g_class2label = {cls:label for label,cls in self.g_label2class.iteritems()}
        self.g_class2color = {}
        for i in self.g_label2class:
            cls = self.g_label2class[i]
            self.g_class2color[cls] = self.g_label2color[i]
        self.num_classes = len(self.g_label2class)


class Normed_H5f():
    '''
    format: .nhf5
    (1) There are 3 datasets:
        'data' data_set store all normalized data, shape: N*(H*W)*C, like [num_block,4096,9]
        'labels' [num_block,4096]
        'pred_logits'
    (2) The root attrs are inherited from Sorted_H5f, just record the raw data information. Not all attrs are meaningful here.
        Especially, 'element_names' means the raw elements in Sorted_H5f.
        The normed data elements are stored in attrs of dataset "data".
    (3) The elements to be stored are flexible, the idx order responds to self.normed_ele_idx_order
    (4) The label_category in Sorted_H5f is raw_category_idx, the label_category in Normed_H5f is mpcat40 index

    *** example of root attrs:
    The root_attr:  [u'datasource_name', u'element_names', u'total_block_N', u'xyz_max', u'xyz_min', u'xyz_max_aligned', u'xyz_min_aligned', u'xyz_scope_aligned', u'block_step', u'block_stride', u'block_dims_N', u'total_row_N', u'sample_num']
    datasource_name: MATTERPORT
    element_names: ['label_material' 'label_instance' 'label_category' 'color' 'nxnynz' 'xyz']
    total_row_N: 98304
    total_block_N: 12
    block_step: [4 4 2]
    block_stride: [2 2 2]
    block_dims_N: [2 3 2]
    xyz_min: [-2.2845428   0.40383905 -0.02779843]
    xyz_max: [ 0.87131613  5.36070538  2.57167315]
    xyz_min_aligned: [-2.3  0.4 -0.2]
    xyz_max_aligned: [ 0.9  5.4  2.6]
    xyz_scope_aligned: [ 3.2  5.   2.8]

    *** example of attrs of data_dset: ( shape= (12, 8192, 15))
    color_1norm  =  [12 13 14]
    nxnynz  =  [ 9 10 11]
    xyz  =  [0 1 2]
    xyz_midnorm  =  [3 4 5]
    xyz_1norm  =  [6 7 8]
    valid_num  =  12

    *** example of attrs of labels_dset:
    labels   shape= (12, 8192, 3)
    label_material  =  [2]
    label_instance  =  [1]
    label_category  =  [0]
    valid_num  =  12
    label_category_hist  =  [    5 25276     0 14310  9288     0     0     0     0     0     0     0
    5122     0 26148     0     0     0     0     0     0     0     0     0
    0     0     0     0     0 13049     0     0     0     0     0     0
    0     0   409     0  1477     0]
    label_category_1norm  =  [  5.25850827e-05   2.65828110e-01   0.00000000e+00   1.50498507e-01
    '''
    # -----------------------------------------------------------------------------
    # CONSTANTS
    # -----------------------------------------------------------------------------
    g_label2class_dic = {}
    g_label2class_dic['MATTERPORT'] = MatterportMeta['label2class']
    g_label2class_dic['ETH'] = {0: 'unlabeled points', 1: 'man-made terrain', 2: 'natural terrain',\
                     3: 'high vegetation', 4: 'low vegetation', 5: 'buildings', \
                     6: 'hard scape', 7: 'scanning artefacts', 8: 'cars'}

    g_label2class_dic['STANFORD_INDOOR3D'] = \
                    {0:'ceiling', 1:'floor', 2:'wall', 3:'beam', 4:'column', 5:'window', 6:'door', 7:'table',
                     8:'chair', 9:'sofa', 10:'bookcase', 11:'board', 12:'clutter'}

    g_label2class_dic['SCANNET'] = g_label2class_dic['scannet']   = {0:'unannotated', 1:'wall', 2:'floor', 3:'chair', 4:'table', 5:'desk',\
                                6:'bed', 7:'bookshelf', 8:'sofa', 9:'sink', 10:'bathtub', 11:'toilet',\
                                12:'curtain', 13:'counter', 14:'door', 15:'window', 16:'shower curtain',\
                                17:'refridgerator', 18:'picture', 19:'cabinet', 20:'otherfurniture'}
    g_label2color_dic = {}
    g_label2color_dic['MATTERPORT'] = MatterportMeta['label2color']
    g_label2color_dic['ETH'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],5:[10,200,100]}
    g_label2color_dic['STANFORD_INDOOR3D'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100]}
    g_label2color_dic['SCANNET'] = \
                    {0:	[0,0,0],1:	[0,0,255],2:	[0,255,255],3: [255,255,0],4: [255,0,255],10: [100,100,255],
                    6: [0,255,0],7: [170,120,200],8: [255,0,0],9: [200,100,100],5:[10,200,100],11:[200,200,200],12:[200,200,100],
                    13: [100,200,200],14: [200,100,200],15: [100,200,100],16: [100,100,200],
                     17:[100,100,100],18:[200,200,200],19:[200,200,100],20:[200,200,100]}

    g_label2class_dic['KITTI'] = {0:'background', 1:'car', 2:'pedestrian', 3:'cyclist'}   ## benz_m
    g_label2color_dic['KITTI'] = { 0:[0,0,0], 1:[0,0,255], 2:[0,255,255], 3:[255,255,0] }     ## benz_m

    #g_easy_view_labels = [7,8,9,10,11,1]
    #g_is_labeled = True

    ## normed data channels
    normed_data_elements_candi = {}
    #normed_data_elements_candi['xyz'] = ['xyz','xyz_midnorm_block','xyz_1norm_file']
    normed_data_elements_candi['xyz'] = ['xyz']
    normed_data_elements_candi['nxnynz'] = ['nxnynz']
    normed_data_elements_candi['color'] = ['color_1norm']
    normed_data_elements_candi['intensity'] = ['intensity_1norm']
    normed_ele_idx_order = ['xyz_midnorm_block','xyz_1norm_file','xyz_1norm_block','xyz','color_1norm','nxnynz','intensity_1norm']
    normed_data_ele_candi_len = {'xyz':3,'xyz_midnorm_block':3,'xyz_1norm_file':3,'xyz_1norm_block':3,'nxnynz':3,'color_1norm':3,'intensity_1norm':1}

    labels_order = ['label_category','label_instance','label_material']
    label_candi_eles_len = {'label_category':1,'label_instance':1,'label_material':1}
    max_rootb_num = 20000

    def __init__(self,h5f,file_name,datasource_name=None):
        '''
        DATASET_NAME = 'ETH'
        DATASET_NAME = 'STANFORD_INDOOR3D'
        '''
        self.h5f = h5f
        self.file_name = file_name
        if datasource_name == None:
            assert 'datasource_name' in self.h5f.attrs
        else:
            self.h5f.attrs['datasource_name'] = datasource_name
        assert self.h5f.attrs['datasource_name'] in DATA_SOURCE_NAME_LIST
        self.datasource_name = self.h5f.attrs['datasource_name']
        self.g_label2class = self.g_label2class_dic[self.datasource_name]
        self.g_label2color = self.g_label2color_dic[self.datasource_name]
        self.g_class2label = {cls:label for label,cls in self.g_label2class.iteritems()}
        self.g_class2color = {}
        for i in self.g_label2class:
            cls = self.g_label2class[i]
            self.g_class2color[cls] = self.g_label2color[i]
        self.num_classes = len(self.g_label2class)

        self.dataset_names = ['data','labels','raw_xyz','pred_logits']
        for dn in self.dataset_names:
            if dn in h5f:
                setattr(self,dn+'_set', h5f[dn])
        self.update_norm_eles_by_attrs()

    @staticmethod
    def get_norm_eles_by_attrs(h5f_attrs_element_names):
        norm_ele_info = {}
        norm_data_eles = [Normed_H5f.normed_data_elements_candi[de] for de in Normed_H5f.normed_data_elements_candi if de in h5f_attrs_element_names]
        norm_data_eles = [e  for e_ls in norm_data_eles for e in e_ls]
        norm_ele_info['norm_data_eles'] = [e for e in Normed_H5f.normed_ele_idx_order if e in norm_data_eles]
        norm_data_ele_lens = np.array( [Normed_H5f.normed_data_ele_candi_len[e] for e in norm_data_eles ])
        norm_ele_info['norm_data_eles_num'] = np.sum(norm_data_ele_lens)

        norm_ele_info['label_eles'] = [lb for lb in Normed_H5f.labels_order if lb in h5f_attrs_element_names]
        norm_ele_info['label_eles_num'] = len(norm_ele_info['label_eles'])

        norm_ele_info['norm_data_ele_idxs'] = Normed_H5f.get_normeddata_ele_idxs(norm_ele_info['norm_data_eles'])
        norm_ele_info['label_ele_idxs'] = Normed_H5f.get_label_ele_ids(norm_ele_info['label_eles'])

        return norm_ele_info

    def update_norm_eles_by_attrs(self):
        if 'element_names' not in self.h5f.attrs:
            return # new created file
        norm_ele_info = Normed_H5f.get_norm_eles_by_attrs(self.h5f.attrs['element_names'])

        self.norm_data_eles_num = norm_ele_info['norm_data_eles_num']
        self.label_eles_num = norm_ele_info['label_eles_num']
        self.normed_data_set_elements = norm_ele_info['norm_data_eles']
        self.label_set_elements = norm_ele_info['label_eles']
        self.normed_data_ele_idxs = norm_ele_info['norm_data_ele_idxs']
        self.label_ele_idxs = norm_ele_info['label_ele_idxs']

    def get_feed_ele_ids(self,feed_data_elements,feed_label_elements):
        if feed_data_elements==None:
            feed_data_ele_idxs = self.normed_data_ele_idxs
        else:
            feed_data_ele_idxs = self.get_normeddata_ele_idxs(feed_data_elements)
        if feed_label_elements==None:
            feed_label_ele_ids = self.label_ele_idxs
        else:
            feed_label_ele_ids = self.get_label_ele_ids(feed_label_elements)
        return feed_data_ele_idxs,feed_label_ele_ids

    def get_normed_data(self,start_block,end_blcok,feed_elements=None):
        # the data ele order store according to feed_elements
        if feed_elements==None:
            datas = self.data_set[start_block:end_blcok,...]
        else:
            check_feed_ele = [ ele in self.data_set.attrs for ele in feed_elements]
            assert all( check_feed_ele ), " not all ele in feed_elements exist, feed_elements=%s "%(feed_elements)
            normed_data_ele_idx = np.sort(list(set([k for e in feed_elements for k in self.data_set.attrs[e] ])))
            datas = self.data_set[start_block:end_blcok,...,normed_data_ele_idx]
        return datas

    @staticmethod
    def get_bidxmaps(bxmh5_fn, start_block,end_block):
        with h5py.File(bxmh5_fn,'r') as h5f:
            if 'bidxmaps_flat' in h5f.keys():
                assert h5f['bidxmaps_flat'].shape[0] >= end_block
                flatten_bidxmaps = h5f['bidxmaps_flat'][start_block:end_block,:]
                fmap_neighbor_idis = h5f['fmap_neighbor_idis'][start_block:end_block,:]
                sg_bidxmaps = h5f['bidxmaps_sample_group'][start_block:end_block,:]
                globalb_bottom_center_xyz = h5f['globalb_info'][start_block:end_block]
                globalb_bottom_center_xyz = globalb_bottom_center_xyz.reshape( [globalb_bottom_center_xyz.shape[0],1,6] )
                return  sg_bidxmaps, flatten_bidxmaps, fmap_neighbor_idis, globalb_bottom_center_xyz
            else:   ## benz_m
                assert h5f['bidxmaps_sample_group'].shape[0] >= end_block
                sg_bidxmaps = h5f['bidxmaps_sample_group'][start_block:end_block,:]
                #block_step_cascades = h5f.attrs['block_step_cascades']
                #block_stride_cascades = h5f.attrs['block_stride_cascades']
                ## Add global block xyz_min_aligned to block_step_cascades
                ##   for extra global layer while using 3DCNN
                #block_step_cascades = np.concatenate( [block_step_cascades, np.expand_dims(h5f.attrs['xyz_scope_aligned'],0) ], 0 )
                #block_stride_cascades = np.concatenate( [block_stride_cascades, np.expand_dims(h5f.attrs['xyz_min_aligned'],0) ], 0 )
                globalb_bottom_center_xyz = h5f['globalb_info'][start_block:end_block]
                globalb_bottom_center_xyz = globalb_bottom_center_xyz.reshape( [globalb_bottom_center_xyz.shape[0],1,6] )
                return  sg_bidxmaps, globalb_bottom_center_xyz

    def get_label_eles(self,start_block,end_blcok,feed_label_elements=None):
        # order according to feed_label_elements
        if feed_label_elements==None:
            labels = self.labels_set[start_block:end_blcok,...]
        else:
            labels_ele_idx = np.sort(list(set( [k for e in feed_label_elements for k in self.labels_set.attrs[e]] )))
            labels = self.labels_set[start_block:end_blcok,...,labels_ele_idx]
            assert labels.ndim == self.labels_set.ndim
        return labels
    @staticmethod
    def get_normeddata_ele_idxs(normed_data_elements):
        # order according to Normed_H5f.normed_ele_idx_order
        # len of each ele according to  normed_data_ele_candi_len
        data_ele_idxs = {}
        k = 0
        for e in Normed_H5f.normed_ele_idx_order:
            if e in normed_data_elements:
                #assert e in Normed_H5f.normed_ele_idx_order,"%s not in Normed_H5f.normed_ele_idx_order"%(e)
                idx = range(k,k+Normed_H5f.normed_data_ele_candi_len[e])
                k += Normed_H5f.normed_data_ele_candi_len[e]
                data_ele_idxs[e] = idx
        return data_ele_idxs
    @staticmethod
    def get_label_ele_ids(label_elements):
        # order according to  Normed_H5f.labels_order
        label_ele_idxs = {}
        k = 0
        for e in Normed_H5f.labels_order:
            if e in label_elements:
                label_ele_idxs[e] = range(k,k+Normed_H5f.label_candi_eles_len[e])
                k += Normed_H5f.label_candi_eles_len[e]
        return label_ele_idxs

    def raw_category_idx_2_mpcat40(self,labels_with_rawcategory):
        assert labels_with_rawcategory.ndim == 2
        assert 'label_category' in self.labels_set.attrs, "no label_category"
        raw_category_idx = self.labels_set.attrs['label_category'][0]
        labels_with_rawcategory[:,raw_category_idx] = get_cat40_from_rawcat(labels_with_rawcategory[:,raw_category_idx])
        labels_without_rawcategory = labels_with_rawcategory
        return labels_without_rawcategory

    def copy_root_attrs_from_sorted(self,sortedh5f_attrs,block_sample_num,sortedh5f_IS_CHECK):
        attrs=['datasource_name','element_names','total_block_N',
               'xyz_max','xyz_min','xyz_max_aligned','xyz_min_aligned','xyz_scope_aligned',
               'block_step','block_stride','block_dims_N','total_row_N']
        for attr in attrs:
            if attr in sortedh5f_attrs:
                self.h5f.attrs[attr] = sortedh5f_attrs[attr]
        self.h5f.attrs['is_intact_sph5'] = 0
        self.h5f.attrs['sample_num'] = block_sample_num

        # - org_row_index when sortedh5f IS_CHECK=True
        self.create_dsets()

    def copy_root_attrs_from_normed(self,h5f_normed, in_bxmh5_fn=None, flag=None):
        if 'data' in h5f_normed:
            self.copy_root_attrs_from_normed_plsph5( h5f_normed, flag )
        elif 'bidxmaps_sample_group' in h5f_normed:
            self.copy_root_attrs_from_normed_bxmh5( h5f_normed, in_bxmh5_fn, flag )
        else:
            assert False

    def copy_root_attrs_from_normed_plsph5(self,h5f_normed,flag=None):
       # attrs_candis=['datasource_name','element_names','total_block_N',
       #        'xyz_max','xyz_min','xyz_max_aligned','xyz_min_aligned','xyz_scope_aligned',
       #        'block_step','block_stride','block_dims_N','total_row_N']
        for attr in h5f_normed.attrs:
            self.h5f.attrs[attr] = h5f_normed.attrs[attr]
        self.h5f.attrs['is_intact_sph5'] = 0
        if flag=='MergeNormed_H5f':
            if 'total_block_N' in self.h5f.attrs:
                del self.h5f.attrs['total_block_N']
            if 'total_row_N' in self.h5f.attrs:
                del self.h5f.attrs['total_row_N']

        normed_data_shape = h5f_normed['data'].shape
        if len(normed_data_shape)==3:
            sample_num = (normed_data_shape[1],)
        elif len(normed_data_shape)==4:
            sample_num = (normed_data_shape[1],normed_data_shape[2],)
        self.h5f.attrs['sample_num'] = sample_num
        self.create_dsets()
        for attr in h5f_normed['rootb_split_idxmap'].attrs:
            self.h5f['rootb_split_idxmap'].attrs[attr] = h5f_normed['rootb_split_idxmap'].attrs[attr]
        self.h5f['rootb_split_idxmap'].attrs['valid_num'] = 0

    def copy_root_attrs_from_normed_bxmh5(self,h5f_normed, in_bxmh5_fn, flag=None):
        for attr in h5f_normed.attrs:
            self.h5f.attrs[attr] = h5f_normed.attrs[attr]
            #if attr in GlobalSubBaseBLOCK.meta_names:
            #    self.h5f.attrs[attr] = h5f_normed.attrs[attr] * 0
            #print(attr,h5f_normed.attrs[attr])
        self.h5f.attrs['is_intact_sph5'] = 0
        gsbb_empty = GlobalSubBaseBLOCK( )
        gsbb_empty.load_para_from_bxmh5( in_bxmh5_fn )
        self.create_bidxmap_dsets(gsbb_empty)

    def show_summary_info(self):
        print('\n\nsummary of file: ',self.file_name)
        show_h5f_summary_info(self.h5f)

    @staticmethod
    def show_all_colors( datasource_name ):
        from PIL import Image
        label2color = Normed_H5f.g_label2color_dic[datasource_name]
        label2class = Normed_H5f.g_label2class_dic[datasource_name]
        path = os.path.join( BASE_DIR,'label_colors' )
        if not os.path.exists(path):
            os.makedirs(path)
        for label,color in label2color.iteritems():
            if label < len( label2class ):
                cls = label2class[label]
            else:
                cls = 'empty'
            data = np.zeros((512,512,3),dtype=np.uint8)
            color_ = np.array(color,dtype=np.uint8)
            data += color_
            img = Image.fromarray(data,'RGB')
            img.save(path+'/'+str(label)+'_'+cls+'.png')
            img.show()

    def label2color(self,label):
        assert( label in self.g_label2color )
        return self.g_label2color[label]

    def get_data_shape(self):
        dset = self.h5f['data']
        return dset.shape

    def create_dsets(self):
        self.h5f.attrs['is_intact_sph5'] = 0
        if 'total_block_N' in self.h5f.attrs:
            total_block_N = self.h5f.attrs['total_block_N']
        else:
            total_block_N = 0
        chunks_n = 1
        sample_num = self.h5f.attrs['sample_num']
        if sample_num.size==1:
            sample_num_size = sample_num
            sample_num = (sample_num,)
        elif sample_num.size==2:
            sample_num_size = sample_num[0] * sample_num[1]
            sample_num = (sample_num[0],sample_num[1],)
        self.update_norm_eles_by_attrs()
        norm_data_eles_num = self.norm_data_eles_num
        label_eles_num = self.label_eles_num

        #chunks_n = math.ceil( 1024 / 1*sample_num_size*norm_data_eles_num*4 )
        data_set = self.h5f.create_dataset( 'data',shape=(total_block_N,)+sample_num+(norm_data_eles_num,),\
                maxshape=(None,)+sample_num+(norm_data_eles_num,),dtype=np.float32,compression="gzip", chunks = (chunks_n,)+sample_num+(norm_data_eles_num,)  )
        data_set.attrs['valid_num'] = 0
        bsample_rate_set = self.h5f.create_dataset( 'block_sample_rate',shape=(total_block_N,),\
                maxshape=(None,), dtype=np.float32  )
        bsample_rate_set.attrs['valid_num'] = 0
        if label_eles_num > 1:
            labels_set = self.h5f.create_dataset( 'labels',shape=(total_block_N,)+sample_num+(label_eles_num,),\
                    maxshape=(None,)+sample_num+(label_eles_num,),dtype=np.int16,compression="gzip", chunks = (chunks_n,)+sample_num+(label_eles_num,)  )
            labels_set.attrs['valid_num'] = 0
            self.labels_set = labels_set
        gbixyz_set = self.h5f.create_dataset( 'gbixyz',shape=(total_block_N,3,),\
                maxshape=(None,3,),dtype=np.int32,compression="gzip", chunks = (chunks_n,3,)  )
        gbixyz_set.attrs['valid_num'] = 0
        rootb_split_idxmap_set = self.h5f.create_dataset( 'rootb_split_idxmap',shape=(total_block_N, Normed_H5f.max_rootb_num,2),\
                maxshape=(None, Normed_H5f.max_rootb_num, 2),dtype=np.int32,compression="gzip", chunks = (chunks_n,Normed_H5f.max_rootb_num,2,)  )
        rootb_split_idxmap_set.attrs['valid_num'] = 0

        # predicted label
        #pred_logits_set = self.h5f.create_dataset( 'pred_logits',shape=(total_block_N,)+sample_num+(label_eles_num,),\
        #        maxshape=(None,)+sample_num+(label_eles_num,),dtype=np.int16,compression="gzip",\
        #        chunks = (chunks_n,)+sample_num+(label_eles_num,)  )
        #pred_logits_set.attrs['valid_num'] = 0
        #pred_logits_set[:] = -1

        for ele in self.normed_data_set_elements:
            data_set.attrs[ele] = self.normed_data_ele_idxs[ele]
        for ele in self.label_set_elements:
            labels_set.attrs[ele] = self.label_ele_idxs[ele]

        self.data_set = data_set
        #self.bidxmap_dsets = bidxmap_dsets
        #self.pred_logits_set = pred_logits_set

    def create_dsets_new_sample_num( self, new_sample_num ):
        total_block_N = 0
        chunks_n = 1000
        str_new_sample_num = str(int(new_sample_num))

        if str_new_sample_num+'-rootb_split_idxmap' in self.h5f:
            self.h5f.__delitem__(str_new_sample_num+'-rootb_split_idxmap')
        if str_new_sample_num+'-point_indices' in self.h5f:
            self.h5f.__delitem__(str_new_sample_num+'-point_indices')

        new_rootb_split_idxmap_set = self.h5f.create_dataset( str_new_sample_num+'-rootb_split_idxmap',shape=(total_block_N, Normed_H5f.max_rootb_num,2),\
                maxshape=(None, Normed_H5f.max_rootb_num, 2),dtype=np.int32,compression="gzip", chunks = (chunks_n,Normed_H5f.max_rootb_num,2,)  )
        new_rootb_split_idxmap_set.attrs['valid_num'] = 0
        new_point_indices_set = self.h5f.create_dataset( str_new_sample_num+'-point_indices',shape=(total_block_N, new_sample_num),\
                maxshape=(None, new_sample_num),dtype=np.int32,compression="gzip", chunks = (chunks_n,new_sample_num,)  )
        new_point_indices_set.attrs['valid_num'] = 0
        self.h5f[str_new_sample_num+'-rootb_split_idxmap'].attrs['is_intact_new_sample_num'] = 0
        new_sample_num = np.array([new_sample_num])
        if 'smaller_sample_num' not in self.h5f.attrs:
            self.h5f.attrs['smaller_sample_num'] = new_sample_num
        else:
            self.h5f.attrs['smaller_sample_num'] = np.concatenate( [self.h5f.attrs['smaller_sample_num'],new_sample_num] )

    def create_bidxmap_dsets(self, gsbb_write_or_load):
        self.h5f.attrs['is_intact_sph5'] = 0
        chunks_n = 1
        total_block_N = 1
        sg_block_shape = gsbb_write_or_load.get_sg_bidxmaps_fixed_shape()
        sg_bidxmap_dset = self.h5f.create_dataset('bidxmaps_sample_group',shape=(total_block_N,)+sg_block_shape, dtype=np.int32,
                            maxshape=(None,)+sg_block_shape,chunks = (chunks_n,)+sg_block_shape  )
        sg_bidxmap_dset.attrs['valid_num'] = 0

        globalb_info_dset = self.h5f.create_dataset('globalb_info',shape=(total_block_N,2,3), dtype=np.float32,
                            maxshape=(None,2,3),chunks = (chunks_n,2,3)  )
        globalb_info_dset.attrs['valid_num'] = 0
        globalb_info_dset.attrs['readme'] = 'global block bottom and center xyz'

        flatten_bidxmap_shape = gsbb_write_or_load.get_flatten_bidxmaps_shape()
        if flatten_bidxmap_shape[1]>0:
            flatten_bidxmap_dset = self.h5f.create_dataset('bidxmaps_flat',shape=(total_block_N,)+flatten_bidxmap_shape[0:-1]+(2,),dtype=np.int32,
                                                        maxshape=(None,)+flatten_bidxmap_shape, chunks = (chunks_n,)+flatten_bidxmap_shape[0:-1]+(2,)  )
            flatten_bidxmap_dset.attrs['valid_num'] = 0

            fmap_neighbor_idis_dset = self.h5f.create_dataset('fmap_neighbor_idis',shape=(total_block_N,)+flatten_bidxmap_shape[0:-1]+(1,),dtype=np.float32,
                                                        maxshape=(None,)+flatten_bidxmap_shape, chunks = (chunks_n,)+flatten_bidxmap_shape[0:-1]+(1,)  )
            fmap_neighbor_idis_dset.attrs['valid_num'] = 0

    def write_summary_sph5( self ):
        def get_rootb_num( rootb_split_idxmap ):
            rootb_nums = []
            for i in range(rootb_split_idxmap.shape[0]):
                rootb_num = index_in_sorted( rootb_split_idxmap[i,:,0]==-1, np.array([1]) )
                if not rootb_num.size == 1:
                    print('rootb_num:',rootb_num)
                    import pdb; pdb.set_trace()  # XXX BREAKPOINT
                    assert False, "rootb_num err"
                rootb_nums.append(rootb_num)
            rootb_num = np.sum( rootb_nums )
            return rootb_num

        summary_fn = os.path.splitext( self.file_name )[0] + '.txt'
        with open(summary_fn,'w') as sf:
            dataset_ls = [ds for ds in self.h5f]
            sf.write('Totally %d datasets: %s\n'%( len(dataset_ls), dataset_ls ))
            for ds in self.h5f:
                sf.write( '  %s : %s\n'%(ds, self.h5f[ds].shape) )
            sf.write('\n')

            summary = 'basic info'
            attrs = self.h5f.attrs
            for ele_name in attrs:
                summary += '\t%s: %s\n'%(ele_name, attrs[ele_name])

            base_sample_num = attrs['sample_num']
            summary += '\nsampling meta for sample_num=%d\n'%( base_sample_num )
            attrs = self.h5f['rootb_split_idxmap'].attrs
            rootb_split_idxmap = self.h5f['rootb_split_idxmap']
            global_b_num = rootb_split_idxmap.shape[0]
            rootb_num = get_rootb_num( rootb_split_idxmap )
            valid_num = attrs['valid_num'] * 1.0
            for ele_name in attrs:
                if ele_name == 'missed_point_num':
                    summary += '\t%s: %s'%(ele_name, attrs[ele_name]/valid_num)
                    total_point_num = base_sample_num + attrs[ele_name]/valid_num
                    summary += ' / %d   %f'%( total_point_num, 1.0*attrs[ele_name]/valid_num/total_point_num )
                elif ele_name == 'missed_rootb_num':
                    summary += '\t%s: %s'%(ele_name, attrs[ele_name]/valid_num)
                    summary += ' / %d   %f'%( rootb_num, 1.0*attrs[ele_name]/rootb_num)
                else:
                    summary += '\t%s: %s'%(ele_name, attrs[ele_name])
                summary += '\n'

            if 'smaller_sample_num' in self.h5f.attrs:
                for sample_num in self.h5f.attrs['smaller_sample_num']:
                    rootb_num = get_rootb_num( self.h5f[str(sample_num)+'-rootb_split_idxmap'] )
                    summary += '\nsampling meta for point_num=%d\n'%(sample_num)
                    attrs = self.h5f[str(sample_num)+'-rootb_split_idxmap'].attrs
                    for ele_name in attrs:
                        summary += '\t%s: %s'%(ele_name, attrs[ele_name])
                        if ele_name == 'missed_point_num':
                            total_point_num = sample_num + attrs[ele_name]
                            summary += ' / %d   %f'%( total_point_num, 1.0*attrs[ele_name]/total_point_num )
                        if ele_name == 'missed_rootb_num':
                            summary += ' / %d   %f'%( rootb_num, 1.0*attrs[ele_name]/rootb_num)
                        summary += '\n'
            sf.write( summary )
            sf.write( '\ngen t: %0.2f sec'%(self.h5f.attrs['t']) )
            print('finish summary file: %s'%(summary_fn))

    def write_summary_bxmh5( self, bmh5_fn = None, pl_sph5_fn = None ):
        summary_fn = os.path.splitext( self.file_name )[0] + '.txt'
        with open(summary_fn,'w') as sf:
            dataset_ls = [ds for ds in self.h5f]
            sf.write('Totally %d datasets: %s\n'%( len(dataset_ls), dataset_ls ))
            for ds in self.h5f:
                sf.write( '  %s : %s\n'%(ds, self.h5f[ds].shape) )
            sf.write('\n')
            if pl_sph5_fn!=None:
                sf.write( 'Responding sph5 folder: %s\n'%( os.path.basename( os.path.dirname( pl_sph5_fn ) ) ))
                sf.write( 'Responding bmh5 folder: %s\n\n'%( os.path.basename( os.path.dirname( bmh5_fn ) ) ))
            summary = 'basic info\n'
            attrs = self.h5f.attrs
            for ele_name in attrs:
                if ele_name not in GlobalSubBaseBLOCK.meta_names:
                    summary += '\t%s: %s\n'%(ele_name, attrs[ele_name])
            sf.write( summary )

            cascade_num = attrs['sub_block_step_candis'].size
            meta_str = '\n\n'
            for cascade_id in range(cascade_num):
                meta_str += 'cascade_id %d bxmap meta:\n'%(cascade_id)
                count = attrs['count'][cascade_id]
                for ele_name in GlobalSubBaseBLOCK.meta_names:
                    if ele_name not in attrs:
                        continue
                    meta_str += '\t%s: %s'%( ele_name, attrs[ele_name][cascade_id] / count )
                    if ele_name == 'count':
                        meta_str += ' (%d)'%(count)
                    if ele_name == 'aimbnum_missed_add':
                        meta_str += ' \t<-- stride:%s  nsubblock:%s'%( attrs['sub_block_stride_candis'][cascade_id], attrs['nsubblock_candis'][cascade_id] )
                    if ele_name == 'baseb_exact_flat_num':
                        meta_str += ' \t<-- nsubblock:%s  step:%s'%( attrs['nsubblock_candis'][cascade_id], attrs['sub_block_step_candis'][cascade_id] )
                        baseb_exact_flat_num = attrs[ele_name][cascade_id] / count
                        meta_str += '\n\t\tmissed_baseb_num:%d/%0.1f%%'%( baseb_exact_flat_num[0], 100.0*baseb_exact_flat_num[0]/np.sum(baseb_exact_flat_num) )
                    if ele_name == 'npointsubblock_missed_add':
                        meta_str += ' \t<-- npoint_subblock:%s'%( attrs['npoint_subblock_candis'][cascade_id] )
                    meta_str += '\n'
                meta_str += '\n'
            sf.write( meta_str )
            sf.write( '\ngen t: %0.2f sec'%(attrs['t']) )

    def raw_xyz_set(self):
        return self.data_set[...,self.data_set.attrs['xyz']]

    def create_areano_dset(self,total_block_N,sample_num):
        chunks_n = 4
        area_no_set = self.h5f.create_dataset( 'area_no',shape=(total_block_N,sample_num),\
                maxshape=(None,sample_num),dtype=np.int16,compression="gzip",\
                chunks = (chunks_n,sample_num)  )
        area_no_set.attrs['valid_num'] = 0

    def append_to_dset(self,dset_name,data_i,vacant_size=0,IsLabelWithRawCategory=False):
        dset = self.h5f[dset_name]
        valid_num = dset.attrs['valid_num']
        if data_i.ndim == len(dset.shape) -1:
            for i in range(1,len(dset.shape)):
                assert(dset.shape[i] == data_i.shape[i-1]), "(A) dset.shape: %s \t data_i.shape: %s"%(dset.shape, data_i.shape)
            new_valid_num = valid_num + 1
        elif data_i.ndim == len(dset.shape):
            assert(dset.shape[1:] == data_i.shape[1:]), "(B) dset.shape: %s \t data_i.shape: %s"%(dset.shape, data_i.shape)
            new_valid_num = valid_num + data_i.shape[0]
            print('%s  %d -> %d'%(dset_name,valid_num,new_valid_num) )

        if new_valid_num > dset.shape[0]:
            dset.resize( (new_valid_num + vacant_size,)+dset.shape[1:] )

        if IsLabelWithRawCategory and dset_name == 'labels':
            data_i = self.raw_category_idx_2_mpcat40(data_i)

        dset[valid_num : new_valid_num,...] = data_i
        dset.attrs['valid_num'] = new_valid_num

        self.h5f.flush()

    def set_dset_value(self,dset_name,data_i,start_idx,end_idx):
        if dset_name not in self.h5f:
            return
        dset = self.h5f[dset_name]
        if dset.shape[0] < end_idx:
            dset.resize( (end_idx,) + dset.shape[1:] )
        if data_i.shape[1] < dset.shape[1]:
            dset[start_idx:end_idx,data_i.shape[1]:] = -1
        dset[start_idx:end_idx,0:data_i.shape[1]] = data_i
        if dset.attrs['valid_num'] < end_idx:
            dset.attrs['valid_num'] = end_idx

    def merge_file(self,another_file_name):
        # merge all the data from another_file intto self
        with h5py.File(another_file_name,'r') as f:
            ano_normed_h5f = Normed_H5f(f,another_file_name)
            for dset_name in ano_normed_h5f.h5f:
                self.append_to_dset(dset_name,ano_normed_h5f.h5f[dset_name])
                # set area no
            if self.datasource_name == 'STANFORD_INDOOR3D':
                base_name = os.path.basename(another_file_name)
                tmp = base_name.split('Area_')[1].split('_')[0]
                area_no = int(tmp)

                num_blocks,num_sample = ano_normed_h5f.h5f['label'].shape
                area_data = np.ones((num_blocks,num_sample)) * area_no
                self.append_to_dset('area_no',area_data)

    def sph5_create_done(self,bmh5_fn = None, pl_sph5_fn = None ):
        self.rm_invalid_data()
        self.add_label_histagram()
        self.h5f.attrs['is_intact_sph5'] = 1
        if 'smaller_sample_num' in self.h5f.attrs:
            for new_sample_num in self.h5f.attrs['smaller_sample_num']:
                self.h5f[str(int(new_sample_num))+'-rootb_split_idxmap'].attrs['is_intact_new_sample_num'] = 1
        if 'data' in self.h5f:
            self.write_summary_sph5()
        else:
            self.write_summary_bxmh5( bmh5_fn = bmh5_fn, pl_sph5_fn = pl_sph5_fn )

    @staticmethod
    def check_sph5_intact( file_name ):
        f_format = os.path.splitext(file_name)[-1]
        assert f_format == '.sph5' or f_format == '.prh5' or f_format == '.bxmh5'
        if not os.path.exists(file_name):
            return False, "%s not exist"%(file_name)

        file_type = magic.from_file(file_name)
        if "Hierarchical Data Format" not in file_type:
            return False,"File signature err"
        #print('checking sph5: %s'%(file_name))
        with h5py.File(file_name,'r') as h5f:
            if 'intact_void_file' in h5f.attrs:
                return True,"void file"
            if 'is_intact_sph5' not in h5f.attrs and 'is_intact_nh5' not in h5f.attrs:
                return False,"no is_intact_sph5 attr"
            if 'is_intact_nh5' in h5f.attrs:
                IsIntact = h5f.attrs['is_intact_nh5'] == 1
            if 'is_intact_sph5' in h5f.attrs:
                IsIntact = h5f.attrs['is_intact_sph5'] == 1
            return IsIntact,"is_intact_sph5=1"

    def rm_invalid_data(self):
        for dset_name_i in self.h5f:
            def rm_invalid_dset(dset):
                valid_n = dset.attrs['valid_num']
                if dset.shape[0] > valid_n:
                    #print('resizing block %s from %d to %d'%(dset_name_i,dset.shape[0],valid_n))
                    dset.resize( (valid_n,)+dset.shape[1:] )
            rm_invalid_dset( self.h5f[dset_name_i] )
           # else:
           #     grp = self.h5f[dset_name_i]
           #     for dset_name_ij in grp:
           #         rm_invalid_dset( grp[dset_name_ij] )


    def add_label_histagram(self):
        label_name = 'label_category'
        if not hasattr(self,'labels_set'):
            return
        if label_name in self.labels_set.attrs:
            label_hist,_ = np.histogram(self.labels_set[:,:,self.labels_set.attrs[label_name]],range(self.num_classes+1))
            label_hist_1norm = label_hist / np.sum(label_hist).astype(np.float)
            self.labels_set.attrs[label_name+'_hist'] = label_hist
            self.labels_set.attrs[label_name+'_hist1norm'] = label_hist_1norm

    def Get_file_accuracies(self,IsWrite=False,out_path=None):
        # get the accuracy of each file by the pred data in hdf5
        if self.pred_logits_set.shape[0] != self.labels_set.shape[0]:
            return ''
        class_num = len(self.g_class2label)
        class_TP = np.zeros(shape=(class_num))
        class_FN = np.zeros(shape=(class_num))
        class_FP = np.zeros(shape=(class_num))
        total_num = self.raw_xyz_set.size

        for j in range(0,self.raw_xyz_set.shape[0]):
            xyz_block = self.raw_xyz_set[j,:]
            label_gt = self.labels_set[j,:]
            label_pred = self.pred_logits_set[j,:]
            for i in range(xyz_block.shape[0]):
                # calculate accuracy
                if (label_gt[i]==label_pred[i]):
                    class_TP[label_gt[i]] += 1
                else:
                    class_FN[label_gt[i]] += 1
                    class_FP[label_pred[i]] += 1
        acc_str,ave_acc_str = Normed_H5f.cal_accuracy(class_TP,class_FN,class_FP,total_num)
        return class_TP,class_FN,class_FP,total_num,acc_str,ave_acc_str

    @staticmethod
    def cal_accuracy(TP,FN,FP,total_num):
        precision = np.nan_to_num(TP/(TP+FP))
        recall = np.nan_to_num(TP/(TP+FN))
        IOU = np.nan_to_num(TP/(TP+FN+FP))
        # weighted ave
        real_Pos = TP+FN
        normed_real_TP = real_Pos/np.sum(real_Pos)
        ave_4acc = np.zeros(shape=(4),dtype=np.float)
        ave_4acc[0] = np.sum( precision*normed_real_TP )
        ave_4acc[1] = np.sum( recall*normed_real_TP )
        ave_4acc[2] = np.sum( IOU*normed_real_TP )
        ave_4acc[3] = np.sum(TP)/total_num
        ave_4acc_name = ['ave_class_pre','ave_class_rec','ave_class_IOU','ave_point_accu']
        # gen str
        delim = '' # ','
        def getstr(array,mean=None,str_format='%0.3g'):
            if mean!=None:
                mean_str = '%9s'%(str_format%mean) + delim
            else:
                mean_str = '%9s'%('  ')
                if delim != '': mean_str = mean_str + ' '

            return mean_str + delim.join(['%9s'%(str_format%v) for v in array])
        ave_acc_str = 'point average:  %0.3f,  class ave pre/rec/IOU: %0.3f/ %0.3f/ %0.3f    N = %f M'% \
            ( ave_4acc[0],  ave_4acc[1], ave_4acc[2], ave_4acc[3],total_num/1000000.0)
        acc_str = ave_acc_str + '\n\t       average'+delim  + delim.join(['%9s'%c for c in Normed_H5f.g_class2label])+'\n'
        acc_str += 'class_pre:   '+getstr(precision,ave_4acc[0])+'\n'
        acc_str += 'class_rec:   '+getstr(recall,ave_4acc[1])+'\n'
        acc_str += 'class_IOU:   '+getstr(IOU,ave_4acc[2])+'\n'
        acc_str += 'number(K):   '+getstr(np.trunc(real_Pos/1000.0),str_format='%d')+'\n'
        return acc_str,ave_acc_str

    def gen_gt_pred_obj_examples(self,config_flag = ['ALL'],out_path=None):
        #all_catigories = ['ceiling','floor','wall','beam','column','window','door','table','chair','sofa','bookcase','board','clutter']
        all_catigories = [key for key in self.g_class2label]
        #config_flag = ['Z','building_6_no_ceiling']
        #config_flag = ['all_single','Y']
        #config_flag = ['ALL','Y']
        #config_flag = ['Y']
        def get_config(config_flag):
            if config_flag == 'all_single':
                show_categaries_ls = [[c] for c in all_catigories]
                visu_flag = all_catigories
                return  [None]*len(all_catigories), show_categaries_ls, visu_flag
            else:
                if config_flag =='ALL':
                    xyz_cut_rate=None
                    show_categaries=None
                elif config_flag =='Y':
                    xyz_cut_rate=[0,0.92,1]
                    show_categaries=None
                elif config_flag =='Z':
                    xyz_cut_rate=[0,0,0.93]
                    show_categaries=None
                elif config_flag =='XZ':
                    xyz_cut_rate=[0.95,0,0.93]
                    show_categaries=None
                elif config_flag == 'wall':
                    show_categaries = ['wall']
                    xyz_cut_rate = None
                elif config_flag == 'all_single':
                    show_categaries = all_catigories
                    xyz_cut_rate = [None]*len(all_catigories)
                elif config_flag =='building_7':
                    xyz_cut_rate=None
                    show_categaries=['ceiling','floor','wall','beam','column','window','door']
                elif config_flag =='building_6_no_ceiling':
                    xyz_cut_rate=None
                    show_categaries=['floor','wall','beam','column','window','door']
                elif config_flag =='void':
                    xyz_cut_rate=None
                    show_categaries=['void']
                return [xyz_cut_rate],[show_categaries],[config_flag]

        num_blocks = self.data_set.shape[0]
        vis_block_ids = range(num_blocks)
        vis_block_ids = [None]

        for flag in config_flag:
            xyz_cut_rate_ls,show_categaries,visu_flags = get_config(flag)
            for i in range(len(xyz_cut_rate_ls)):
                for vis_block_id in vis_block_ids:
                    self.gen_gt_pred_obj(out_path,xyz_cut_rate_ls[i],show_categaries[i],
                                        pre_fn=str(visu_flags[i]),visu_flag=str(flag),block_id=vis_block_id )
           # self.Get_file_accuracies(IsWrite=True)

    def gen_gt_pred_obj(self,out_path=None,xyz_cut_rate=None,show_categaries=None,
                        pre_fn='',visu_flag=None,block_id=None):
        '''
            (1)xyz_cut_rate:
                # when rate < 0.5: cut small
                # when rate >0.5: cut big
            (2) show_categaries:  ['ceiling']
                the categaries to show, if None  show all
        '''
        if show_categaries != None:
            show_categaries = [self.g_class2label[c] for c in show_categaries]
        #if self.pred_logits_set.shape[0] ==0:
        #    print('File: %s \n   has no pred data'%(self.file_name))
        #    return
        base_fn = os.path.basename(self.file_name)
        base_fn = os.path.splitext(base_fn)[0]
        folder_path = os.path.dirname(self.file_name)
        if out_path == None:
            obj_folder = os.path.join(folder_path,'obj_file',base_fn)
            if visu_flag != None:
                obj_folder = os.path.join(obj_folder,visu_flag)
            if block_id != None:
                pre_fn += 'b'+str(block_id)+'_'
        else:
            obj_folder = os.path.join(out_path,base_fn)
        print('obj_folder=',obj_folder)
        if not os.path.exists(obj_folder):
            os.makedirs(obj_folder)

        raw_obj_fn = os.path.join(obj_folder, 'raw_'+pre_fn+'.obj')
        raw_colored_obj_fn = os.path.join(obj_folder, 'raw_color_'+pre_fn+'.obj')
        gt_obj_fn = os.path.join(obj_folder, 'gt_'+pre_fn+'.obj')
        pred_obj_fn = os.path.join(obj_folder,'pred_'+pre_fn+'.obj')
        dif_FN_obj_fn = os.path.join(obj_folder, 'dif_FN_'+pre_fn+'.obj')
        dif_FP_obj_fn = os.path.join(obj_folder, 'dif_FP_'+pre_fn+'.obj')
        correct_obj_fn = os.path.join(obj_folder,'correct_'+pre_fn+'.obj')
        correct_num = 0
        pred_num = 0
        raw_xyz_set = self.raw_xyz_set()
        if raw_xyz_set.ndim == 4:
            raw_xyz_set = np.reshape(raw_xyz_set,(raw_xyz_set.shape[0],-1,raw_xyz_set.shape[-1]))
        file_size = raw_xyz_set.shape[0] * raw_xyz_set.shape[1]

        if xyz_cut_rate != None:
            # when rate < 0.5: cut small
            # when rate >0.5: cut big
            xyz_max = np.array([np.max(raw_xyz_set[:,:,i]) for i in range(3)])
            xyz_min = np.array([np.min(raw_xyz_set[:,:,i]) for i in range(3)])
            xyz_scope = xyz_max - xyz_min
            xyz_thres = xyz_scope * xyz_cut_rate + xyz_min
            print('xyz_thres = ',str(xyz_thres))
        cut_num = 0

        with open(gt_obj_fn,'w') as gt_f, open(raw_obj_fn,'w') as raw_f, open(raw_colored_obj_fn,'w') as raw_colored_f:
          with open(pred_obj_fn,'w') as pred_f,open(dif_FN_obj_fn,'w') as dif_FN_f,open(dif_FP_obj_fn,'w') as dif_FP_f:
            with open(correct_obj_fn,'w') as correct_f:
                for j in range(0,raw_xyz_set.shape[0]):
                    if block_id != None and block_id != j:
                        continue
                    xyz_block = raw_xyz_set[j,...]
                    label_gt = np.reshape( self.labels_set[j,...],(-1,self.labels_set.shape[-1])  )[:,self.label_ele_idxs['label_category'][0]]
                    if 'color_1norm' in self.data_set.attrs: hascolor = True
                    else: hascolor = False
                    if hascolor:
                        color_block = (self.data_set[j,:,self.data_set.attrs['color_1norm']]*255).astype(np.uint8)
                    #if self.pred_logits_set.shape[0] !=0 and  j < self.pred_logits_set.shape[0]:
                    #    IsGenPred = True
                    #    label_pred = np.reshape( self.pred_logits_set[j,:], (-1,3) )[:,self.label_ele_idxs['label_category'][0]]
                    #else:
                    #    IsGenPred = False
                    IsGenPred = False
                    for i in range(xyz_block.shape[0]):

                        # cut parts by xyz or label
                        is_cut_this_point = False
                        if show_categaries!=None and label_gt[i] not in show_categaries:
                                #label_pred[i] not in show_categaries:
                            # cut by category
                            is_cut_this_point = True
                        elif xyz_cut_rate!=None:
                            # cut by position
                            for xyz_j in range(3):
                                if (xyz_cut_rate[xyz_j] >0.5 and xyz_block[i,xyz_j] > xyz_thres[xyz_j]) or \
                                    (xyz_cut_rate[xyz_j]<=0.5 and xyz_block[i,xyz_j] < xyz_thres[xyz_j]):
                                    is_cut_this_point =  True
                        if is_cut_this_point:
                            cut_num += 1
                            continue

                        color_gt = self.label2color( label_gt[i] )
                        str_xyz = 'v ' + ' '.join( ['%0.3f'%(d) for d in  xyz_block[i,:] ])
                        str_xyz = str_xyz + ' \t'
                        if hascolor:
                            str_raw_color = ' '.join( ['%d'%(d) for d in  color_block[i,:]]) + '\n'
                        str_color_gt = ' '.join( ['%d'%(d) for d in  color_gt]) + '\n'
                        str_gt = str_xyz + str_color_gt

                        if show_categaries == None or label_gt[i] in show_categaries:
                            raw_f.write(str_xyz+'\n')
                            if hascolor:
                                raw_colored_f.write(str_xyz+str_raw_color)
                            else:
                                raw_colored_f.write(str_xyz)
                            gt_f.write( str_gt )

                        if IsGenPred and label_pred[i] in self.g_label2color:
                            color_pred = self.label2color( label_pred[i] )
                            str_color_pred = ' '.join( ['%d'%(d) for d in  color_pred]) + '\n'
                            str_pred = str_xyz + str_color_pred
                            if show_categaries==None or label_pred[i] in show_categaries:
                                pred_f.write( str_pred )
                            if label_gt[i] != label_pred[i]:
                                if show_categaries == None or label_gt[i] in show_categaries:
                                    dif_FN_f.write(str_pred)
                                if show_categaries == None or label_pred[i] in show_categaries:
                                    dif_FP_f.write(str_gt)
                            else:
                                correct_f.write(str_pred)
                                correct_num += 1
                            pred_num += 1
                    if j%20 ==0: print('batch %d / %d'%(j,raw_xyz_set.shape[0]))


                print('gen gt obj file (%d): \n%s'%(file_size,gt_obj_fn) )
                if pred_num > 0:
                     print('gen pred obj file (%d,%f): \n%s '%(pred_num,1.0*pred_num/file_size,pred_obj_fn) )
                     print('gen correct obj file (%d,%f),: \n%s '%(correct_num,1.0*correct_num/pred_num,correct_obj_fn) )
                print('gen dif obj file: ',pred_obj_fn)
                print('cut roof ponit num = %d, xyz_cut_rate = %s'%(cut_num,str(xyz_cut_rate)) )


def MergeNormed_H5f(in_filename_ls,merged_filename, Always_CreateNew = False, IsShowSummaryFinished=False):
    if len(in_filename_ls) == 0:
        print('no .sph5/.prh5 file in the list')
        return
    for k,fn in enumerate(in_filename_ls):
        if not Normed_H5f.check_sph5_intact( fn )[0]:
            print('Abort merging. file not intact: %s'%(fn))
            return
    if not Always_CreateNew:
        IsIntact,_ = Normed_H5f.check_sph5_intact(merged_filename)
        if IsIntact:
            if not SHOW_ONLY_ERR: print('sph5/prh5 file intact: %s'%(merged_filename))
            return
    if not os.path.exists( os.path.dirname(merged_filename) ):
        os.makedirs( os.path.dirname(merged_filename) )
    print('start generating merged file: %s'%(merged_filename))
    with h5py.File(merged_filename,'w') as merged_h5f:
        void_f_N = 0
        for k,fn in enumerate(in_filename_ls):
            print('merging %s'%(fn))
            with h5py.File(fn,'r') as in_h5f:
                if 'intact_void_file' in in_h5f.attrs:
                    void_f_N += 1
                    continue
                if k == 0:
                    merged_normed_h5f = Normed_H5f(merged_h5f,merged_filename,in_h5f.attrs['datasource_name'])
                    merged_normed_h5f.copy_root_attrs_from_normed(in_h5f, fn, 'MergeNormed_H5f')
                else:
                    for attr in in_h5f.attrs:
                        if attr in GlobalSubBaseBLOCK.meta_names:
                            merged_normed_h5f.h5f.attrs[attr] = merged_normed_h5f.h5f.attrs[attr] + in_h5f.attrs[attr]

                    if 'xyz_scope_aligned' in in_h5f.attrs:
                        merged_normed_h5f.h5f.attrs['xyz_scope_aligned'] += in_h5f.attrs['xyz_scope_aligned']
                    if 'rootb_split_idxmap' in in_h5f:
                        for attr in in_h5f['rootb_split_idxmap'].attrs:
                            if attr != 'valid_num':
                                merged_normed_h5f.h5f['rootb_split_idxmap'].attrs[attr] += in_h5f['rootb_split_idxmap'].attrs[attr]

                in_normed_h5f = Normed_H5f(in_h5f,fn)
                for ele in in_h5f:
                    merged_normed_h5f.append_to_dset(ele, in_h5f[ele] )
        # average metrics
        if 'xyz_scope_aligned' in merged_normed_h5f.h5f.attrs:
            merged_normed_h5f.h5f.attrs['xyz_scope_aligned_ave'] = merged_normed_h5f.h5f.attrs['xyz_scope_aligned'] / len(in_filename_ls)


        merged_normed_h5f.sph5_create_done()

        format = os.path.splitext( merged_filename )[1]
        if format == '.bxmh5':
            gsbb_empty = GlobalSubBaseBLOCK()
            gsbb_empty.load_para_from_bxmh5( merged_filename )
            #gsbb_empty.write_bxm_paras_in_txt( merged_filename )

        if IsShowSummaryFinished:
            merged_normed_h5f.show_summary_info()
        print('merged h5f OK: %s'%(merged_filename))
        return void_f_N

def Write_all_file_accuracies(normed_h5f_file_list=None,out_path=None,pre_out_fn=''):
    if normed_h5f_file_list == None:
        normed_h5f_file_list = glob.glob( GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096 +
                            '/Area_2_office_1*' )
    if out_path == None: out_path = os.path.join(GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096,
                                    'pred_accuracy')
    if not os.path.exists(out_path):
        os.makedirs(out_path)
    all_acc_fn = os.path.join(out_path,pre_out_fn+'accuracies.txt')
    all_ave_acc_fn = os.path.join(out_path,pre_out_fn+'average_accuracies.txt')
    class_TP = class_FN = class_FP = np.zeros(shape=(len(Normed_H5f.g_class2label)))
    total_num = 0
    average_class_accu_ls = []
    with open(all_acc_fn,'w') as all_acc_f,open(all_ave_acc_fn,'w') as all_ave_acc_f:
        for i,fn in enumerate(normed_h5f_file_list):
            h5f = h5py.File(fn,'r')
            norm_h5f = Normed_H5f(h5f,fn)
            class_TP_i,class_FN_i,class_FP_i,total_num_i,acc_str_i,ave_acc_str_i = norm_h5f.Get_file_accuracies(
                IsWrite=False, out_path = out_path)
            class_TP = class_TP_i + class_TP
            class_FN = class_FN_i + class_FN
            class_FP = class_FP_i + class_FP
            total_num = total_num_i +  total_num

            if acc_str_i != '':
                all_acc_f.write('File: '+os.path.basename(fn)+'\n')
                all_acc_f.write(acc_str_i+'\n')
                all_ave_acc_f.write(ave_acc_str_i+'\t: '+os.path.basename(fn)+'\n')

        acc_str,ave_acc_str = Normed_H5f.cal_accuracy(class_TP,class_FN,class_FP,total_num)
        ave_str = 'Throughout All %d files.\n'%(i+1) +  acc_str
        all_acc_f.write('\n'+ave_str)
        all_ave_acc_f.write('\n'+ave_str)
    print('accuracy file: '+all_acc_fn)
    print('average accuracy file: '+all_ave_acc_fn)
    return ave_str,out_path,class_TP,class_FN,class_FP,total_num

def Write_Area_accuracies():
    ave_str_areas = ''
    class_TP = class_FN = class_FP = np.zeros(shape=(len(Normed_H5f.g_class2label)))
    total_num = 0
    for i in range(6):
        glob_i = 'Area_%d'%(i+1)
        normed_h5f_file_list = glob.glob( os.path.join(GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096,
                                glob_i+'*') )
        ave_str,out_path,class_TP_i,class_FN_i,class_FP_i,total_num_i = Write_all_file_accuracies(normed_h5f_file_list,pre_out_fn=glob_i+'_')
        class_TP = class_TP_i + class_TP
        class_FN = class_FN_i + class_FN
        class_FP = class_FP_i + class_FP
        total_num = total_num_i + total_num

        ave_str_areas += '\nArea%d\n'%i
        ave_str_areas += ave_str
    acc_str,ave_acc_str = Normed_H5f.cal_accuracy(class_TP,class_FN,class_FP,total_num)
    all_area_str = '\nThrough %d areas.\n'%(i+1)+acc_str
    with open(os.path.join(out_path,'areas_accuracies.txt'),'w' ) as area_acc_f:
        area_acc_f.write(ave_str_areas)
        area_acc_f.write(all_area_str)



#-------------------------------------------------------------------------------
# Test above codes
#-------------------------------------------------------------------------------
def Gen_raw_label_color_obj():
    base_fn = os.path.join(GLOBAL_PARA.ETH_raw_partA,'untermaederbrunnen_station1_xyz_intensity_rgb')
    data_fn = base_fn + '.txt'
    label_fn = base_fn + '.labels'
    obj_fn = base_fn + '.obj'
    obj_labeled_fn = base_fn + '_labeled.obj'
    obj_unlabeled_fn = base_fn + '_unlabeled.obj'
    labeled_N = 0
    unlabeled_N = 0
    with open(data_fn,'r') as data_f:
     with open(label_fn,'r') as label_f:
      with open(obj_fn,'w') as obj_f:
       with open(obj_labeled_fn,'w') as obj_labeled_f:
        with open(obj_unlabeled_fn,'w') as obj_unlabeled_f:
            data_label_fs = itertools.izip(data_f,label_f)
            for k,data_label_line in enumerate(data_label_fs):
                data_k =np.fromstring( data_label_line[0].strip(),dtype=np.float32,sep=' ' )
                label_k = np.fromstring( data_label_line[1].strip(),dtype=np.int16,sep=' ' )
                color_k = Normed_H5f.g_label2color[label_k[0]]
                str_k = 'v ' + ' '.join( [str(d) for d in data_k[0:3] ] ) + ' \t' +\
                    ' '.join( [str(c) for c in color_k] ) + '\n'
                obj_f.write(str_k)
                if label_k == 0:
                    unlabeled_N += 1
                    obj_unlabeled_f.write(str_k)
                else:
                    labeled_N += 1
                    obj_labeled_f.write(str_k)
                if k%(1000*100) == 0:
                    print('gen raw obj %d'%(k))
                if k > 1000*1000*1:
                    break
    total_N = k+1
    print('total_N = %d, labeled_N = %d (%0.3f), unlabeled_N = %d (%0.3f)'%\
          (total_N,labeled_N,1.0*labeled_N/total_N,unlabeled_N,1.0*unlabeled_N/total_N))

def Do_Norm(file_list):
    for fn in file_list:
        with h5py.File(fn,'r') as f:
            sf = Sorted_H5f(f,fn)
            sf.file_normalize_to_NormedH5F()

def Do_sample(file_list):
    #h5f_name = os.path.join(GLOBAL_PARA.ETH_A_stride_8_step_8,\
                      #'bildstein_station5_sub_m80_m5_stride_8_step_8.h5')
    for h5f_name in file_list:
        sample_num =  4096
        sample_method = 'random'
        with h5py.File(h5f_name,'r') as h5f:
            sh5f = Sorted_H5f(h5f,h5f_name)
            sh5f.file_random_sampling(sample_num)


def Test_get_block_data_of_new_stride_step():
    folder_base = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1'
    base_h5f_name = os.path.join(folder_base,'region9.sh5')

    xyz1norm_k = [0.2,0.3,0.1]
    new_stride = np.array([0.1,0.1,0.1])*10
    new_step = np.array([0.1,0.1,0.1])*20
    feed_data_elements=['xyz_midnorm','color_1norm']
    feed_label_elements=['label_category','label_instance']
    sample_num=8

    with h5py.File(base_h5f_name,'r') as base_h5f:
        GlobalSubBaseBLOCK.save_bmap_between_dif_stride_step(base_h5f,base_h5f_name)
        #GlobalSubBaseBLOCK.show_all(base_h5f,base_h5f_name)

        base_sh5f = Sorted_H5f(base_h5f,base_h5f_name)
        base_sh5f.file_saveas_pyramid_feed(True)
        #base_sh5f.load_blockids(new_stride,new_step)
        #base_sh5f.get_block_data_of_new_stride_step_byxyz1norm(xyz1norm_k,new_stride,new_step,feed_data_elements,feed_label_elements,sample_num)
        #base_sh5f.show_summary_info()

def Do_normalize_sorted_to_self():
    folder_base = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1'
    base_h5f_name = os.path.join(folder_base,'region0.sh5')
    with h5py.File(base_h5f_name,'r') as base_h5f:
        base_sh5f = Sorted_H5f(base_h5f,base_h5f_name)
        base_sh5f.file_normalize_to_self()

def Test_get_blockids_of_dif_stride_step():
    folder_base = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1'
    folder_new = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1_testtmp'
    if not os.path.exists(folder_new):
        os.makedirs(folder_new)
    base_h5f_name = os.path.join(folder_base,'region0.sh5')
    new_h5f_name = os.path.join(folder_new,'region0_testtmp.sh5')

    xyz1norm_k = [0.2,0.3,0.1]
    new_stride = [0.1,0.1,0.1]
    new_step = [0.1,0.1,0.1]

    with h5py.File(base_h5f_name,'r') as base_h5f:
        with h5py.File(new_h5f_name,'w') as new_h5f:
            base_sh5f = Sorted_H5f(base_h5f,base_h5f_name)
            new_sh5f = Sorted_H5f(new_h5f,new_h5f_name)
            new_sh5f.copy_root_summaryinfo_from_another(base_h5f,'new_stride')
            new_sh5f.set_step_stride(new_step,new_stride)
            #base_sh5f.show_summary_info()

            new_block_id_ls,i_xyz_new_ls,org_blockid0 = base_sh5f.get_blockids_of_dif_stride_step_byxyz(xyz1norm_k, new_stride, new_step )
            #print(new_block_id_ls)
            #print(i_xyz_new_ls)
            for new_block_id in new_block_id_ls:
                org_block_id_ls,org_i_xyz_ls = Sorted_H5f.get_blockids_of_dif_stride_step(new_block_id, base_attrs = new_h5f.attrs, aim_attrs= base_h5f.attrs, padding=0.6)
                assert org_blockid0 in org_block_id_ls
                #print(org_block_id_ls)

    print('check  get_blockids_of_dif_stride_step OK')



def Do_Check_xyz():
    #fnl = glob.glob(os.path.join(folder,'*.hdf5'))
    #for fn in fnl:
        raw_fn = os.path.join(GLOBAL_PARA.ETH_A_rawh5,'bildstein_station5_xyz_intensity_rgb.hdf5')
        fn_s = os.path.join( GLOBAL_PARA.ETH_A_stride_1_step_1,'bildstein_station5_sub_m80_m5_stride_2_step_4.h5')
        fn_s = os.path.join( GLOBAL_PARA.ETH_A_stride_1_step_1,'bildstein_station5_sub_m80_m5_stride_4_step_8.h5')
        print('checking equal and  xyz scope of file: ',fn_s)
        with h5py.File(raw_fn,'r') as h5f:
            with h5py.File(fn_s,'r') as sh5f:
                sorted_h5f = Sorted_H5f(sh5f,raw_fn)
                #sorted_h5f.show_summary_info()
               # flag1 = sorted_h5f.check_equal_to_raw(h5f)
               # if flag1:
               #     print('equal check passed')
               # else:
               #     print('equal check failed')
                flag2 = sorted_h5f.check_xyz_scope()
                if flag2:
                    print('xyz scope check passed')
                else:
                    print('xyz scope check failed')


def Do_extract_sub_area():
    folder = GLOBAL_PARA.ETH_A_rawh5
    fnl = glob.glob(os.path.join(folder,'bildstein_station5_stride_1_step_1.h5'))
    #sub_xyz_scope = np.array([[-30,-30,-20],[0,0,50]])
    #new_flag = '_sub_m30_0'
    sub_xyz_scope = np.array([[-80,-80,-20],[-5,-5,50]])
    new_flag = '_sub_m80_m5'
    print('sub_scope:\n',sub_xyz_scope)
    for fn in fnl:
        fn_parts =  os.path.splitext(fn)
        new_name = fn_parts[0]+new_flag+fn_parts[1]
        print('sub file name: ',new_name)
        with h5py.File(fn,'r') as s_h5f:
            sorted_h5f = Sorted_H5f(s_h5f,fn)
            sorted_h5f.extract_sub_area(sub_xyz_scope,new_name)


def Add_sorted_total_row_block_N_onefile(fn):
        print('calculating row_N block_N of: ',fn)
        with h5py.File(fn,'a') as h5f:
            sorted_h5f = Sorted_H5f(h5f,fn)
            rN,bN = sorted_h5f.add_total_row_block_N()
            print('rn= ',rN, '  bN= ',bN,'\n')
def Add_sorted_total_row_block_N():
    folder = GLOBAL_PARA.ETH_A_step_20_stride_10
    fnl = glob.glob(os.path.join(folder,'*.hdf5'))
    IsMulti_aN = False
    if not IsMulti_aN:
        for fn in fnl:
            Add_sorted_total_row_block_N_onefile(fn)
    else:
        p = mp.Pool(3)
        p.map(Add_sorted_total_row_block_N_onefile,fnl)
        p.close()
        p.join()


def Do_gen_raw_obj():
    ETH_training_partAh5_folder =  GLOBAL_PARA.ETH_A_rawh5
    folder_path = ETH_training_partAh5_folder
    file_list = glob.glob( os.path.join(folder_path,'b*.hdf5') )
    IsLabelColor = True
    for fn in file_list:
        print(fn)
        if IsLabelColor:
            meta_fn = '_labeledColor'
        else:
            meta_fn = ''
        obj_fn = os.path.splitext(fn)[0]+meta_fn+'.obj'
        with h5py.File(fn,'r') as  raw_h5_f:
            raw_h5f = Raw_H5f(raw_h5_f)
            raw_h5f.generate_objfile(obj_fn,IsLabelColor)

def Do_gen_sorted_block_obj(file_list):
    for fn in file_list:
        with  h5py.File(fn,'r') as h5f:
            sorted_h5f = Sorted_H5f(h5f,fn)
            sorted_h5f.gen_file_obj(True)

def Do_gen_normed_obj(file_list):
    for fn in file_list:
        with  h5py.File(fn,'r') as h5f:
            norm_h5f = Normed_H5f(h5f,fn)
            norm_h5f.gen_gt_pred_obj()

def Do_gen_gt_pred_objs(file_list=None):
    if file_list == None:
        folder = GLOBAL_PARA.stanford_indoor3d_globalnormedh5_stride_0d5_step_1_4096
        # many chairs and tables
        #file_list = glob.glob(os.path.join(folder,'Area_1_office_16_stride_0.5_step_1_random_4096_globalnorm.sph5'))
        # simple only one table
        #file_list = glob.glob(os.path.join(folder,'Area_6_pantry_1_stride_0.5_step_1_random_4096_globalnorm.sph5'))

        # wall good 0.88M
        fn_glob_good = 'Area_6_office_25_stride_0.5_step_1_random_4096_globalnorm.sph5'
        # all poor ave 0.5  1.5M
        fn_glob_poor = 'Area_5_storage_1_stride_0.5_step_1_random_4096_globalnorm.sph5'
        # test wall recall low
        fn_glob_test = 'Area_1_office_10_stride_0.5_step_1_random_4096_globalnorm.sph5'
        fn_glob = [fn_glob_good,fn_glob_poor,fn_glob_test]
        file_list = [ os.path.join(folder,fn)  for fn in fn_glob]

    for fn in file_list:
        with h5py.File(fn,'r') as h5f:
            norm_h5f = Normed_H5f(h5f,fn)
            norm_h5f.gen_gt_pred_obj_examples()

def gen_file_list(folder,format='h5'):
    file_list = glob.glob( os.path.join(folder,'*.'+format) )
    print(file_list)
    with open(os.path.join(folder,'all_files.txt'),'w') as f:
        for fn in file_list:
            base_filename = os.path.basename(fn)
            base_dirname = os.path.basename( os.path.dirname(fn) )
            base_dir_file_name = os.path.join(base_dirname,base_filename)
            f.write( base_dir_file_name )
            print(base_dir_file_name)
    print('all file list file write OK ')



def main(file_list):

    outdoor_prep = MAIN_DATA_PREP()
    actions = ['merge','sample_merged','obj_sampled_merged','norm_sampled_merged']
    actions = ['merge','sample_merged','norm_sampled_merged']
    outdoor_prep.main(file_list,actions,sample_num=4096,sample_method='random',\
                      stride=[8,8,-1],step=[8,8,-1])

    #outdoor_prep.Do_sort_to_blocks()
    #Do_extract_sub_area()
    #outdoor_prep.test_sub_block_ks()
    #outdoor_prep.DO_add_geometric_scope_file()
    #outdoor_prep.DO_gen_rawETH_to_h5()

def show_h5f_file():
    fn = '/home/y/Research/dynamic_pointnet/data/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1/region2.sh5'
    fn = '/home/y/DS/Matterport3D/Matterport3D_H5F/v1/scans/17DRP5sb8fy/stride_0d1_step_0d1_pyramid-1_2-512_128_64_16-0d2_0d4_0d8_16/region2.prh5'
    with h5py.File(fn,'r') as h5f:
        show_h5f_summary_info(h5f)

if __name__ == '__main__':
 #   file_list = glob.glob( os.path.join(GLOBAL_PARA.ETH_A_stride_1_step_1, \
 #               '*_m5.h5') )
    #file_list = glob.glob( os.path.join(GLOBAL_PARA.ETH_A_stride_8_step_8, \
                #'*_4096.h5') )
   # file_list = glob.glob( os.path.join(GLOBAL_PARA.seg_train_path, \
   #             '*.h5') )
    #main(file_list)
    #Do_gen_raw_obj()
    #Add_sorted_total_row_block_N()
    #Do_Check_xyz()
    #Do_sample()
    #Do_gen_sorted_block_obj(file_list)
    #Do_gen_normed_obj(file_list)
    #Do_Norm(file_list)
    #gen_file_list(GLOBAL_PARA.seg_train_path)
    #Do_gen_gt_pred_objs()


    #Write_Area_accuracies()
    #Write_all_file_accuracies()
    #Test_get_block_data_of_new_stride_step()
    #Do_normalize_sorted_to_self()
    #Normed_H5f.show_all_colors()
    #Gen_raw_label_color_obj()

    show_h5f_file()
    T = time.time() - START_T
    print('exit main, T = ',T)
